<?xml version="1.0" encoding="UTF-8" ?>

<pretext>

<docinfo>
<brandlogo url="http://www.umanitoba.ca" source="images/umlogo.png" /> 
   <macros>
   <!-- Blackboard bold characters -->
   \def\R{{\mathbb R}}
   \def\C{{\mathbb C}}
   \def\Q{{\mathbb Q}}
   \def\Z{{\mathbb Z}}
   \def\N{{\mathbb N}}
   <!-- redefine \vec from overarrow to bf -->
   \def\vec#1{\mathbf #1}
   <!-- New operators -->
   \newcommand{\adj}{\mathop{\textrm{adj}}}
   \newcommand{\proj}{\mathop{\textrm{proj}}}
   \newcommand{\Span}{\mathop{\textrm{span}}} <!-- \span is a TeX primitive  -->
   <!-- Shortcuts for typesetting elementary row operations-->
   \newcommand{\rowint}[2]{R_{#1} \leftrightarrow R_{#2}}
   \newcommand{\rowmul}[2]{R_{#1}\gets {#2}R_{#1}}
   \newcommand{\rowadd}[3]{R_{#1}\gets R_{#1}+#2R_{#3}}
   \newcommand{\rowsub}[3]{R_{#1}\gets R_{#1}-#2R_{#3}}
   </macros>
</docinfo>

<book xml:id="linalg"><title> Manitoba linear algebra </title>
<frontmatter>
   <titlepage>
      <author>
         <personname>Michael Doob (editor)</personname>
         <department> Department of Mathematics </department>
         <institution> The University of Manitoba 
         </institution>
         <email>Michael_Doob@UManitoba.ca</email>
      </author>
      <credit>
      <title>
      </title>
      <author>
         <personname>
         </personname>
      </author>
      </credit>
      <date>
         <today />
      </date>
   </titlepage>
</frontmatter>

<chapter>
  <title>Systems of Linear Equations</title>
  
  <section><title>Review of two equations in two unknowns </title>
    
    <subsection xml:id="TwoEquationsTwoUnknownsSample">
    <title>The standard method for finding the solution </title>
      <p>Suppose we want to find <em>all</em> solutions to the equations</p>
      <p>
      <md>
      <mrow>2x+3y \amp = 5</mrow>
      <mrow>3x+2y \amp= 7</mrow>
      </md>
      </p>
      
      <p>
      The <em>standard</em> technique is to manipulate one or both 
      of the equations until either <m>x</m> has the same coefficient 
      in both equations or <m>y</m> has the same coefficient, and then 
      subtract to eliminate one variable. Since there is only one variable left,
      its value can be found; with this information, the value 
      for the other variable can be found.
      </p>
     
      <p>In the case above, we can multiply both sides of the second equation by
      <m>\frac23</m>,
      to get
      <me>
      2x+\tfrac43 y= \tfrac{14}3,
      </me>
      and subtracting from the first equation gives
      <me>
      \frac53y = \frac13,
      </me>
      or
      <me>
      y=\frac15.
      </me>
      </p>

      <p>
      Using the first equation and the (now) known value of <m>y</m>, we find that
      <me>
      x=\frac{11}5.
      </me>
      </p>
      
      <p>
      Hence we see that there is exactly one pair of values for 
      <m>x</m>
      and 
      <m>y</m> that simultaneously satisfy both equations:
      <m>x=\frac{11}5</m> and <m>y=\tfrac15.</m> We can also write this
      as <m>(x,y)=(\frac{11}5,\frac15).</m>
      In this case we say that there is a <em>unique</em> solution (in mathematics, the term
      unique means <em>exactly one</em>).
      </p>
    </subsection>
    
    <subsection><title>The geometric method of finding the solution</title> 
      
      <p>
      The set of equations solved in  <xref ref="TwoEquationsTwoUnknownsSample"/>
      can also be
      viewed geometrically. The points in the <m>x</m>-<m>y</m>
      plane satisfying one of the equations lie on a  straight line, and
      any point satisfying both of the equations must lie on both lines.
      The plot of the two lines looks like this:
      </p>
      
      <figure>
      <caption>Intersection of two lines in the plane</caption>
      <image xml:id="TwoLinesInPlane" width="40%" archive="pdf svg">
      <asymptote>
      import graph;
      unitsize(30);
      real xmin=-1; real xmax=4;
      xaxis(LeftTicks);yaxis(LeftTicks);

      real f1(real x) {return (5-2*x)/3;}
      real f2(real x) {return (7-3*x)/2;}

      draw((xmin,f1(xmin))--(xmax,f1(xmax)),green);
      draw((xmin,f2(xmin))--(xmax,f2(xmax)),red);
      dot((11/5,1/5));
      label("\((\frac{11}5,\frac15)\)",(11/5,1/5),NE);
      </asymptote>
      </image>
      </figure>

      <p>The point of intersection is <m>(x,y)=(\frac{11}5,\frac15)</m>,
      so this is the only solution, just as before.</p>

      <exercise>
         <statement>
         <p>
         Are the points satisfying the equation 
         <m>2x+3y = 5</m> on the red line or the green line?
         </p>
         </statement>
         <solution>
         <p>
         Setting <m>x=0</m> gives <m>y=\frac53</m> as the point where the
         line intersects the <m>y</m>-axis. Hence it is the green line.
         </p>
         </solution>
      </exercise>
    </subsection>
 </section> 
  
  <section><title>Equations with multiple solutions or no solutions</title>
    <subsection><title>Equations with no solutions</title> 
     <p>
     Suppose we want to find all solutions to the equations
      <md>
      <mrow>2x+4y \amp = 5</mrow>
      <mrow>-x-2y \amp = 1</mrow>
      </md>
     Using the <em>standard</em> approach we multiply the second equation by <m>2</m> and add it
      to the first one to eliminate the variable <m>x</m>.  This leaves us
      with the equation
      0=7
      This is certainly an equality that is not valid. What happened? We can see by
      multiplying the second equation by <m>-2</m>. We then have 
      <md>
      <mrow>2x+4y \amp = 5</mrow>
      <mrow>2x+4y \amp = -2</mrow>
      </md>
     </p>

      <p>
      Whatever <m>x</m> and <m>y</m> are, the value of <m>2x+4y</m> can't be 
      <m>5</m> and <m>-2</m> at the same time. So there are no solutions.
      What happens if we try to graph these two equations? Here is what we get:
     </p>
     
     <figure>
     <caption>Two Parallel lines in the Plane</caption>
     <image xml:id="ParallelLinesInPlane" width="70%">
     <asymptote>
      import graph;
      unitsize(40);
      real xmin=-2; real xmax=4;
      xaxis(LeftTicks);yaxis(LeftTicks);

      real f1(real x) {return (5-2*x)/4;}
      real f2(real x) {return (1+x)/-2;}

      draw((xmin,f1(xmin))--(xmax,f1(xmax)),green);
      draw((xmin,f2(xmin))--(xmax,f2(xmax)),red);
      </asymptote>
      </image>
     </figure>
      
      <p>
      The geometry of the situation is now clear: 
      the two lines are parallel and so there is no 
      point on both lines (indeed, both lines have slope <m>-\tfrac12</m>).
      When we have equations with no common solution, they are called <em>inconsistent</em>.
      </p>
    </subsection>
    
    <subsection><title> Equations with more than one solution </title>
      
      <p>Now let's alter the equations of <xref ref="TwoEquationsTwoUnknownsSample"/> 
      slightly.  We consider the pair of equations

      <md>
      <mrow>2x+4y \amp = -2</mrow>
      <mrow>-x-2y \amp = 1</mrow>
      </md>

      We apply the <em>standard</em> method again: multiply the second equation by <m>2</m>
      and add it to the first. The result is
      <me>0=0</me> 
      This is certainly a valid, although not very interesting, equation. In fact, if we
      multiply both sides of the second equation by <m>-2</m> the system becomes

      <md>
      <mrow>2x+4y \amp = -2</mrow>
      <mrow>2x+4y \amp = -2</mrow>
      </md>

      This means that any solution of the first equation is also a solution of the second one.
      Geometrically, if we plot the graph of the two equations, the same line results for
      each one. How do we find all solutions in this case? Let us assign a value to <m>y</m>.
      Let's call it <m>t</m> so <m>y=t</m>. Then, using either equation, we have
      <m>x=-2t-1</m>.
      This means that for <em>any</em> value of <m>t</m> we know that
      <m>(x,y)=(-2t-1,t)</m> is a solution to both equations. So, in fact we have an infinite
      number of solutions.</p>

      <theorem xml:id="TwoEquationaTwoUnknowns"><title>Two equations in two unknowns</title>
         <statement>
         <p>Two equations in two unknowns may have:
         <ul>
         <li><p>No solutions</p></li>
         <li><p>A single (unique)  solutions</p></li>
         <li><p>An infinite number of solutions</p></li>
         </ul>
         </p>
         </statement>
         <proof><p>
        The corresponding lines in the plane are parallel, intersect at
        a single point, or are identical.
         </p></proof>
      </theorem>
    </subsection>
  </section>
  
  <section><title>Important Definitions for Linear Equations</title> 
   
   <subsection><title>The Definition of a Linear Equation</title>
     
     <p>
     The equations given in the in  <xref ref="TwoEquationsTwoUnknownsSample"/>
     are examples of <em>linear equations</em>  in two variables. 
      A <em>linear equation in <m>n</m> variables</em> 
      <m>x_1, x_2, \dots, x_n</m> is an equation of the form

      <me>
      a_1x_1+ a_2x_2+ \cdots +a_n x_n= b
      </me>

      where <m>a_1,\dots,a_n</m> and <m>b</m> are constants.
      The numbers <m>a_1,a_2,\dots,a_n</m> are called the <em>coefficients</em>
      of the equation, that is, <m>a_1</m> is the coefficient of <m>x_1</m>, 
      <m>a_2</m> is the coefficient of <m>x_2</m>, <etc />
      This is also called a <em>linear equation in <m>n</m> unknowns</em>.</p>
      
      <p>The following are examples of linear equations:
      <ul>
        <li><p> <m>2x=4</m>   (one variable)</p></li> 
        <li><p> <m>4x-2y=7</m>   (two variables)</p></li> 
        <li><p><m>\tfrac12 x -\tfrac35 y +\pi z=0</m>   (three variables)</p></li> 
        <li><p> <m>x_1-3x_2+4x_3-7x_4=100</m>   (four variables: a coefficient of
        <m>1</m> is usually omitted)</p></li> 
        <li><p><m>3x_1+15x_5=-10</m>
            (five variables: the ones with a coefficient of zero are not written)</p>
        </li> 
      </ul>
      </p>
   </subsection>
 
   <subsection><title>The Definition of a System of Linear Equations</title>
      
      <p>A <em>system of <m>m</m> linear equations in <m>n</m>
      unknowns</em> is a list of <m>m</m> linear equations, each of which
      has the same set of <m>n</m> unknowns. They are usually presented
      with the purpose of finding all of their simultaneous solutions.</p>
      
      <p>The original example given in
      <xref ref="TwoEquationsTwoUnknownsSample"/>
      is a system of 2 equations in 2 unknowns.
      Here are some more examples:
      <ul>
        <li><p>A system of 2 equations in 3 unknowns:
            <md>
            <mrow>x-y+z \amp = 1</mrow>
            <mrow>2x+3y-z \amp = 2</mrow>
            </md>
            </p>
        </li>
        <li><p>A system of 3 equations in 2 unknowns:
            <md>
            <mrow>u+v \amp = 1</mrow>
            <mrow>2u+v \amp = 3</mrow>
            <mrow>u+2v \amp =3</mrow>
            </md>
            </p>
        </li>
        <li><p>A system of 4 equations in 5 unknowns:
            <md alignment="alignat">
            <mrow>x_1 \amp \ +\  \amp x_2 \amp \amp \amp \amp \amp \amp \amp \ =\ 1</mrow>
            <mrow>\amp \amp x_2 \amp \ +\ \amp x_3 \amp \amp \amp \amp \amp  \ =\ 2</mrow>
            <mrow>\amp \amp \amp \amp x_3 \amp \ +\ \amp x_4 \amp \amp \amp  \ =\ 3</mrow>
            <mrow>\amp \amp \amp \amp \amp \amp x_4 \amp \ +\ \amp x_5 \amp  \ =\ 4</mrow>
            </md>
            </p>
        </li>
        <li><p>A system of <m>m</m> equations in <m>n</m> unknowns <m>x_1, x_2, x_3\dots,x_n</m>:
            <md>
            <mrow>a_{1,1}x_1 + a_{1,2}x_2 + \cdots + a_{1,n}x_n  = b_1</mrow>
            <mrow>a_{2,1}x_1 + a_{2,2}x_2 + \cdots + a_{2,n}x_n  = b_2</mrow>
            <mrow>\vdots</mrow>
            <mrow>a_{m,1}x_1 + a_{m,2}x_2 + \cdots + a_{m,n}x_n  = b_m</mrow>
            </md>
            </p>
        </li>
      </ul>
      </p>
      
   </subsection>
 
   <subsection><title>The Coefficient and Augmented Matrix of a System of Linear Equations </title>
      
      <p>A <em>matrix</em> is a rectangular array of numbers (the plural is <em>matrices</em>). Here is a matrix:
      <me>
      \begin{bmatrix}
      1 \amp  2 \amp  3 \amp  4\\
      5 \amp  6 \amp  7 \amp  8\\
      9 \amp  10\amp 11\amp 12
      \end{bmatrix}
      </me>
      This matrix has 3 rows:
      <ul>
      <li>
       <m>
          \begin{bmatrix}
          1 \amp  2 \amp  3 \amp  4
          \end{bmatrix}
      </m> is row 1,
      </li>
      <li>
      <m>
         \begin{bmatrix}
         5 \amp  6 \amp  7 \amp  8
         \end{bmatrix}
      </m> is row 2 and
      </li>
      <li>
      <m>
         \begin{bmatrix}
         9 \amp  10\amp 11\amp 12
         \end{bmatrix}
      </m>
      is row 3.
      </li>
      </ul>
      </p>
      
      <p>Similarly, the matrix has 4 columns:
      <ul>
      <li>
      <m>
         \begin{bmatrix}
         1\\ 5\\ 9 
         \end{bmatrix}
      </m>
      is  column <m>1</m>
      </li>
      <li>
      <m>
         \begin{bmatrix}
         2\\ 6\\ 10
         \end{bmatrix}
      </m>
      is  column <m>2</m>
      </li>
      
      <li>
      <m>
         \begin{bmatrix}
         3 \\ 7\\ 11
         \end{bmatrix}
      </m>
      is  column <m>3</m>, and
      </li>
      <li>
      <m>
         \begin{bmatrix}
         4\\ 8\\ 12
         \end{bmatrix}
      </m>
      is  column <m>4</m>
      </li>
      </ul>
      </p>

      <p>We call a matrix with 3 rows and 4 columns a <m>3\times4</m>
      matrix.</p>
      
      <p>More generally, a matrix with <m>m</m> rows and <m>n</m> columns is called an
      <m>m\times n</m> <em>matrix</em>. An <m>m\times n</m> matrix <m>A</m> has the form
      <me>
      A=\begin{bmatrix}
      a_{1,1} \amp  a_{1,2} \amp  \cdots \amp  a_{1,n}\\
      a_{2,1} \amp  a_{2,2} \amp  \cdots \amp  a_{2,n}\\
      \amp \amp \vdots\\
      a_{m,1} \amp  a_{m,2} \amp  \cdots \amp  a_{m,n}
      \end{bmatrix}
     </me>
      This notation means that <m>a_{i,j}</m> is the number in both row
      <m>i</m> and column <m>j</m>. We will call the rows
      <m>R_1, R_2,\ldots,R_m</m> and the columns <m>C_1, C_2, \ldots,C_n</m>.
      In other words,
      <me>
      R_i =\begin{bmatrix} a_{i,1} \amp  a_{i,2} \amp  a_{i,3}, 
          \amp \cdots \amp  a_{i,n}\end{bmatrix},
      </me>
      and
      <me>
      C_j=\begin{bmatrix}
      a_{1,j}\\ a_{2,j}\\ a_{3,j}\\ \vdots\\ a_{m,j}
      \end{bmatrix}
      </me>
      The notation for this matrix is similar to that used for
      a system of linear equations, and for good reason. Suppose
      we have a system of <m>m</m> linear equations in <m>n</m> unknowns:
      <me>
      \begin{array}{lcl}
      a_{1,1}x_1 + a_{1,2}x_2 +{} \amp \cdots \amp {}+ a_{1,n}x_n  = b_1\\
      a_{2,1}x_1 + a_{2,2}x_2 +{} \amp \cdots \amp {}+ a_{2,n}x_n  = b_2\\
      \amp \vdots\\
      a_{m,1}x_1 + a_{m,2}x_2 +{} \amp \cdots \amp {}+ a_{m,n}x_n  = b_m
      \end{array}
      </me>
      The <em>coefficient matrix</em> <m>A</m> is then the <m>m\times n</m> matrix
      <me>
      A=\begin{bmatrix}
      a_{1,1} \amp  a_{1,2} \amp  \cdots \amp  a_{1,n}\\
      a_{2,1} \amp  a_{2,2} \amp  \cdots \amp  a_{2,n}\\
      \amp \amp \vdots\\
      a_{m,1} \amp  a_{m,2} \amp  \cdots \amp  a_{m,n}
      \end{bmatrix}
      </me>
      and the <em>augmented matrix</em> of the system is the <m>m\times( n+1)</m> matrix
      <me>
      A=\begin{bmatrix}
      a_{1,1} \amp  a_{1,2} \amp  \cdots \amp  a_{1,n}\amp b_1\\
      a_{2,1} \amp  a_{2,2} \amp  \cdots \amp  a_{2,n}\amp b_2\\
      \amp \amp \vdots\amp \amp \\
      a_{m,1} \amp  a_{m,2} \amp  \cdots \amp  a_{m,n}\amp b_m
      \end{bmatrix}
      </me>
      Hence the augmented is the coefficient matrix with one column (the constants on the
      right side of the equations) added. To emphasize the extra column, the augmented
      matrix is sometimes written as
      <me>
      A=\left[\begin{array}{llcl|l}
      a_{1,1} \amp  a_{1,2} \amp  \cdots \amp  a_{1,n}\amp b_1\\
      a_{2,1} \amp  a_{2,2} \amp  \cdots \amp  a_{2,n}\amp b_2\\
      \amp \amp \vdots\amp \amp \\
      a_{m,1} \amp  a_{m,2} \amp  \cdots \amp  a_{m,n}\amp b_m
      \end{array}\right]
      </me></p>
      
      <p>Here are some examples of coefficient matrices and augmented matrices:
      <ul>
      <li><p>A system of 2 equations in 3 unknowns:
            <md>
            <mrow>x-y+z \amp = 1</mrow>
            <mrow>2x+3y-z \amp = 2</mrow>
            </md>
      Coefficient matrix: 
        <me>
        \begin{bmatrix}
        1\amp -1\amp 1\\
        2\amp 3\amp -1
        \end{bmatrix}
        </me>
      Augmented matrix: 
        <me>
        \begin{bmatrix}
        1\amp -1\amp 1\amp 1\\
        2\amp 3\amp -1\amp 2
        \end{bmatrix}
        </me>
      </p></li> 
     
     <li>
      <p>A system of 3 equations in 2 unknowns:
      <md>
      <mrow>u+v  \amp = 1</mrow>
      <mrow>2u+v \amp = 3</mrow>
      <mrow>u+2v \amp =3</mrow>
      </md>
      </p>
      
      <p>Coefficient matrix: <m>\begin{bmatrix}
      1\amp 1\\
      2\amp 1\\
      1\amp 2
      \end{bmatrix}</m></p>
      
      <p>Augmented matrix: <m>\left[\begin{array}{cc|c}
      1\amp 1\amp 1\\
      2\amp 1\amp 3\\
      1\amp 2\amp 3
      \end{array}\right]</m></p>
      </li> 
      
     <li><p>A system of 4 equations in 5 unknowns
            <md alignment="alignat">
            <mrow>x_1 \amp \ +\  \amp x_2 \amp \amp \amp \amp \amp \amp \amp \ =\ 1</mrow>
            <mrow>\amp \amp x_2 \amp \ +\ \amp x_3 \amp \amp \amp \amp \amp  \ =\ 2</mrow>
            <mrow>\amp \amp \amp \amp x_3 \amp \ +\ \amp x_4 \amp \amp \amp  \ =\ 3</mrow>
            <mrow>\amp \amp \amp \amp \amp \amp x_4 \amp \ +\ \amp x_5 \amp  \ =\ 4</mrow>
            </md>
            </p> 
      <p>Coefficient matrix: 
         <m>\begin{bmatrix}
            \begin{array}{rrrrr}
                1\amp 1\amp 0\amp 0\amp 0\\
                0\amp 1\amp 1\amp 0\amp 0\\
                0\amp 0\amp 1\amp 1\amp 0\\
                0\amp 0\amp 0\amp 1\amp 1
            \end{array}
            \end{bmatrix}
         </m>
      </p>
      
      <p>Augmented matrix: <m>\left[\begin{array}{ccccc|c}
      1\amp 1\amp 0\amp 0\amp 0\amp 1\\
      0\amp 1\amp 1\amp 0\amp 0\amp 2\\
      0\amp 0\amp 1\amp 1\amp 0\amp 3\\
      0\amp 0\amp 0\amp 1\amp 1\amp 4
      \end{array}\right]</m></p></li> 
      </ul>
    </p>     
    <exercises> 
       <exercise>
       <statement>
       <p>Which of the following equations are linear?
       <ol>
       <li> <p><m>x+y-z^2=0</m></p></li>
       <li> <p><m>2x+yz=1</m></p></li>
       <li> <p><m>z=3</m></p></li>
       <li> <p><m>\pi^2 x-\sqrt[3]{3}y=8\sqrt{5}</m></p></li>
       <li> <p><m>3x-4y=\frac73 z-5+t</m></p></li>
       <li> <p><m>\cos{x}=3y+2z</m></p></li>
       </ol></p>
       </statement>
       <solution>
       <p>
       <ol>
       <li><p>Not linear because of <m>z^2</m></p></li>
       <li><p>Not linear because of <m>yz</m></p></li>
       <li><p>Linear</p></li>
       <li><p>Linear (the exponents involve the constants, but not the variables)</p></li>
       <li><p>Linear (Tricky because variables are on both sides of the equal sign.
       It is the same as <m>3x-4y-\frac73 z-t =-5 </m>.)</p></li>
       <li><p>Not linear because of the <m>\cos</m> function</p></li>
       </ol>
       </p>
       </solution>    
       </exercise>

       <exercise>
       <statement>
       <p>Consider the following system of linear equations:
       <me>
       2x-4y+z=-1\\
       3x-3y+z=1
       </me>
       Which of the following are solutions to this system
       <ul>
       <li><p><m> (x,y,z)=(2,1,-1) </m></p></li>
       <li><p><m> (x,y,z)=(1,1,1) </m></p></li>
       <li><p><m> (x,y,z)=(0,0,1) </m></p></li>
       </ul>
       </p>
       </statement>
       <solution>
       <p>
       <ul>
       <li><p>
       Substitute <m> (x,y,z)=(2,1,-1) </m> into the equations:
       <me>
         2x-4y+z=4-4-1=-1\\
         3x-3y+z=6-3-1=2 \not=1
       </me>
       and so <m>(2,1,-1)</m> is <em>not</em> a solution.
       </p></li>
       <li><p> Substitute <m> (x,y,z)=(1,1,1) </m> into the equations:
       <me>
         2x-4y+z=2-4+1=-1\\
         3x-3y+z=3-3+1=1
       </me>
       and so <m>(1,1,1)</m> <em>is</em> a solution.
       </p></li>
       <li><p> Substitute <m> (x,y,z)=(0,0,1) </m> into the equations:
       <me>
         2x-4y+z=0+0+1\not=-1\\
         3x-3y+z=0+0+1=1
       </me>
       and so <m>(0,0,1)</m> is <em>not</em> a solution.
       </p></li>
       </ul></p>
       </solution>
       </exercise>
   
       <exercise>
       <statement>
       <p>
       Give a system of two equations in two unknowns that has <m>(1,2)</m>
       as its unique solution.
       </p>
       </statement>
       <solution>
       <p>
       There are many solutions corresponding to two lines intersecting
       at <m>(1,2)</m>. Surely the easiest is
       <md>
       <mrow>x\amp=1</mrow>
       <mrow>y\amp=2</mrow>
       </md>
       </p>
       </solution>
       </exercise>
   
       <exercise>
       <statement>
           <p>
           Give the coefficient matrix and the augmented matrix this system of
           linear equations:
           <md>
           <mrow>x+y \amp= 2 </mrow>
           <mrow>x-y \amp= 0 </mrow>
           </md>
           </p>
       </statement>
       <solution>
         <p>
         <m>
          \begin{bmatrix}
          1\amp1\\1\amp-1
          \end{bmatrix}
         </m> 
      and
      <m>
         \left[\begin{array}{cc|c}
         1 \amp 1 \amp 2 \\  1 \amp -1 \amp 0
         \end{array}\right]
      </m>
      </p>
       </solution>    
       </exercise>
   
       <exercise>
       <statement>
           <p>
           Give the coefficient matrix and the augmented matrix this system of
           linear equations:
           <md>
           <mrow>x_1+x_2-x_3+x_5\amp= 4 </mrow>
           <mrow>2x_1-3x_2 +x_4\amp= 7 </mrow>
           </md>
           </p>
       </statement>
       <solution>
      <p>
      <m>
      \begin{bmatrix}
      1 \amp 1 \amp -1 \amp 0 \amp 1 \\
      2\amp-3 \amp 0 \amp 1 \amp 0 
      \end{bmatrix}
      </m> 
      and
      <m>
      \left[\begin{array}{ccccc|c}
      1 \amp 1 \amp -1 \amp 0 \amp 1 \amp 4\\
      2\amp-3 \amp 0 \amp 1 \amp 0 \amp7
      \end{array}\right]
      </m>
      </p>
       </solution>
       </exercise>
   
       <exercise>
       <statement>
           <p>
           Give the coefficient matrix and the augmented matrix this system of
           linear equations:
           <md>
           <mrow>x+y-z \amp=1</mrow>
           <mrow>3x+y\amp= 2</mrow>
           <mrow>y+z \amp=3</mrow>
           <mrow>x+z\amp=4</mrow>
           <mrow>x+y+z\amp=5</mrow>
           </md>
           </p>
       </statement>
       <solution>
      <p>
      <m>
      \begin{bmatrix}
      1 \amp 1 \amp -1 \\
      3 \amp 1\amp 0 \\
      0 \amp 1 \amp 1\\
      1 \amp 0 \amp 1 \\
      1 \amp 1 \amp 1
      \end{bmatrix}
      </m> 
      and
      <m>
      \left[\begin{array}{ccc|c}
      1 \amp 1 \amp -1 \amp 1\\
      3 \amp 1\amp 0 \amp 2\\
      0 \amp 1 \amp 1\amp 3\\
      1 \amp 0 \amp 1 \amp 4\\
      1 \amp 1 \amp 1\amp 5
      \end{array}\right]
      </m>
      </p>
       </solution>
       </exercise>
   
       <exercise>
       <statement>
       <p>
       Give the system of equations whose augmented matrix is
       <me>
      \left[\begin{array}{cc|c}
      1 \amp 4  \amp 1\\
      3 \amp -2\amp 0 \\
      0 \amp 1 \amp 1\\
      -1 \amp 1 \amp 3
      \end{array}\right]
       </me>
       </p>
       </statement>
       <solution>
       <p>
       <md>
       <mrow>x+4y\amp= 1</mrow>
       <mrow>3x-2y\amp=0</mrow>
       <mrow>y\amp=1</mrow>
       <mrow>-x+y\amp=3</mrow>
       </md>
       </p>
       </solution>
       </exercise>
   
   
       <exercise>
       <statement>
       <p>
       There is a special matrix <m>I_n</m> called the identity
       matrix. It has <m>n</m> rows and <m>n</m> columns. The entries <m>a_{i,j}</m>
       of the matrix satisfy
       <me>
       a_{i,j}=
       \begin{cases}
       1 \amp \text{if } i=j\\
       0 \amp \text{if } i\not=j\\
       \end{cases}
       </me>
       <ol>
       <li><p>
       Show the the matrix <m>I_n</m> has the value <m>1</m> along the (main) diagonal going from the upper
       left corner to the lower right corner of the matrix, and has the value <m>0</m> elsewhere.
       </p></li> 
       <li><p>Suppose that a system of linear equations has an augmented matrix of the form
       <me>
       \left[ I_n \mid B \right]
       </me>
       where
       <me>
       B= \begin{bmatrix}
       b_1\\b_2\\ \vdots\\b_n
       \end{bmatrix}
       </me>
       What does the say about the number of variables (unknowns)
       for the corresponding system of linear equations, and
       what does it say about the solutions to the system?
       </p>
       </li>
       </ol>
       </p>
       </statement>
   
       <solution>
       <p>
       <ol>
       <li>
       <p>
       When <m>i=j</m>, the row number and column number are identical, and so the entry
       <m>a_{i.j}</m> is on the diagonal. This implies that 
       <me>
       a_{i,j}=
       \begin{cases}
       1 \amp \text{ for entries on the main diagonal}\\
       0 \amp \text{ for entries not on the main diagonal} \\
       \end{cases}
       </me>
       </p>
       </li>
       <li>
       <p>
       The number of unknowns is <m>n</m>, the number of columns in the matrix.
       We may call them <m>x_1, x_2,\dots,x_n</m>.
       The augmented matrix now says that
       <md>
       <mrow>x_1\amp =b_1</mrow>
       <mrow>x_2\amp =b_2</mrow>
       <mrow>\amp \vdots</mrow>
       <mrow>x_n\amp =b_n</mrow>
       </md>
       </p>
       </li>
       </ol>
       </p>
       </solution>
       </exercise>
  
   </exercises>
   
   </subsection>
   
</section>


<section><title>Elementary row operations</title>
    <introduction>
      <p>
      Gaussian elimination, which we shall describe in detail
      presently, is an algorithm (a well-defined procedure for computation
      that eventually completes) that finds all solutions to any <m>m\times n</m> system
      of linear equations. It is defined via certain operations carried out
      on the augmented matrix. These operations change the matrix (and hence
      the system of linear equations associated with it), <em>but they leave the
      set of solutions unchanged</em>. There are three of them, which we now describe.
     </p></introduction>

      <subsection><title>First operation: interchanging two rows</title>
      <p>
         Exchanging two rows in the augmented matrix is the same as writing the the equations 
         in a different order. The equations themselves are unchanged, as is the set of all 
         solutions. When we interchange row <m>i</m> and row <m>j</m>, we denote it by 
         <m>R_i\leftrightarrow R_j</m>. 
      </p>

        <example><title>Augmented matrix change (interchange rows)</title>
        <p>The system of equations
        <me>
        \begin{array}{rl}
        2x+3y-z \amp = 2\\
        -x+y+z \amp =4\\
        3x-y-z \amp = 3
        \end{array}
        </me> 
        corresponds to the augmented matrix
        <me>
        \left[\begin{array}{ccc|c}
        2 \amp 3 \amp -1 \amp 2\\
        -1 \amp 1 \amp 1 \amp 4\\
        3 \amp -1 \amp -1 \amp 3
        \end{array}\right]
        </me>
        Now we interchange rows 1 and 3 (<m>R_1\leftrightarrow R_3</m>) to get
        the matrix
        <me>
        \left[\begin{array}{ccc|c}
        3 \amp -1 \amp -1 \amp 3\\
        -1 \amp 1 \amp 1 \amp 4\\
        2 \amp 3 \amp -1 \amp 2
        \end{array}\right]
        </me>
        which corresponds to the equations
        <me>
        \begin{array}{rl}
        3x-y-z \amp = 3\\
        -x+y+z \amp =4\\
        2x+3y-z \amp = 2
        \end{array}
        </me>
        </p>
        </example>

      </subsection>
      
      <subsection><title>Second operation: multiplying a row by a nonzero constant</title>
       <p>
       If we have a linear equation and multiply both sides of the equation by a nonzero
        constant <m>\lambda</m>, (The symbol <m>\lambda</m> is the Greek letter lambda; it
        is the traditional name for this constant.) then the linear equation
        <me>
        a_1x_1 + a_2x_2 +a_3x_3 + \cdots + a_nx_n =b
        </me>
        becomes
        <me>
        \lambda(a_1x_1 + a_2x_2 +a_3x_3 + \cdots + a_nx_n) =\lambda b
        </me>
        and so
        <me> \lambda a_1x_1 + \lambda a_2x_2 +\lambda a_3x_3 + \cdots +\lambda a_nx_n =\lambda b.
        </me>
        This would apply to any equation in a system, of course, 
        and so when we apply this operation to row <m>i</m> we denote it by 
        <m>R_i \gets \lambda R_i</m> (The symbol <m>\gets</m> means <em>is replaced by</em>). 
        <em>As long as <m>\lambda</m> is nonzero</em>, 
        this operation leaves the set of solutions unchanged.
        </p>

        <example><title>Augmented matrix change (multiply row by <m>\lambda</m>)</title>
        <p>The system of equations
        <me>
        \begin{array}{rl}
        2x+3y-z \amp = 2\\
        -x+y+z \amp =4\\
        3x-y-z \amp = 3
        \end{array}
        </me> 
        corresponds to the augmented matrix
        <me>
        \begin{bmatrix}
        2 \amp 3 \amp -1 \amp 2\\
        -1 \amp 1 \amp 1 \amp 4\\
        3 \amp -1 \amp -1 \amp 3
        \end{bmatrix}
        </me>
        Now we multiply row <m>2</m> by <m>\lambda=-3</m> (<m>R_2\gets -3R_2</m>) to get
        the matrix
        <me>
        \begin{bmatrix}
        2 \amp 3 \amp -1 \amp 2\\
        3 \amp -3 \amp -3 \amp -12\\
        3 \amp -1 \amp -1 \amp 3
        \end{bmatrix}
        </me>
        which corresponds to the system of equations
        <me>
        \begin{array}{rl}
        2x+3y-z \amp = 2\\
        3x-3y-3z \amp =-12\\
        3x-y-z \amp = 3
        \end{array}
        </me>
        </p>
        </example>
      
      </subsection>

      <subsection><title>Third operation: adding a multiple of one row to another</title>
        <p>Recall that one of our basic techniques for solving a
        system of equations is to pick a variable,
        make the coefficients of that variable equal in two equations
        and then take the difference of the equations
        to create a new one with that variable eliminated. In terms of
        the associated augmented matrix, this means we get a new row
        formed by subtracting the corresponding entries in the rows of
        the two equations.  In other words, we replace one row by the
        difference of two rows. We use the notation <m>R_i\gets R_i-R_j</m>
        to indicate that row <m>i</m> is replaced by the difference of
        row <m>i</m> and row <m>j</m>.  The set of solutions is unchanged. We
        have already seen that multiplying a row by a nonzero constants
        leaves the set of solutions unchanged; it is often convenient to
        combine these two steps as one: <m>R_i\gets R_i+\lambda R_j.</m>
        Note that <m>\lambda=-1</m> is the case of subtracting one row
        from another.</p>

        <example><title>Augmented matrix change (add multiple of one
        row to another)</title> 
        <p>Eliminating <m>x</m> from the second
        equation: 
        <me> 
        \begin{array}{rl}
        -x+y+z \amp =4\\
        2x+3y-z \amp = 2\\
        3x-y-z \amp = 3
        \end{array}
        </me> 
        corresponds to the
        augmented matrix 
        <me> 
        \begin{bmatrix} 
        -1 \amp 1 \amp 1 \amp 4\\
        2 \amp 3 \amp -1 \amp 2\\ 
        3 \amp -1 \amp -1 \amp 3 
        \end{bmatrix}
        </me> 
        Now we replace row 2 by the sum of row 2 and twice row 1
        <m>(R_2\gets R_2+2R_1)</m> to get the matrix 
        <me> 
        \begin{bmatrix}
        -1 \amp 1 \amp 1 \amp 4\\ 
        0 \amp 5 \amp 1 \amp 10\\ 
        3 \amp -1 \amp -1 \amp 3 
        \end{bmatrix} 
        </me> 
        which corresponds to the system of linear equations 
        <me> 
        \begin{array}{rl} -
        x+y+z
        \amp =4\\ 
        5y+z \amp =10\\ 
        3x-y-z \amp = 3 
        \end{array} 
        </me>
        </p> 
        </example>
      </subsection>

      <subsection><title>Summary of the three elementary row operations</title>
        <p> In summary, here is a table of the three elementary row
        operations: 
        </p>

        <table xml:id="TableOfElemantaryRowOperations">
        <title>Elementary Row Operations</title>
        <tabular bottom="minor" top="minor" left="minor" right="minor" halign="center">
        <row>
            <cell><em>Elementary Operation</em></cell>
            <cell><em>Notation</em></cell>
        </row>
        <row>
            <cell>Interchange rows</cell>
            <cell><m>R_i\leftrightarrow R_j</m></cell>
        </row>
        <row>
            <cell>Multiply row by a nonzero constant</cell>
            <cell><m>R_i\gets\lambda R_i</m>, <m>\lambda\not=0</m></cell>
        </row>
        <row>
            <cell>Add multiple of one row to another</cell>
            <cell><m>R_i\gets R_i+\lambda R_j</m></cell>
        </row>
        </tabular>
        </table>
        
        <p>
        Each operation changes
        the matrix and the associated system of linear equations, but
        it leaves the set of solutions unchanged.  </p>
      </subsection>

      <subsection><title>Changing a specific entry of a matrix using elementary row
      operations</title>

      <p>
      We want to show that it is possible to change specific entries
      in a matrix in an advantageous way using elementary row 
      operations. Specifically, we will show two things:
      <ul>
      <li><p> Any <m>a_{i,k}\not=0</m> may be changed to <m>1</m> with one elementary row
         operation. </p>
      </li>
      <li>
        <p>If <m>a_{i,k}\not=0</m>, then any other entry in the same column
        may be changed to <m>0</m> with one elementary row operation.
        </p>
      </li>
      </ul>
      </p>

      <theorem xml:id="ChangeTo01"><title>Change a matrix entry to <m>0</m> or <m>1</m> 
      with elementary row operations</title>
      <statement>
      <p>
      <ol>
      <li>
        <p>If a matrix has an entry <m>a_{i,k}\not=0</m>, then, with one elementary
        row operation, it may be changed to <m>1</m>.</p>
      </li>
      <li>
        <p>If some column <m>k</m> of a matrix has two non-zero entries
        <m>a_{i,k}</m> and <m>a_{j,k}</m> then, with one elementary row
        operation, (either) one of them 
        may be changed to <m>0</m>. </p>
      </li>
      </ol>
      </p>
      </statement>
      <proof>
    <p>
    <ol>
      <li>
      <p>If a matrix has an entry <m>a_{i,k}\not=0</m>, then multiply
      the row <m>R_i</m> by <m>\frac 1{a_{i,k}}</m>, that is, carry out
      the elementary row operation <m>R_i\gets \frac 1{a_{i,k}}R_i</m>.
      In the resulting matrix, the <m>i</m>-<m>k</m> entry is
      <m>\frac {a_{i,k}}{a_{i,k}}=1</m>.
      </p>
      </li>
      <li>
      <p>Suppose a matrix has two non-zero entries <m>a_{i,k}</m> and <m>a_{j,k}</m>.
      First, do the elementary row operation 
      <m>R_i\gets \frac 1{a_{i,k}}R_i</m> to change <m>a_{i,k}</m> to <m>1</m>.
      Then, do the elementary row operation 
      <m>R_j\gets R_j-a_{j,k}R_i</m>. The matrix  now has a new value
      in the <m>j</m>-<m>k</m> position which is <m>a_{j,k}-a_{j,k}=0</m>, and we
      have accomplished our goal using two elementary operations.
      Now we note that these two operations can be carried out with
      the single operation 
      <m>
      R_j\gets R_j-\frac {a_{j,k}}{a_{i,k}}R_i
      </m>.
      </p>
      </li>
      </ol>
      </p>
      </proof>
      </theorem>

   <exercises>
   <exercisegroup>
   <introduction>
   <p>
   For these exercises, let
   <m>A=
   \begin{bmatrix}
   1\amp2\amp3\amp4\\  5\amp6\amp7\amp-6\\ -5\amp-4\amp-3\amp-2
   \end{bmatrix}
   </m>.
   </p>
   </introduction>
   <exercise>
   <statement>
   <p>Find the elementary row operation that
   <ul>
   <li><p>changes the <m>2</m> in the first row to a <m>1</m></p></li>
   <li><p>changes the <m>7</m> in the second row to a <m>1</m></p></li>
   <li><p>changes the <m>-4</m> in the third row to a <m>1</m></p></li>
   </ul>
   In each case give the matrix that results from the 
   application of your elementary row operation.</p>
   </statement>
   <solution>
   
   <p>
   <ul>
   <li><m>R_1\gets \frac12 R_1\colon
   \begin{bmatrix}
   \frac12\amp1\amp\frac32\amp\frac12\\  5\amp6\amp7\amp-6\\ -5\amp-4\amp-3\amp-2
   \end{bmatrix}</m>
   </li>
   <li><m>R_2\gets \frac17 R_2\colon
   \begin{bmatrix}
   1\amp2\amp3\amp4\\  \frac57\amp\frac67\amp1\amp-\frac67\\ -5\amp-4\amp-3\amp-2
   \end{bmatrix}
   </m>
   </li>
   <li><m>R_3\gets -\frac14 R_3\colon
   \begin{bmatrix}
   1\amp2\amp3\amp4\\  5\amp6\amp7\amp-6\\ \frac54\amp1\amp\frac34\amp\frac12
   \end{bmatrix}</m>
   </li>
   </ul>
   </p>
   </solution>
   </exercise>
   
   <exercise>
   <statement>
   <p>Find an elementary row operation that
   <ul>
       <li>changes the <m>5</m> in the second row to a <m>0</m></li>
       <li>changes the <m>6</m> in the second row to a <m>0</m></li>
       <li>changes the <m>-4</m> in the third row to a <m>0</m></li>
       <li>changes the <m>7</m> in the second row to a <m>0</m></li>
   </ul>
   In each case give the matrix that results from the 
   application of your elementary row operation.</p>
   </statement>
   <solution>
   <p>
   <ul>
       <li><m>R_2\gets R_2+R_3\colon
       \begin{bmatrix}
       1\amp2\amp3\amp4\\  0\amp2\amp4\amp-8\\ -5\amp-4\amp-3\amp-2
       \end{bmatrix}</m>
       </li>
   
       <li><m>R_2\gets R_2-3R_1\colon
       \begin{bmatrix}
       1\amp2\amp3\amp4\\  2\amp0\amp-2\amp-18\\ -5\amp-4\amp-3\amp-2
       \end{bmatrix}</m>
       </li>
   
       <li><m>R_3\gets R_3+2R_1\colon
       \begin{bmatrix}
       1\amp2\amp3\amp4\\  5\amp6\amp7\amp-6\\ -3\amp0\amp3\amp6
       \end{bmatrix}</m>
       </li>
   
       <li><m>R_2\gets R_2-\frac73 R_1\colon
       \begin{bmatrix}
       1\amp2\amp3\amp4\\  \frac83\amp\frac43\amp0\amp-\frac{46}3\\ -5\amp-4\amp-3\amp-2
       \end{bmatrix}</m>
       </li>
   </ul>
   </p>
   </solution>
   </exercise>
   
    <exercise>
    <statement>
    <p>Use two elementary rwo operations to make the first column
    of the resulting matrix
    <m>
    \begin{bmatrix}
    1\\0\\0
    \end{bmatrix}
    </m></p>
    </statement>
    <solution>
    <p><m>R_2\gets R_2-5R_1</m> and  <m>R_3\gets R_3+5R_1</m> give
    <me>
    \begin{bmatrix}
    1\amp2\amp3\amp4\\
    0\amp-4\amp-8\amp-26\\
    0\amp6\amp12\amp18
    \end{bmatrix}
    </me>
    </p>
    </solution>
    </exercise>

   </exercisegroup>
   </exercises>

   </subsection>

   </section>

    
    <section><title>The Gaussian elimination algorithm</title>
    
        <subsection><title> Some matrices whose associated system of equations
        are easy to solve</title>
        <p>The elementary row operations allow us to change matrices
        and their associated system of linear equations without
        changing the solutions of those equations. The goal is
        to end up with matrices which make these common solutions
        obvious.  Here are some examples.

        <ul> <li><p>The augmented matrix 
        <me>
        \left[\begin{array}{cccc|c} 
        1\amp 0\amp 0\amp 0\amp 1\\ 
        0\amp 1\amp 0\amp 0\amp 2\\
        0\amp 0\amp 1\amp 0\amp 5\\ 
        0\amp 0\amp 0\amp 1\amp -1 
        \end{array}\right] 
        </me> 
        corresponds to the system of linear equations 
        <me>\begin{array}{cccccc} 
        x_1\amp
        \amp \amp \amp =\amp 1\\ 
        \amp x_2\amp \amp \amp =\amp 2\\ 
        \amp \amp x_3\amp \amp =\amp 5\\ 
        \amp \amp \amp x_4\amp =\amp -1
        \end{array} </me> 
        The equations themselves actually describe
        the unique solution! Notice the structure of the coefficient
        matrix that makes this possible. There is only one nonzero
        entry in each row, its value is 1, and as you proceed down
        through from one row to the next, the nonzero entry moves
        one column to the right.</p></li>

        <li><p> The augmented matrix 
        <me> 
        \left[\begin{array}{cc|c}
        1\amp 1\amp 4\\ 0\amp 2\amp 6 
        \end{array}\right]
        </me> 
        corresponds to the system of linear equations
        <me>
        \begin{array}{rcc}
        x_1+x_2\amp =\amp 4\\ 
        2x_2\amp =\amp 6   
        \end{array} 
        </me> 
        The last row is easy to solve:
        we get <m>x_2=3</m>. Using this value, it is also easy to solve
        <m>x_1+x_2=x_1+3=4</m>, or <m>x_1=1</m>.  </p></li>

       <li><p>
        The augmented matrix <me> \left[\begin{array}{ccc|c} 1\amp
        1\amp 1\amp 4\\ 0\amp 0\amp 2\amp 6 \end{array}\right]
        </me> corresponds to the system of linear equations <me>
        \begin{array}{rcc} x_1+x_2+x_3\amp =\amp 4\\ 2x_3\amp =\amp 6
        \end{array} </me> As before, we get <m>x_3=3</m>. We still have two
        variables undefined: we assign a parameter to the second one:
        <m>x_2=t</m>. Using this value, we have <m>x_1+x_2+x_3=x_1+t+3=4</m>,
        or <m>x_1=1-t</m>. We now know the solutions: <m>x_1=1-t</m>, <m>x_2=t</m>,
        <m>x_3=3</m> where <m>t</m> can be any real number. 
        In fact we can check this result with the first
        equation: <m>x_1+x_2+x_3=(1-t) + t + 3=4</m>. The compact way
        of writing this solution is <m>(x_1,x_2,x_3)=(1-t,t,3)</m>.
        </p></li> </ul>

         It's clear from the examples given that having lots of
        zeros in the coefficient matrix is helpful for computing the
        solutions. It's also clear that 
        if the first nonzero entry in a row is one, then
        the computation easier. Our plan is to use elementary row
        operations to change a given coefficient matrix into one with
        these properties, and then to describe all the solutions.
        Here are some observations that will help us:
        <ol>
       <li><p>If a column has some nonzero entry, we can always 
           make the top entry nonzero by interchanging rows, if
            necessary using <m>R_1\leftrightarrow R_i</m>
            for some <m>i>1</m>.</p></li> 
        <li><p>If the first nonzero entry of
            a row <m>R_i</m> is <m>\lambda</m>, we can turn it into a 1 by using
            <m>R_i\leftarrow \tfrac 1\lambda R_i</m>.</p></li> 
        <li><p> If two rows <m>R_i</m> and <m>R_j</m> have nonzero entries 
            in some column <m>k</m>, we can turn the <m>j,k</m> entry into 
            a zero using <m>R_j \leftarrow R_j - \frac{a_{j,k}}{a_{i,k}} R_i</m>.
            </p></li> 
        </ol></p> 

       </subsection>


       <subsection><title>Row echelon form</title>
       <p>
        We now want to define a general class of matrices whose
        corresponding system of linear equations have solutions
        that are easy to find. These matrices have a special pattern
        of zeros and ones, and are said to be in
        <em>row echelon form.</em>
        </p>

        
        <figure>
        <caption>A matrix in row echelon form</caption>
        <image source="images/gausselim.png" width="50%">
        </image>
        </figure>
        
        <p>
        The matrix above gives an idea of what we want.
        Notice the staircase line drawn through the matrix
        has all entries below it equal to zero.
        The entries marked with a <m>*</m> can take on any value.
        </p>
        <p>
        The first nonzero entry in a row (if there is one)
        is called the <em>leading entry.</em>
        If it equals <m>1</m>, then it is called a <em>leading one.</em>
        </p>
        
        <definition><title>Row echelon form</title>
        <statement>
        <p>A matrix is in <term> row echelon form</term> if
        <ul>
        <li><p>Every leading entry is a leading one. </p></li>
        <li><p>Every entry below a leading one is <m>0</m>. </p></li>
        <li><p>As you go down the matrix, the leading ones move to the right.</p></li>
        <li><p>Any all zero rows are at the bottom.</p></li>
        </ul></p>
        </statement>
        </definition>
        
        <exercise>
        <statement>
        <p>Which of the following matrices are in row echelon form?
        <ul>
       
        <li><p><m>
        \begin{bmatrix}
        1\amp2\amp3\amp4\\
        0\amp5\amp6\amp7\\
        0\amp0\amp8\amp9\\
        0\amp0\amp0\amp10
        \end{bmatrix}
        </m></p></li>
        
        <li><p><m>
        \begin{bmatrix}
        1\amp2\amp3\amp4\\
        0\amp1\amp2\amp3\\
        0\amp0\amp0\amp0\\
        0\amp0\amp0\amp1
        \end{bmatrix}
        </m></p></li>
        
        <li><p><m>
        \begin{bmatrix}
        1\amp2\amp3\amp4\\
        0\amp1\amp2\amp3\\
        0\amp0\amp0\amp1\\
        0\amp0\amp0\amp0
        \end{bmatrix}
        </m></p></li>
        
        <li><p><m>
        \begin{bmatrix}
        1\amp2\amp3\amp4\\
        0\amp1\amp6\amp7\\
        0\amp0\amp1\amp1\\
        0\amp0\amp1\amp1
        \end{bmatrix}
        </m></p></li>
        
        <li><p><m>
        \begin{bmatrix}
        1\amp0\amp0\amp0\\
        0\amp0\amp0\amp0\\
        0\amp0\amp0\amp0\\
        0\amp0\amp0\amp0
        \end{bmatrix}
        </m></p></li>
        
        
        </ul></p>
        </statement>
        <solution>
        <p>
        <ul>
        <li><p>Not in row echelon form because not every leading entry is a <m>1</m>. </p></li>
        <li><p>Not in row echelon form because the zero row is not at the bottom. </p></li>
        <li><p>It is row echelon form.</p></li>
        <li><p>Not row echelon form because a leading entry has a 
        nonzero entry below it.</p></li>
        <li><p>It is row echelon form.</p></li>
        </ul></p>
        </solution>
        </exercise>

        </subsection>
    
        <subsection><title>The Gaussian elimination algorithm </title> 
         <p>The plan is now start with the augmented matrix and, by using
         a sequence of elementary row operations, change it to a
         new matrix where it is easy to identify the solutions of
         the associated system of linear
        equations. Since any elementary row operation leaves the
        solutions unchanged, the solutions to the final system of
        linear equations will be identical to the solutions of the
        original one. </p>

        <p>We work on the columns of the matrix from left to
        right and change the matrix in the following way:
        <ol> 
        <li><p>Start with the first column. If it has all
        entries equal to zero, move on to the next column to the
        right.
        </p></li> 
        <li><p>If the column has nonzero entries,
        interchange rows, if necessary, to get a nonzero entry on
        top.
        </p></li> 
        <li>
        <p>
        Change the top entry, if necessary, to make it a <m>1</m>.
        </p>
        </li>
        <li><p>For any nonzero entry below the top one,
        use an elementary row operation to change it to zero.
        </p></li>
        <li><p>Now consider the part of the matrix below the top
        row and to the right of the column under consideration: if
        there are no such rows or columns, stop since the procedure
        is finished.Otherwise, carry out the same procedure on the
        new matrix.
        </p></li> 
        </ol></p> 
        
        <p>Here is a first example:
        <me>
        \begin{array}{r}
        3x_1-2x_2-x_3+x_4=1\\
        6x_1-8x_2+x_3+2x_4=4
        \end{array}
        </me>
        has augmented matrix
        <me>
        \begin{bmatrix}
        3 \amp  -2 \amp  -1 \amp 1 \amp 1 \\
        6 \amp  -8 \amp  1 \amp  2 \amp  4
        \end{bmatrix}.
       </me>
       We don't need to exchange rows to make the top entry of the
       first column nonzero, so we proceed to make the top entry <m>1</m> using
       the elementary row operation
       <m>R_1\gets \frac13R_1</m>. The matrix becomes
        <me>
        \begin{bmatrix}
        1 \amp  -\frac23 \amp  -\frac13 \amp \frac13 \amp \frac13 \\
        6 \amp  -8 \amp  1 \amp  2 \amp  4
        \end{bmatrix}.
       </me>


       Now we must make all entries below the top
       one in the column equal to zero. There is, of course, only one such entry,
       and so,  using <m>R_2\leftarrow R_2-6R_1</m>, we get
       <me>
       \begin{bmatrix}
        1 \amp  -\frac23 \amp  -\frac13 \amp \frac13 \amp \frac13 \\
        0 \amp  -4 \amp  3 \amp  0 \amp  2
       \end{bmatrix}
       </me>.

       We're now done with the first column, so we 
       continue with the same procedure on the matrix obtained by deleting the first
       row and first column:

       <me>
       \begin{bmatrix}
         -4 \amp  3 \amp  0 \amp  2
       \end{bmatrix}.
       </me>

       Since there is only one row, we need only change the top entry to <m>1</m>
       using division by <m>-4</m>, that is, <m>R_1\gets -\frac14R_1</m>. We then get
       <me>
       \begin{bmatrix}
         1 \amp  -\frac34 \amp  0 \amp  -\frac12
       \end{bmatrix}.
       </me>
       and, putting it back into the original matrix, we get
       <me>
       \begin{bmatrix}
        1 \amp  -\frac23 \amp  -\frac13 \amp \frac13 \amp \frac13 \\
          0 \amp  1 \amp  -\frac34 \amp  0 \amp  -\frac12
      \end{bmatrix}
      </me></p>
    
      <p>The matrix is in row echelon form. Now we can determine all of the solutions
      to the original system of linear equations. 
      The first nonzero
      entry in the first row is in the first column, the column associated
      with <m>x_1</m>. The first nonzero entry in the second row, similarly, is
      associated with <m>x_2</m>. We assign parameters to the other variables:
      <m>x_3=s</m> and <m>x_4=t</m>. The second row then tells us that
      <m>x_2-\frac34s =-\frac12 </m>, or <m>x_2=\frac34s-\frac12</m>. Now
      that we know <m>x_2</m>, we can use the first row to find <m>x_1</m>: we get
      <m>x_1-\frac23x_2-\frac13s+\frac13t=\frac13</m>. We substitute in our known
      value for <m>x_2</m> into this equation, and after some
      simplification, we get <m>x_1=\tfrac56s-\tfrac13t</m>.  In summary, we have:
      All solutions to
      <me>
      \begin{array}{r}
      3x_1-2x_2-x_3+x_4=1\\
      6x_1-8x_2+x_3+2x_4=4
      \end{array}
      </me>
      are of the form
      <me>
      \begin{array}{rcl}
      x_1\amp =\amp \tfrac56s-\tfrac13t\\
      x_2\amp =\amp -\tfrac12 +\tfrac34 s\\
      x_3\amp =\amp s\\
      x_4\amp =\amp t
      \end{array}
      </me>
      where <m>s</m> and <m>t</m> are any real numbers. 
      More compactly, we write this as
      <m>(x_1,x_2,x_3,x_4)= \tfrac56s-\tfrac13t,-\tfrac12 +\tfrac34 s,s,t)</m>.
      In other words, for any assignment of real numbers to
      <m>s</m> and <m>t</m>, we get a solution to the system of linear equations.</p>
      
      <p>It is easy (and worthwhile) to check that substituting <m>x_1,\dots,x_4</m>
      into the two equations does indeed give a solution.</p>
    
      <!-- continue here -->
      <p>Now consider another example with the equations
      <md alignment="alignat">
      <mrow>x_1  \amp {}+2x_2   \amp{}-2x_3  \amp{}= \amp{}-1</mrow>
      <mrow>3x_1 \amp {}-  2x_2 \amp{}-4x_3  \amp{}= \amp{}3</mrow>
      <mrow>4x_1 \amp           \amp{}-2x_3 \amp{}= \amp{}-2</mrow>
      <mrow>-x_1 \amp {}-x_2    \amp{}+2x_3  \amp{}= \amp{}0</mrow>
      </md>
      and its corresponding augmented matrix as it is changed 
      by Gaussian elimination:
      <me>
      \begin{bmatrix}
      1\amp 2\amp -2\amp -1 \\
      3\amp -2\amp -4\amp 3\\
      4\amp 0\amp -2\amp -2\\
      -1\amp -1\amp 2\amp 0
      \end{bmatrix}
      \begin{array}{c}
      \\R_2 \leftarrow R_2-3R_1\\ R_3\gets R_3-4R_1\\ R_4\gets R_4+R_1
      \end{array}
      </me>
      <me>\begin{bmatrix}
      1\amp 2\amp -2\amp -1 \\
      0\amp -8\amp 2\amp 6\\
      0\amp -8\amp 6\amp 2\\
      0\amp 1\amp 0\amp -1
      \end{bmatrix}
      \begin{array}{c}
      R_2\gets-\frac18 R_2\\ R_3\gets R_3+8R_2\\R_4\gets R_4-R_2
      \end{array}
      </me>
      <me>
      \begin{bmatrix}
      1\amp 2\amp -2\amp -1 \\
      0\amp 1\amp -\frac14\amp -\frac34\\
      0\amp 0\amp 4\amp -4\\
      0\amp 0\amp \tfrac14 \amp -\tfrac14
      \end{bmatrix}
      \begin{array}{c}
       R_3 \gets \frac14 R_3\\R_4\gets R_4-\frac14 R_3
      \end{array}
      </me>
      <me>
      \begin{bmatrix}
      1\amp 2\amp -2\amp -1 \\
      0\amp 1\amp -\frac14\amp -\frac34\\
      0\amp 0\amp 1\amp -1\\
      0\amp 0\amp 0\amp 0
      \end{bmatrix}
      </me>
      The last row, for any choice of <m>x_1, x_2, x_3</m>, reduces to
      <m>0=0,</m> so any solution of the associated first three equations 
      will also be a solution to the last one. In other words, 
      the last row of the matrix has no effect
      on the solution set and may be dropped from the matrix. The third row gives
      <m>x_3=-1.</m> The second row gives <m>x_2=-1</m> 
      and the first row gives <m>x_1=-1.</m>
      Hence there is one solution: <m>(x_1,x_2,x_3)=(-1,-1,-1).</m>
      </p>
    
      <p>
      Now change the equations from the last example very slightly 
      (the right-hand side of the last equation is changed from <m>0</m> to <m>1</m>):
      <md alignment="alignat">
      <mrow>x_1  \amp {}+2x_2   \amp{}-2x_3  \amp{}= \amp{}-1</mrow>
      <mrow>3x_1 \amp {}-  2x_2 \amp{}-4x_3  \amp{}= \amp{}3</mrow>
      <mrow>4x_1 \amp           \amp{}-2x_3 \amp{}= \amp{}-2</mrow>
      <mrow>-x_1 \amp {}-x_2    \amp{}+2x_3  \amp{}= \amp{}1</mrow>
      </md>
      The Gaussian elimination is almost identical as
      <me>
      \begin{bmatrix}
      1\amp 2\amp -2\amp -1 \\
      3\amp -2\amp -4\amp 3\\
      4\amp 0\amp -2\amp -2\\
      -1\amp -1\amp 2\amp 1
      \end{bmatrix}
      </me>
      is reduced to
      <me>
      \begin{bmatrix}
      1\amp 2\amp -2\amp -1 \\
      0\amp 1\amp -\frac14\amp -\frac34\\
      0\amp 0\amp 1\amp -1\\
      0\amp 0\amp 0\amp 1

      \end{bmatrix}
      </me>
      </p>
    
      <p>Now the last row says <m>0x_1+0x_2+0x_3=1</m>, which, for any choice of
      <m>x_1</m>, <m>x_2</m> and <m>x_3</m>, reduces to <m>0=1</m>
      and is never true. This means the original system of equations
      have no solutions, that is, the system is inconsistent.
      </p>
    
      <p>We can make a useful observation here: 
      If a row of the augmented matrix is of the form
      <me>
      \begin{bmatrix}
      0 \amp 0 \amp 0 \amp \cdots \amp 0 \amp *
      \end{bmatrix}
      </me>
      where <m>*</m> is either zero or nonzero, then one of two things happens:
      <ol>
      <li><p><m>*=0</m> in which case the row may be dropped from the matrix</p></li>
      <li><p><m>*\not=0</m> in which case there is no solution.</p></li>
      </ol>
      </p>
    
      <example>
      <p>Consider the system of linear equations:
      <me>
      \begin{array}{rcl}
      x+y+z\amp =\amp  1\\
      2x+y+z \amp =\amp  2\\
      3x+ay+bz\amp =\amp c
      \end{array}
      </me>
      We wish to know the values of <m>a,</m> <m>b</m> and <m>c</m> for which there 
      are there no solutions, one solution or more than one solution.
      To solve this problem, we apply Gaussian elimination to the augmented matrix: 
      
      <me>
      \begin{bmatrix} 
        1\amp 1\amp 1\amp 1\\
        2\amp 1\amp 1\amp 2\\
        3\amp a\amp b\amp c 
      \end{bmatrix}
      \begin{matrix}R_2\gets R_2-2R_1\\ R_3\gets R_3-3 R_1\end{matrix}
      </me> 
      <me>
      \begin{bmatrix} 1\amp 1\amp 1\amp 1\\0\amp -1\amp -1\amp 0\\0\amp a-3\amp b-3\amp c-3 \end{bmatrix}
      \begin{matrix}R_2\gets -R_2\\R_3\gets R_3-(a-3)R_2\end{matrix}
      </me>
      <me>
      \begin{bmatrix} 1\amp 1\amp 1\amp 1\\0\amp 1\amp 1\amp 0\\0\amp 0\amp b-a\amp c-3 \end{bmatrix}
      </me>

      An analysis of the last row tells us everything: If <m>b-a\not=0,</m>
      then there is exactly one solution. If <m>b-a=0,</m> and <m>c-3\not=0,</m> 
      then there are no solutions. Otherwise (when
      <m>b=a</m> and <m>c=3</m>) there are an infinite number of solutions. 
      </p>
      </example>

      <exercises>
      <exercise>
      <statement>
      <p>
        Find all solutions to the system of equations
        <md>
        <mrow>x+2y-z \amp=2</mrow>
        <mrow>x+y-z\amp=0</mrow>
        <mrow>2x-y+z\amp=3</mrow>
        </md>
      </p>
      </statement>
      <solution>
      <p>We put the augemented matrix into row echelon form:
        <me>
        \begin{bmatrix}
        1 \amp 2 \amp -1 \amp 2\\
        1 \amp 1 \amp -1 \amp 0\\
        2 \amp -1 \amp 1 \amp 3\\
        \end{bmatrix}
        \\
        \begin{array}{l}
        R_2\gets R_2 - R_1\\
        R_3\gets R_3-2R_1
        \end{array}
        \\
        \begin{bmatrix}
        1 \amp 2 \amp -1 \amp 2\\
        0 \amp -1 \amp 0 \amp -2\\
        0 \amp -5 \amp 3 \amp -1\\
        \end{bmatrix}
        \\
        \begin{array}{l}
        R_2\gets -R_2\\
        \end{array}
        \\
        \begin{bmatrix}
        1 \amp 2 \amp -1 \amp 2\\
        0 \amp 1 \amp 0 \amp 2\\
        0 \amp -5 \amp 3 \amp -1\\
        \end{bmatrix}
        \\
        R_3\gets R_3+5R_2
        \\
        \begin{bmatrix}
        1 \amp 2 \amp -1 \amp 2\\
        0 \amp 1 \amp 0 \amp 2\\
        0 \amp 0 \amp 3 \amp 9\\
        \end{bmatrix}
        \\
        R_3\gets \frac13 R_3
        \\
        \begin{bmatrix}
        1 \amp 2 \amp -1 \amp 2\\
        0 \amp 1 \amp 0 \amp 2\\
        0 \amp 0 \amp 1 \amp 3\\
        \end{bmatrix}
        </me>
        The last row gives <m>z=3</m>. The second row gives <m>y=2</m>. The first row gives
        <m>x=2-2y+z=2-4+3=1</m>, and so there is a unique solution <m>(x,y,z)=(1,2,3)</m>.
      </p>
      </solution>
      </exercise>
      </exercises>
      </subsection>
    </section>
    
    
    <section><title>Gauss-Jordan reduction</title>
    <introduction>
     <p>Gauss-Jordan reduction is an extension of the Gaussian elimination algorithm.
      It produces a matrix, called <em>the reduced row echelon form</em> in the following
      way: after carrying out Gaussian elimination, continue by changing
      all nonzero entries <em>above</em> the leading ones to a zero. The resulting
      matrix looks something like:
      </p>
      
        
        <figure>
        <caption/>
        <image source="images/rref.png" width="50%">
        <description>A matrix in row echelon form</description>
        </image>
        </figure>
        
        <p>
        The matrix above gives an idea of what we want.
        Notice the staircase line drawn through the matrix
        has all entries below it equal to zero.
        The entries marked with a <m>*</m> can take on any value.
        </p>

        <definition><title>Reduced row echelon form</title>
        <statement>
        <p>A matrix is in <term>reduced row echelon form</term>  if
        <ul>
        <li><p>Every leading entry is a leading one. </p></li>
        <li><p>Every entry below <em>and above</em> a leading one is <m>0</m>. </p></li>
        <li><p>As you go down the matrix, the leading ones move to the right.</p></li>
        <li><p>Any all zero rows are at the bottom.</p></li>
        </ul></p>
        </statement>
        </definition>

      <p>

      For completeness, we'll describe Gauss-Jordan reduction algorithm in detail. As with Gaussian 
      elimination, the columns of the matrix are processed from left to right.
      
      <ol>
      <li><p>Start with the first column. If it has all entries equal to zero, 
      move on to the next column to the right.</p></li>
      <li><p>If, to the contrary, the column has nonzero entries, interchange rows, 
      if necessary, to get a nonzero entry on top.</p></li>
      <li><p>Multiply the top row by a constant to change the nonzero entry to a (leading) one.</p></li>
      <li><p>If there are nonzero entries above or below this (leading) one, 
      use an elementary row operation on each one to change it to a zero.</p></li>
      <li><p>Now consider the part of the matrix below the top row and to the right of the column 
      under consideration: if there are no such rows or columns, 
      stop and the algorithm is finished. Otherwise, carry out the same procedure on the 
      new matrix.</p></li>
      </ol>
      </p>
      
      <example><title>Putting a matrix in reduced row echelon form</title>
      <p>
      <m>
      \begin{align}
      \amp \begin{bmatrix}
      1 \amp  2 \amp  3 \\
      4 \amp  5 \amp  6 \\
      7 \amp  8 \amp  9 \\
      10 \amp  12 \amp  15 
      \end{bmatrix} \amp \amp 
      \begin{array}{l}
      R_2\gets R_2-4R_1\\
      R_3\gets R_3-7R_1\\
      R_4\gets R_4-10R_1
      \end{array}\\[6pt]

      \amp \begin{bmatrix}
      1 \amp  2 \amp  3 \\
      0 \amp  -3 \amp  -6 \\
      0 \amp  -6 \amp  -12 \\
      0 \amp  -8 \amp  -15
      \end{bmatrix} \amp \amp 
      \begin{array}{l}
      R_2\gets -\tfrac13 R_2
      \end{array}\\[6pt]

      \amp \begin{bmatrix}
      1 \amp  2 \amp  3 \\
      0 \amp  1 \amp  2 \\
      0 \amp  -6 \amp  -12 \\
      0 \amp  -8 \amp  -15
      \end{bmatrix} \amp \amp 
      \begin{array}{l}
      R_1\gets R_1-2R_2\\
      R_3\gets R_3+6R_2\\
      R_4\gets R_4+8R_2
      \end{array}\\[6pt]

      \amp \begin{bmatrix}
      1 \amp  0 \amp  -1 \\
      0 \amp  1 \amp  2 \\
      0 \amp  0 \amp  0 \\
      0 \amp  0 \amp  1
      \end{bmatrix} \amp \amp 
      \begin{array}{l}
      R_3\leftrightarrow R_4 \\
      \end{array}\\[6pt]

      \amp \begin{bmatrix}
      1 \amp  0 \amp  -1 \\
      0 \amp  1 \amp  2 \\
      0 \amp  0 \amp  1 \\
      0 \amp  0 \amp  0
      \end{bmatrix} \amp \amp 
      \begin{array}{l}
      R_1 \gets R_1+R_3 \\
      R_2\gets R_2-2R_3
      \end{array}\\[6pt]

      \amp \begin{bmatrix}
      1 \amp  0 \amp  0 \\
      0 \amp  1 \amp  0 \\
      0 \amp  0 \amp  1 \\
      0 \amp  0 \amp  0
      \end{bmatrix}
      \end{align} 
      </m>
      </p>
      </example>
     
     <example><title>Putting another matrix in reduced row echelon form</title>
     <p>
     <m>
      \begin{align}
      \amp \begin{bmatrix}
      1 \amp  2 \amp  6 \amp  1 \amp  4 \amp  6 \\
      2 \amp  4 \amp  9 \amp  2 \amp  8 \amp  9 \\
      1 \amp  2 \amp  9 \amp  2 \amp  10 \amp  9
      \end{bmatrix}\amp \amp 
      \begin{array}{c}
      R_2 \gets R_2-2R_1 \\
      R_3 \gets R_3-R_1
      \end{array}\\[6pt]

      \amp \begin{bmatrix}
      1 \amp  2 \amp  6 \amp  1 \amp  4 \amp  6 \\
      0 \amp  0 \amp  -3 \amp  0 \amp  0 \amp  -3 \\
      0 \amp  0 \amp  3 \amp  1 \amp  6 \amp  3
      \end{bmatrix}\amp \amp 
      \begin{array}{c}
      R_2 \gets -\tfrac13R_2
      \end{array}\\[6pt]

      \amp \begin{bmatrix}
      1 \amp  2 \amp  6 \amp  1 \amp  4 \amp  6 \\
      0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  1 \\
      0 \amp  0 \amp  3 \amp  1 \amp  6 \amp  3
      \end{bmatrix}\amp \amp 
      \begin{array}{c}
      R_1 \gets R_1 -6R_2\\
      R_3 \gets R_3 -3R_2
      \end{array}\\[6pt]

      \amp \begin{bmatrix}
      1 \amp  2 \amp  0 \amp  1 \amp  4 \amp  0 \\
      0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  1 \\
      0 \amp  0 \amp  0 \amp  1 \amp  6 \amp  0
      \end{bmatrix}\amp \amp 
      \begin{array}{c}
      R_1 \gets R_1 - R_3
      \end{array}\\[6pt]

      \amp \begin{bmatrix}
      1 \amp  2 \amp  0 \amp  0 \amp  -2 \amp  0 \\
      0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  1 \\
      0 \amp  0 \amp  0 \amp  1 \amp  6 \amp  0
      \end{bmatrix}
      \end{align}
     </m>
     </p> 
     </example>
      
      <p>
      Notice the pattern of zero, 1 and nonzero entries after Gauss-Jordan reduction
      (with the leading ones in red):
      <me>
      \left[\begin{array}{ccccccc}
      {{\color{red}1}} \amp  0 \amp  0 \amp  * \amp  0 \amp  \cdots \amp  *\\
      0 \amp  {\color{red}1} \amp  0 \amp  * \amp  0 \amp  \cdots \amp  *\\
      0 \amp  0 \amp  {\color{red}1}\amp  * \amp  0 \amp  \cdots \amp  *\\
      0 \amp  0 \amp  0 \amp  0 \amp  {\color{red}1} \amp  \cdots \amp  *\\
      \end{array}\right]
      </me>

      Now that we can put a matrix in reduced row echelon form, let's see what this implies for 
      finding all solutions to the associated system of linear equations. Remember that the 
      first <m>n</m> columns correspond to coefficients of variables 
      <m>x_1,x_2,\dots,x_n</m>, and the last column corresponds to the constants on
      the right side of the equations. Each column either contains a leading one or it does not.
      If it does, the corresponding variable is called <term>constrained</term> or
      <term>basic</term>. 
      If not, the variable is called <term>free.</term> 
      Each free variable is assigned a parameter which may take on any number
      when finding solutions. The values of the constrained variables are then determined by
      the reduced row echelon form.
      </p>
      
      <p>
     As an example, suppose we have a system of linear equations whose augmented matrix has
      the following reduced row echelon form:
      
      <me>
      \begin{bmatrix}
      {\color{red}1} \amp  2 \amp  0 \amp  1 \amp  0 \amp  1\\
      0 \amp  0 \amp   {\color{red}1}\amp  2 \amp  0 \amp  2\\
      0 \amp  0 \amp  0 \amp  0 \amp  {\color{red}1} \amp  3
      \end{bmatrix}
      </me>
      
      Notice that this means that our system has three equations and five unknowns.
      The leading ones are in columns one, three and five, so <m>x_1</m>, 
      <m>x_3</m> and <m>x_5</m> are constrained variables. This leaves
      <m>x_2</m> and <m>x_4</m> as free variables. We assign parameters
      to the free variables: <m>x_2=s</m> and <m>x_4=t</m>. The rows of
      the matrix then determine the constrained variables: 
      
     <ul>
     <li><p><m>x_5=3</m> from the bottom row</p></li> 
     <li><p><m>x_3 = 2-2t</m> from the middle row</p></li> 
     <li><p><m>x_1=1-2s-t</m> from the top row</p></li> 
     </ul>
     </p>
      
     <p>
      The compact way of writing this is 
      <m>(x_1,x_2,x_3,x_4,x_5) = (1-2s-t,s,2-2t,t,3).</m>
     </p>
      
     <p>
     Notice the role of the zeros above and below each leading one: the evaluation of a
      constrained variable involves only free variables.
     </p>
      
     <p>
      In summary, we can say the following:
     <ol>
     <li><p>
         If the reduced row echelon form has a row of the form <m>[0,0,\dots,0,1]</m>, 
        then the system of linear equations has no solution.</p></li>
     <li><p>
        If the reduced row echelon form has no free variables, then it looks like:
        <me>
        \begin{bmatrix}
        1 \amp  0 \amp  0 \amp \cdots \amp 0 \amp 0 \amp c_1\\
        0 \amp  1 \amp  0 \amp \cdots \amp 0 \amp 0 \amp c_2\\
        0 \amp  0 \amp  1 \amp \cdots \amp 0 \amp 0 \amp c_3\\
            \amp    \amp    \amp \vdots \\
        0 \amp  0 \amp  0 \amp \cdots \amp 1 \amp 0 \amp c_{n-1}\\
        0 \amp  0 \amp  0 \amp \cdots \amp 0 \amp 1 \amp c_n
        \end{bmatrix}
        </me>
        and there is a unique solution, namely, <m>x_1=c_1, x_2=c_2,\dots x_n=c_n</m>.
     </p></li>
     <li><p>
        If the reduced row echelon form has free variables, then there are an infinite 
        number of solutions. Indeed, the parameter assigned to any one free variable can take 
        on an infinite number of values.
     </p></li>
     </ol> 
      
      Consider the following system of linear equations:
      <me>
      \begin{array}{rl}
      x_1-x_2+2x_3-x_4 \amp =  -1 \\
      2x_1+x_2-2x_3-2x_4 \amp =  -2 \\
      -x_1+2x_2-4x_3+x_4 \amp =  1 \\
      3x_1-3x_4 \amp =  -3
      \end{array}
      </me>


    The augmented matrix is then put into reduced row echelon form:

      <me>\begin{bmatrix}
      1 \amp  -1 \amp  2 \amp  -1 \amp  -1\\
      2 \amp  1 \amp -2 \amp  -2 \amp  -2\\
      -1 \amp  2 \amp  -4 \amp  1 \amp  1\\
      3 \amp  0 \amp  0 \amp  -3 \amp  -3
      \end{bmatrix}</me>

      <me>\begin{array}{rl}
      R_2 \amp \gets R_2-2R_1\\
      R_3 \amp \gets R_3+R_1\\
      R_4 \amp \gets R_4-3R_1
      \end{array}</me>
      </p>
      
      <p>
     <me>\begin{bmatrix}
      1 \amp  -1 \amp  2 \amp  -1 \amp  -1\\
      0 \amp  3 \amp  -6 \amp  0 \amp  0\\
      0 \amp  1 \amp  -2 \amp  0 \amp  0\\
      0 \amp  3 \amp  -6 \amp  0 \amp  0
      \end{bmatrix}</me>

      <me>\begin{array}{rl}
      R_2 \amp \gets \tfrac13 R_2\\
      \end{array}</me>

      <me>\begin{bmatrix}
      1 \amp  -1 \amp  2 \amp  -1 \amp  -1\\
      0 \amp  1 \amp  -2 \amp  0 \amp  0\\
      0 \amp  1 \amp  -2 \amp  0 \amp  0\\
      0 \amp  3 \amp  -6 \amp  0 \amp  0
      \end{bmatrix}</me>
      
      <me>\begin{array}{rl}
      R_1 \amp \gets R_1 + R_2\\
      R_3 \amp \gets R_3- R_2\\
      R_4 \amp \gets R_4- 3R_2
      \end{array}</me> 
      
      <me>\begin{bmatrix}
      {\color{red} 1} \amp  0 \amp  0 \amp  -1 \amp  -1\\
      0 \amp  {\color{red} 1} \amp  -2 \amp  0 \amp  0\\
      0 \amp  0 \amp  0 \amp  0 \amp  0\\
      0 \amp  0 \amp  0 \amp  0 \amp  0
      \end{bmatrix}</me>
      </p>
      
      <p>
     Since  <m>x_3</m> and <m>x_4</m> are free variables,
      we assign them parameters: <m>x_3=s</m> and <m>x_4=t</m>.  We can 
      now evaluate the constrained variables:
      <m>x_1=-1+t</m> (from the first row) and <m>x_2=2s</m> (from the second row).
      In short, <m>(x_1,x_2,x_3,x_4)=(-1+t,2s,s,t)</m> for any choice of <m>s</m> or <m>t</m>. 
      </p>
      
     <p>
       Now, as a check, let's put these solutions back into the original equations.

      <m>\begin{array}{rlll}
      x_1-x_2+2x_3-x_4 \amp =\amp  (-1+t) - (2s) + 2(s) - (t) \amp= -1\\
      2x_1+x_2-2x_3-2x_4 \amp =\amp  2(-1+t) + 2s -2s-2t \amp= -2\\
      -x_1+2x_2-4x_3+x_4 \amp =\amp  -(-1+t) +2(2s) -4s+t \amp= 1 \\
      3x_1-3x_4 \amp =\amp  3(-1+t) -3t \amp= -3
      \end{array}</m> 
     </p>

    <example>
    <title>A matrix row-reducing itself</title>
        <p>
        The following example is a matrix row reducing itself. Notice how it
        proceeds left to right in columns, and each column with a leading one
        has zeros above and below when the algorithm completes.
        </p>
    </example>

    <figure>
    <caption>A matrix row reduces itself</caption>
    <image width="100%" source="images/rref.gif" />
    </figure>
    
</introduction>

    <subsection><title>The rank of a matrix</title>
    <definition>
    <statement>
    <p>
        The <term>rank</term> of a matrix is the number of leading ones in the reduced row
        echelon form.
    </p>
    </statement>
    </definition>

    <p>
    Since the matrix
    <me>A=
    \begin{bmatrix}
    1\amp 2\amp 3\amp 4\amp 5 \\
    6\amp 7\amp 8\amp 9\amp 10 \\
    11\amp 12\amp 13\amp 14\amp 15 \\
    \end{bmatrix}
    </me>
    has a reduced row echelon form of
    <me>
    \begin{bmatrix}
    1\amp 0\amp -1\amp -2\amp -3 \\
    0\amp 1\amp 2\amp 3\amp 4 \\
    0\amp 0\amp 0\amp 0\amp 0 \\
    \end{bmatrix}
    </me>
    the rank of <m>A</m> is <m>2</m>.
    </p>

    </subsection>

      <exercises>

      <exercisegroup>
      <introduction><p>Put the following matrices into reduced row echelon
      form.</p></introduction>
       
            <exercise> <statement> <p>
            <me>
            \begin{bmatrix}
            2\amp4\amp6\amp8\\
            3\amp6\amp9\amp10
            \end{bmatrix}
            </me>
            </p> </statement> <solution> <p> 
            <me>
            \begin{bmatrix}
            2\amp4\amp6\amp8\\
            3\amp6\amp9\amp10
            \end{bmatrix}\\
            \rowmul1{\frac12}\\
            \begin{bmatrix}
            1\amp2\amp3\amp4\\
            3\amp6\amp9\amp10
            \end{bmatrix}\\
            \rowsub231\\
            \begin{bmatrix}
            1\amp2\amp3\amp4\\
            0\amp0\amp0\amp-2
            \end{bmatrix}\\
            \rowmul 2{-\frac12}\\
            \begin{bmatrix}
            1\amp2\amp3\amp4\\
            0\amp0\amp0\amp1
            \end{bmatrix}\\
            \rowsub142\\
            \begin{bmatrix}
            1\amp2\amp3\amp0\\
            0\amp0\amp0\amp1
            \end{bmatrix}
            </me>
            </p></solution> </exercise>
        
            <exercise> <statement> <p>
            <me>
            \begin{bmatrix}
            0 \amp  1 \amp -1\\
            1 \amp  0 \amp -1\\
            1 \amp -1 \amp  0
            \end{bmatrix}
            </me>
            </p> </statement> <solution> <p> 
            <me>
            \begin{bmatrix}
            0 \amp  1 \amp -1\\
            1 \amp  0 \amp -1\\
            1 \amp -1 \amp  0
            \end{bmatrix}\\
            \rowint 12\\
            \begin{bmatrix}
            1 \amp  0 \amp -1\\
            0 \amp  1 \amp -1\\
            1 \amp -1 \amp  0
            \end{bmatrix}\\
            \rowsub 3{}1\\
            \begin{bmatrix}
            1 \amp  0 \amp -1\\
            0 \amp  1 \amp -1\\
            0 \amp -1 \amp  1
            \end{bmatrix}\\
            \rowadd3{}2\\
            \begin{bmatrix}
            1 \amp  0 \amp -1\\
            0 \amp  1 \amp -1\\
            0 \amp  0 \amp  0
            \end{bmatrix}\\
            </me>
            </p></solution> </exercise>
            
            <exercise> <statement> <p>
            <me>\begin{bmatrix}
            1\amp 2\amp3\\
            4\amp5\amp6\\
            7\amp8\amp9
            \end{bmatrix}</me>
            </p> </statement> <solution> <p> 
            <me>\begin{bmatrix}
            1\amp 2\amp3\\
            4\amp5\amp6\\
            7\amp8\amp9
            \end{bmatrix}\\
            \rowsub241\\
            \rowsub371\\
            \begin{bmatrix}
            1\amp 2\amp3\\
            0\amp-3\amp-6\\
            0\amp-6\amp-12
            \end{bmatrix}\\
            \rowmul2{-\frac13}\\
            \begin{bmatrix}
            1\amp 2\amp3\\
            0\amp1\amp2\\
            0\amp-6\amp-12
            \end{bmatrix}\\
            \rowsub 122\\
            \begin{bmatrix}
            1\amp 0\amp-1\\
            0\amp1\amp2\\
            0\amp-6\amp-12
            \end{bmatrix}\\
            \rowadd 362\\
            \begin{bmatrix}
            1\amp 0\amp-1\\
            0\amp1\amp2\\
            0\amp0\amp0
            \end{bmatrix}</me>
            </p></solution> </exercise>
        
            <exercise> <statement> <p>
            <me>
            \begin{bmatrix}
            1\amp2\\ 1\amp3 \\1\amp4
            \end{bmatrix}
            </me>
            </p> </statement> <solution> <p> 
            <me>
            \begin{bmatrix}
            1\amp2\\ 1\amp3 \\1\amp4
            \end{bmatrix}\\
            \rowsub 2 {} 1\\
            \rowsub 3 {} 1\\
            \begin{bmatrix}
            1\amp2\\ 0\amp1 \\0\amp2
            \end{bmatrix}\\
            \rowsub 1 2 2\\
            \rowsub 3 2 2\\
            \begin{bmatrix}
            1\amp0\\ 0\amp1 \\0\amp0
            \end{bmatrix}\\
            </me>
            </p></solution> </exercise>
        
            <exercise> <statement> <p>
            <me>
            \begin{bmatrix}
            0\amp 1\amp 2\amp 3\amp 4 \\
            4\amp 3\amp 2\amp 1\amp 0 
            \end{bmatrix}
            </me>
            </p> </statement> <solution> <p> 
            <me>
            \begin{bmatrix}
            0\amp 1\amp 2\amp 3\amp 4 \\
            4\amp 3\amp 2\amp 1\amp 0 
            \end{bmatrix}\\
            \rowint12\\
            \begin{bmatrix}
            4\amp 3\amp 2\amp 1\amp 0 \\
            0\amp 1\amp 2\amp 3\amp 4 
            \end{bmatrix}\\
            \rowmul1{\frac14}\\
            \begin{bmatrix}
            1\amp \frac34\amp \frac12\amp \frac14\amp 0 \\
            0\amp 1\amp 2\amp 3\amp 4 
            \end{bmatrix}\\
            \rowsub 1{\frac34}2\\
            \begin{bmatrix}
            1\amp 0\amp -1\amp -2\amp -3 \\
            0\amp 1\amp 2\amp 3\amp 4 
            \end{bmatrix}
            </me>
            </p></solution> </exercise>
            </exercisegroup>

            <exercisegroup>
                <introduction>
                <p>Consider the system of four linear equations in four unknowns:
                <md>
                <mrow>a_{1,1}x_1+a_{1,2}x_2+a_{1,3}x_3+a_{1,4}x_4=b_1</mrow>
                <mrow>a_{2,1}x_1+a_{2,2}x_2+a_{2,3}x_3+a_{2,4}x_4=b_2</mrow>
                <mrow>a_{3,1}x_1+a_{2,2}x_2+a_{3,3}x_3+a_{3,4}x_4=b_3</mrow>
                <mrow>a_{4,1}x_1+a_{4,2}x_2+a_{4,3}x_3+a_{4,4}x_4=b_4</mrow>
                </md>
                In each case, suppose that the given matrix is the reduced
                row echelon form of the augmented matrix. Find all of the solutions.
                to the original system of linear equations.
                </p>
                </introduction>
            <exercise>
            <statement>
            <p>
            <m>
            \begin{bmatrix}
            1\amp 0 \amp 0 \amp 0 \amp 1 \\
            0\amp 1 \amp 0 \amp 0 \amp -1 \\
            0\amp 0 \amp 1 \amp 0 \amp 2 \\
            0\amp 0 \amp 0 \amp 1 \amp 3 \\
            \end{bmatrix}
            </m>
            </p></statement>
            <solution><p> 
            <m>x_1=1</m>, <m>x_2=-1</m>, <m>x_3=2</m> and <m>x_4=3</m> is the unique solution. 
            </p></solution>
            </exercise>
            <exercise>
            <statement>
            <p>
            <m>
            \begin{bmatrix}
            1\amp 0 \amp 0 \amp 0 \amp 1 \\
            0\amp 1 \amp 0 \amp 0 \amp -1 \\
            0\amp 0 \amp 1 \amp 3 \amp 2 \\
            0\amp 0 \amp 0 \amp 0 \amp 1 \\
            \end{bmatrix}
            </m>
            </p>
            </statement>
            <solution>
            <p>
            The last line satisfies the condition for there to be no solution.
            </p>
            </solution>
            </exercise>
        
            <exercise> <statement> <p>
            <m>
            \begin{bmatrix}
            1\amp 0 \amp 0 \amp 1 \amp 1 \\
            0\amp 1 \amp 0 \amp -1 \amp -1 \\
            0\amp 0 \amp 1 \amp 3 \amp 2 \\
            0\amp 0 \amp 0 \amp 0 \amp 0 \\
            \end{bmatrix}
            </m>
            </p> </statement> 
            <solution> 
            <p><m>x_4=t</m> since it is a free variable.</p> 
            <p>From row 1, we have <m>x_1=1-t</m></p> 
            <p>From row 2, we have <m>x_2=-1+t</m></p> 
            <p>From row 3, we have <m>x_3=2-3t</m></p> 
            <p>
            In other words, <m>(x_1,x_2,x_3,x_4)=(1-t,-1+t,2-3t,t)</m>.
            </p>
            </solution></exercise>
        
            <exercise> <statement> <p>
            <m>
            \begin{bmatrix}
            1\amp -2 \amp 0 \amp 3 \amp -1 \\
            0\amp 0 \amp 1 \amp 1 \amp 2 \\
            0\amp 0 \amp 0 \amp 0 \amp 0 \\
            0\amp 0 \amp 0 \amp 0 \amp 0 \\
            \end{bmatrix}
            </m>
            </p> </statement> 
            <solution> 
            <p><m>x_2=t</m> and <m>x_4=u</m> since they are free variables.</p> 
            <p>From row 1, we have <m>x_1=-1+2t-3u</m></p> 
            <p>From row 2, we have <m>x_3=2-u</m></p> 
            <p>
            In other words, <m>(x_1,x_2,x_3,x_4)=(-1+2t-3u,t,2-u,u)</m>.
            </p>
            </solution></exercise>
            </exercisegroup>

       <exercisegroup>
       <introduction>
        <p>Find <em>all</em> solutions to the following 
        systems of linear equations
        </p>
       </introduction>
      <exercise>
      <statement>
        <p>
        <md>
        <mrow>x+2y-z =2</mrow>
        <mrow>x+y-z=0</mrow>
        <mrow>2x-y+z=3</mrow>
        </md>
        </p>
      </statement>
      <solution>
      <p>We put the augemented matrix into reduced row echelon form:
        <m>
        \begin{bmatrix}
        1 \amp 2 \amp -1 \amp 2\\
        1 \amp 1 \amp -1 \amp 0\\
        2 \amp -1 \amp 1 \amp 3\\
        \end{bmatrix}
        \\
        \begin{array}{l}
        R_2\gets R_2 - R_1\\
        R_3\gets R_3-2R_1
        \end{array}
        \\
        \begin{bmatrix}
        1 \amp 2 \amp -1 \amp 2\\
        0 \amp -1 \amp 0 \amp -2\\
        0 \amp -5 \amp 3 \amp -1\\
        \end{bmatrix}
        \\
        \begin{array}{l}
        R_2\gets -R_2\\
        \end{array}
        \\
        \begin{bmatrix}
        1 \amp 2 \amp -1 \amp 2\\
        0 \amp 1 \amp 0 \amp 2\\
        0 \amp -5 \amp 3 \amp -1\\
        \end{bmatrix}
        \\
        R_3\gets R_3-5R_2
        \\
        \begin{bmatrix}
        1 \amp 2 \amp -1 \amp 2\\
        0 \amp 1 \amp 0 \amp 2\\
        0 \amp 0 \amp 3 \amp 9\\
        \end{bmatrix}
        \\
        R_3\gets \frac13 R_3
        \\
        \begin{bmatrix}
        1 \amp 2 \amp -1 \amp 2\\
        0 \amp 1 \amp 0 \amp 2\\
        0 \amp 0 \amp 1 \amp 3\\
        \end{bmatrix} \\
        R_1 \gets R_1-2R_2
        \\
        \begin{bmatrix}
        1 \amp 0 \amp -1 \amp-2\\
        0 \amp 1 \amp 0 \amp 2\\
        0 \amp 0 \amp 1 \amp 3\\
        \end{bmatrix} \\
        R_1 \gets R_1+R_3
        \\
        \begin{bmatrix}
        1 \amp 0 \amp 0 \amp1\\
        0 \amp 1 \amp 0 \amp 2\\
        0 \amp 0 \amp 1 \amp 3\\
        \end{bmatrix} \\
        </m>
        The first, second and third rows give us, respectively,
        <me>
        \begin{matrix}
        x=1\\y=2\\z=3
        \end{matrix}
        </me>
      </p>
      </solution>
      </exercise>

      <exercise> <statement> 
      <p>
      <md>
      <mrow>x+2y+z=0</mrow>
      <mrow>x+y+2z=5</mrow>
      <mrow>-x+y+z=0</mrow>
      </md>
      </p>
      </statement> <solution> <p> 
      <m>
      \begin{bmatrix}
      1 \amp 2 \amp 1 \amp 0\\
      1 \amp 1 \amp 2 \amp 5\\
      -1 \amp 1 \amp 1 \amp 0
      \end{bmatrix}\\
      \rowsub 211\\
      \rowadd 3{}1\\
      \begin{bmatrix}
      1 \amp 2 \amp 1 \amp 0\\
      0 \amp -1 \amp 1 \amp 5\\
      0 \amp 3 \amp 2 \amp 0
      \end{bmatrix}\\
      \rowmul 2{-}\\
      \begin{bmatrix}
      1 \amp 2 \amp 1 \amp 0\\
      0 \amp 1 \amp -1 \amp -5\\
      0 \amp 3 \amp 2 \amp 0
      \end{bmatrix}\\
      \rowsub122\\
      \rowsub332\\
      \begin{bmatrix}
      1 \amp 0 \amp 3 \amp 10\\
      0 \amp 1 \amp -1 \amp -5\\
      0 \amp 0 \amp 5 \amp 15
      \end{bmatrix}\\
      \rowmul3{\frac15}\\
      \begin{bmatrix}
      1 \amp 0 \amp 3 \amp 10\\
      0 \amp 1 \amp -1 \amp -5\\
      0 \amp 0 \amp 1 \amp 3
      \end{bmatrix}\\
      \rowsub 133\\
      \rowadd 3{}2\\
      \begin{bmatrix}
      1 \amp 0 \amp 0 \amp 1\\
      0 \amp 1 \amp 0 \amp -2\\
      0 \amp 0 \amp 1 \amp 3
      \end{bmatrix}\\
      </m>
      and so the unique solution is <m>(x,y,z)=(1,-2,3)</m>. 
      </p>
      </solution> 
      </exercise>

      <exercise> <statement>
      <p>
      <md>
      <mrow>x+y-z-w=2</mrow>
      <mrow>x-y-z+w=4</mrow>
      <mrow>2x+y-z+w=1</mrow>
      </md>
      </p>
      </statement> <solution> <p> 
      <m>
      \begin{bmatrix} 
      1 \amp 1 \amp -1 \amp -1 \amp 2\\
      1 \amp -1 \amp -1 \amp 1 \amp 4\\
      2 \amp 1 \amp -1 \amp 1 \amp 1
      \end{bmatrix} \\
      \rowsub 2{}1\\
      \rowsub 321\\
      \begin{bmatrix} 
      1 \amp 1 \amp -1 \amp -1 \amp 2\\
      0 \amp -2 \amp 0 \amp 2 \amp 2\\
      0 \amp -1 \amp 1 \amp 3 \amp -3
      \end{bmatrix} \\
      \rowmul2{-\frac12}\\
      \begin{bmatrix} 
      1 \amp 1 \amp -1 \amp -1 \amp 2\\
      0 \amp 1 \amp 0 \amp -1 \amp -1\\
      0 \amp -1 \amp 1 \amp 3 \amp -3
      \end{bmatrix} \\
      \rowsub1{}2\\
      \rowadd3{}2\\
      \begin{bmatrix} 
      1 \amp 0 \amp -1 \amp 0 \amp 3\\
      0 \amp 1 \amp 0 \amp -1 \amp -1\\
      0 \amp 0 \amp 1 \amp 2 \amp -4
      \end{bmatrix} \\
      \rowadd1{}3\\
      \begin{bmatrix} 
      1 \amp 0 \amp 0 \amp 2 \amp -1\\
      0 \amp 1 \amp 0 \amp -1 \amp -1\\
      0 \amp 0 \amp 1 \amp 2 \amp -4
      \end{bmatrix} 
      </m>
      and so <m>w</m> is a free variable that can be set equal to <m>t</m>. This gives
      <m>(x,y,z,w)=(-1-2t, -1+t, -4-2t,t)</m>.
      </p></solution> </exercise>
  
      <exercise> <statement>
      <p>
      <md>
      <mrow>x-y=0</mrow>
      <mrow>y-z=0</mrow>
      <mrow>x-z=0</mrow>
      </md>
      
      Clearly, if <m>x=y</m> and <m>y=z</m>, then we have a solution
      to this system. Show that every solution is of this form.
      </p> </statement> <solution> <p> 
      We put the augmented matrix in reduced row echelon form:
      <m>
      \begin{bmatrix}
      1 \amp -1 \amp 0 \amp 0\\
      0 \amp  1 \amp -1 \amp 0\\
      1 \amp  0 \amp -1 \amp 0\\
      \end{bmatrix}\\
      \rowsub3{}1\\
      \begin{bmatrix}
      1 \amp -1 \amp 0 \amp 0\\
      0 \amp  1 \amp -1 \amp 0\\
      0 \amp  1\amp -1 \amp 0\\
      \end{bmatrix}\\
      \rowadd1{}2\\
      \begin{bmatrix}
      1 \amp 0\amp -1 \amp 0\\
      0 \amp  1 \amp -1 \amp 0\\
      0 \amp  1\amp -1 \amp 0\\
      \end{bmatrix}\\
      \rowsub3{}2\\
      \begin{bmatrix}
      1 \amp  0 \amp -1 \amp 0\\
      0 \amp  1 \amp -1 \amp 0\\
      0 \amp  0\amp 0 \amp 0\\
      \end{bmatrix}
      </m>
      We then have that <m>z</m> is a free variable that may be set to <m>t</m>.
      All solutions are then of the form <m>(x,y,z)=(t,t,t)</m>, which
      implies that <m>x=y</m> and <m>y=z</m>.
      </p></solution> </exercise>
        
       </exercisegroup> 
  
      <exercise> <statement> <p>
      Consider the two systems of linear equations:
      
      <md>
      <mrow>3x+y-2z\amp=2 \amp x+y-z\amp=1</mrow>
      <mrow>-x+3y-z\amp=1 \amp -3x+y+z\amp=-1</mrow>
      </md>
   
      Show that every solution of the first set is a solution of
      the second set, and vice-versa.
      </p> </statement> 
      <hint><p>Compare the RREF of the corresponding augmented matrices. </p></hint>  
      
      <solution> <p> 
      Each system has a RREF of
      <m>
      \begin{bmatrix}
      1 \amp 0 \amp -\frac12 \amp \frac12\\
      0 \amp 1 \amp -\frac12 \amp \frac12\\
      \end{bmatrix}
      </m>
      and so all the solutions for either system consist of
      <m>(x,y,z)=(\frac12+\frac12t,\frac12+\frac12t,t)</m> where <m>t</m> is any
      real number.
      </p></solution> </exercise>
  
      </exercises>

    </section>


    
    <section><title>Geometric examples of three equations in three unknowns</title>
      
      <p>
      A single equation in three unknowns can be interpreted geometrically
      in 3-dimensional space. An equation
      <me>
      Ax+By+Cz=D
      </me>
      has solutions that form a plane as long as at least one of <m>A, B, C</m> is
      nonzero.
      If we look at a system of such linear equations, the solution set is
      the set of all points lying in all of the corresponding planes. 
      </p>
      
      
      <example><title>Three planes intersecting at a point</title>
      <p>
      Consider the system of linear equations
      <me>
      \begin{array}{rl}
      x+y+z \amp= 2\\
      x+2y+z\amp=3\\
      x+2y-3z \amp=2
      \end{array}
      </me>
      The augmented matrix is
      
      <me>
      \begin{bmatrix}
      1\amp1\amp1\amp2\\
      1\amp2\amp1\amp3\\
      1\amp2\amp-3\amp2
      \end{bmatrix}
      </me>
      whose reduced row echelon form is 
      <me>
      \begin{bmatrix}
      1\amp0\amp0\amp\frac34\\
      0\amp1\amp0\amp1\\
      0\amp0\amp1\amp\frac14
      \end{bmatrix}
      </me>
      
      This means that there is a single solution: <m>(x,y,z)=(\frac34,1,\frac14)</m>.
      Here are the three planes corresponding to the three equations:
      </p>
      
      <figure>
      <caption> Three planes intersecting at a single point</caption>
      <image width="100%" xml:id="ThreePlanesPoint">
      <asymptote>
      import graph3;
      unitsize(4cm);
      
      currentprojection=perspective(11,2,-9);
      currentprojection=perspective(14.5,3.2,1.1);
      //currentprojection=oblique;
      
      real A,B,C,D;
      real f(real x, real y) { return (D-A*x-B*y)/C;} // equation of plane
      triple F(real x, real y) {return (x,y,f(x,y));} // point above (x,y)
      
      A=1; B=1; C=1; D=2;
      triple u=F(2,0), v=F(0,2), w=F(0.5,0.5);
      path3 g1=plane(u-w,v-w,w);
      draw(surface(g1),paleblue,nolight);
      draw(g1,linewidth(0.85));
      label("\(x+y+z=2\)",F(0,2.0),NE,blue);
      
      A=1; B=2; C=1; D=3;
      triple u=F(2,0), v=F(0,2), w=F(0.5,0.5);
      path3 g2=plane(u-w,v-w,w);
      draw(surface(g2),lightyellow,nolight);
      draw(g2,linewidth(0.75));
      label("\(x+2y+z=3\)",F(0,2),E,olive);
      
      A=1; B=2; C=-3; D=3;
      triple u=F(2,0), v=F(0,2), w=F(0.5,0.5);
      path3 g3=plane(u-w,v-w,w);
      draw(surface(g3),lightgreen,nolight);
      draw(g3,linewidth(0.75));
      label("\(x+2y-3z=3\)",F(0,2),NE,heavygreen);
      
      triple [] s1,s2,s3;
      
      s1=intersectionpoints(g1,g2);
      draw(s1[0]--s1[1],linewidth(1));
      s2=intersectionpoints(g1,g3);
      draw(s2[0]--s2[1],linewidth(1));
      s3=intersectionpoints(g2,g3);
      draw(s3[0]--s3[1],linewidth(1));
      
      triple t;
      t=intersectionpoint(s1[0]--s1[1],s2[0]--s2[1]);
      dot(t,red+linewidth(5.75));

      triple u=(3,2.5,-0.2);
      draw(u--t, Arrow3,Margin3(1,2));
      label("$(x,y,z)=(\frac34,1,\frac14)$",u,E);
      axes3("\(x\)","\(y\)","\(z\)",(0,0,0),(4,3,2));
      </asymptote>
      </image>
      </figure>
      </example>
      
      <example><title>Three planes intersecting in a single line</title>
      <p>
      Consider the system of linear equations
      <me>
      \begin{array}{rl}
      x+y-z \amp= 3\\
      x+y+z\amp=1\\
      2x+2y \amp=4
      \end{array}
      </me>
      The augmented matrix is
      
      <me>
      \begin{bmatrix}
      1\amp1\amp-1\amp3\\
      1\amp1\amp1\amp1\\
      2\amp2\amp0\amp4
      \end{bmatrix}
      </me>
      whose reduced row echelon form is
      <me>
      \begin{bmatrix}
      1\amp1\amp0\amp2\\
      0\amp0\amp1\amp-1\\
      0\amp0\amp0\amp0
      \end{bmatrix}
      </me>
      This means <m>y=t</m>, <m>x=2-t</m> and <m>z=-1</m> 
      so that <m>(x,y,z)=(2-t,t,-1)</m> is a solution
      for any real number <m>t</m>.
      </p>

      <figure>
      <caption>Three planes intersecting in a line</caption>
      <image width="100%" xml:id="ThreePlanesLine">
      <asymptote>
      import graph3;
      size(400);
      currentprojection=orthographic(99,25,20);
      currentlight=White;
      defaultpen(1); // Change default pen width to 1bp 
      
      triple dirvec=1.5(-1,1,0);
      path3 Plane(triple a, triple b) {
         return a--b--b+dirvec--a+dirvec--cycle;
         }
      
      triple a=(1,-1,1), b=(3,1,-3);
      path3 g=Plane(a,b);
      draw(surface(g),grey,nolight);
      draw(g);
      label("\(x+y+z=1\)",b,S,deepgrey);
      
      triple a=(3,1,1), b=(1,-1,-3);
      path3 g=Plane(a,b);
      draw(surface(g),paleblue,nolight);
      draw(g);
      label("\(x+y-z=3\)",b,S,deepblue);
      
      triple a=(2,0,1), b=(2,0,-3);
      path3 g=Plane(a,b);
      draw(surface(g),lightgreen,nolight);
      draw(g);
      label("\(x+y=2\)",b,S,deepgreen);
      
      draw((2,0,-1)-0.5*dirvec--(2,0,-1)+1.5*dirvec,red+linewidth(1));
      
      axes3("\(x\)","\(y\)","\(z\)",(-3,-2,-2),(5,2,2));
      </asymptote>
      </image>
      </figure>
      </example>
      
      <example><title>Three intersecting planes with no solutions</title>
      <p>
      Consider the system of linear equations
      <me>
      \begin{array}{rl}
      2x-y+z \amp= 1\\
      x+y+z\amp=2\\
      4x+y+3z \amp=3
      \end{array}
      </me>
      The augmented matrix is
      <me>
      \begin{bmatrix}
      2\amp-1\amp1\amp1\\
      1\amp1\amp1\amp2\\
      4\amp1\amp3\amp3
      \end{bmatrix}
      </me>
      whose reduced row echelon form is
      <me>
      \begin{bmatrix}
      1\amp0\amp\frac23\amp0\\
      0\amp1\amp\frac13\amp0\\
      0\amp0\amp0\amp1
      \end{bmatrix}
      </me>
      The last row indicates that there is no solution.
      </p>

      <figure>
      <caption>Three pairwise intersecting planes with no common point</caption>
      <image width="90%" xml:id="ThreePlanesNoSolution">
      <asymptote>
      import graph3;
      unitsize(120);
      currentprojection=orthographic(7.25,3.36,-9.7);
      currentlight=White;
      defaultpen(1); // Change default pen width to 1bp  
      
      triple P1=(6/7,3/7,-2/7), P2=(1,1,0), P3=(3/7,12/7,-1/7);
      triple dirvec=(2,1,-3);
      real t=1.25;
      triple Q1=P1+t*dirvec, Q2=P2+t*dirvec, Q3=P3+t*dirvec;
      
      real eps=0.15, p=1+eps, q=-eps;
      path3 q4=(p*P1+q*P2)--(p*P2+q*P1)--(p*Q2+q*Q1)--(p*Q1+q*Q2)--cycle;
      draw(q4);
      draw(surface(q4),heavygrey,nolight);
      path3 q5=(p*P1+q*P3)--(p*P3+q*P1)--(p*Q3+q*Q1)--(p*Q1+q*Q3)--cycle;
      draw(surface(q5),green,nolight);
      draw(q5);
      path3 q6=(p*P2+q*P3)--(p*P3+q*P2)--(p*Q3+q*Q2)--(p*Q2+q*Q3)--cycle;
      draw(surface(q6),lightblue,nolight);
      draw(q6);
      
      draw(P1--Q1);
      draw(P2--Q2);
      draw(P3--Q3);
      
      label("\(2x-y+z=1\)",P2--P3,NE,blue);
      label("\(x+y+z=2\)",P1--P2,NW,heavygrey);
      label("\(4x+y+3z=3\)",Q1--Q3,2SE,deepgreen);
      //axes3("\(x\)","\(y\)","\(z\)",(0,0,0),(1,1,1.1));
      </asymptote>
      </image> 
      </figure> 
      </example>

      <example><title>Three nonintersecting planes with no
      solutions</title> <p> Consider the system of linear equations <me>
      \begin{array}{rl} 2x-y+z \amp= 1\\ 2x-y+z \amp= 2\\ 2x-y+z \amp=3
      \end{array} </me> The augmented matrix is

      <me> \begin{bmatrix} 2\amp-1\amp1\amp1\\ 2\amp-1\amp1\amp2\\
      2\amp-1\amp1\amp3 \end{bmatrix} </me> whose reduced row echelon
      form is <me> \begin{bmatrix} 1\amp -\frac12\amp\frac12\amp0\\
      0\amp0\amp0\amp1\\ 0\amp0\amp0\amp0\\ \end{bmatrix} </me> The
      middle row indicates that there is no solution.  </p>

      <figure>
      <caption>Three parallel planes</caption>
      <image width="100%" xml:id="ThreeParallelPlanes">
      <asymptote>
      import graph3;
      size(300);
      currentprojection=orthographic(-6.2,-4.3,9.7);
      currentlight=White;
      
      real A,B,C=1,D;
      real f(real x, real y) { return (D-A*x-B*y)/C;} // equation of plane
      triple F(real x, real y) {return (x,y,f(x,y));} // point above (x,y)

      A=2; B=-1; C=1; D=1;
      path3 g1=plane(F(1,0)-F(-2,-2),F(-1,0)-F(-2,-2),F(-2,-2));
      draw(surface(g1),lightblue,nolight);
      draw(g1);
      label("\(2x-y+z=1\)",F(-2,-2),E,deepblue);

      A=2; B=-1; C=1; D=2;
      path3 g2=plane(F(1,0)-F(-2,-2),F(-1,0)-F(-2,-2),F(-2,-2));
      draw(g2);
      draw(surface(g2),grey,nolight);
      label("\(2x-y+z=2\)",F(-2,-2),E,deepgrey);
      
      A=2; B=-1; C=1; D=3;
      path3 g3=plane(F(1,0)-F(-2,-2),F(-1,0)-F(-2,-2),F(-2,-2));
      draw(g3);
      draw(surface(g3),green,nolight);
      label("\(2x-y+z=3\)",F(-2,-2),E,deepgreen);
      
      axes3("\(x\)","\(y\)","\(z\)",(-1,-1,-1),(1.5,1,4));
      </asymptote>
      </image>
      </figure>
      </example>

      </section>
</chapter>


<chapter><title>Matrix theory</title>
    
    <section><title>First definitions</title>
    <p> We start our study of matrix theory with some important definitions: </p>

    <definition><title>Matrix</title>
    <statement>
    <p>A <term> matrix</term> (the plural is <term>matrices</term>) is a rectangular array of numbers. </p>
    </statement>
    </definition>

    <p>Here is a matrix:
    <me>
    \begin{bmatrix}
    1\amp2\amp-1\amp4\\
    2\amp1\amp3\amp0\\
    4\amp4\amp3\amp1
    \end{bmatrix}
    </me>
    The <term>rows</term> are horizontal parts of the array and the <term>columns</term> are 
    the vertical ones. We often denote the rows by <m>R_1, R_2,\dots</m>
    and the columns by <m>C_1, C_2,\dots</m>.</p>
    
    <p>In the previous example we have three rows and four columns:</p>
    
    <p>Rows: 
    <m>R_1=\begin{bmatrix}1\amp2\amp-1\amp4\end{bmatrix}</m>, 
    <m>R_2=\begin{bmatrix}2\amp1\amp3\amp0\end{bmatrix}</m> and 
    <m>R_3=\begin{bmatrix}4\amp4\amp3\amp1\end{bmatrix}</m> 
    </p>
    
    <p>Columns:
    <m>C_1=\begin{bmatrix}1\amp2\amp4\end{bmatrix}</m>, 
    <m>C_2=\begin{bmatrix}2\amp1\amp4\end{bmatrix}</m>, 
    <m>C_3=\begin{bmatrix}-1\amp3\amp3\end{bmatrix}</m> and  
    <m>C_4=\begin{bmatrix}4\amp0\amp1\end{bmatrix}</m>.
    </p>
    
    <p>Sometimes the columns are written vertically:
    <me>C_1=\begin{bmatrix}1\\2\\4\end{bmatrix}, 
    C_2=\begin{bmatrix}2\\1\\4\end{bmatrix}, 
    C_3=\begin{bmatrix}-1\\3\\3\end{bmatrix} \textrm{and } 
    C_4=\begin{bmatrix}4\\0\\1\end{bmatrix}.</me>
    </p>
    
    
    <definition><title> Size of a matrix</title>
    <statement>
    <p>
    If a matrix had <m>m</m> rows and <m>n</m> columns, then we say that the
    matrix has <term>size</term> <m>m\times n</m>. We call this an <m>m</m> by <m>n</m> matrix.
    When it is desirable to emphasize the size of a matrix <m>A</m>, 
    the following notation is used:
    <me>
    A=
    \begin{bmatrix}
    a_{1,1} \amp \cdots \amp a_{1,n}\\
    \amp\vdots\amp\\
    a_{m,1} \amp \cdots \amp a_{m,n}
    \end{bmatrix}_{m,n}
    </me>
    Notice the special notation used: <m>a_{i,j}</m> is the one and only entry in the matrix
    that is both in the <m>i</m>-th row and in the <m>j</m>-th column. 
    </p>
    <p>
    Sometimes <m>m\times n</m> is called the <term>shape</term> of the matrix.
    </p>
    </statement>
    </definition>
    
    
    <definition xml:id="Square_matrix_definition"><title>Square matrix</title>
    <statement>
    <p>An <m>m\times n</m> matrix is called <term>square</term> if <m>m=n.</m>
    To emphasize the size of a square matrix, a single subscript is may be
    used: 
    <me>
    A=
    \begin{bmatrix}
    a_{1,1} \amp \cdots \amp a_{1,n}\\
    \amp\vdots\amp\\
    a_{n,1} \amp \cdots \amp a_{n,n}
    \end{bmatrix}_n
    </me>
    </p>
    </statement>
    </definition>
    
    <definition><title>Equality of matrices</title>
    <statement>
    <p>Two matrices <m>A=[a_{i,j}]</m> and <m>B = [b_{i,j}]</m> 
    are <term>equal</term> if they are the same size, say <m>m</m> by <m>n,</m>
    and
    <me>
    a_{i,j}=b_{i,j}, \text{ for } i=1,2,\dots,m \text{ and } j=1,2,\dots,n.
    </me>
    In other words, they matrices are equal entry-wise.</p>
    </statement>
    </definition>
    
    <example><title>Equal matrices</title>
    <p>
    <ol>
       <li>
       <p>As a first example, consider
       <me>
       \begin{bmatrix}
       1 \amp 2 \amp 1\\
       1 \amp -1 \amp 0
       \end{bmatrix}
       =
       \begin{bmatrix}
       5-4 \amp 12/6 \amp \cos(0)\\
       10^0 \amp (-1)^5 \amp \sin(4\pi)
       \end{bmatrix}
       </me>
    
        There are six entries in these matrices. We need to check all six to establish equality;
        they are 
       <md>
       <mrow>1\amp=5-4</mrow>
       <mrow>2\amp=\frac{12}6</mrow>
       <mrow>1\amp=\cos(0)</mrow>
       <mrow>1\amp=10^0</mrow>
       <mrow>-1\amp=(-1)^5</mrow>
       <mrow>0\amp=\sin(4\pi)</mrow>
       </md>
       Since all of the individual equations are valid, the two matrices are equal.
       </p>
       </li>
    
       <li><p> Matrix equality can encode interesting information.
            <me>\begin{bmatrix}
            x+y \amp x+2y\\
            2x+y \amp 2x+2y
            \end{bmatrix}
            =
            \begin{bmatrix}
            2 \amp 3 \\
            3 \amp 4
            \end{bmatrix}
            </me> 
            (note that this is the same as four equations in two unknowns; they imply <m>x=y=1</m>)
            </p>
        </li>
    </ol>
    </p>
    </example>
    
    <p>
    On the face of it, in order to show that two <m>m\times n</m> matrices are equal, we must
    check the equaltiy of all <m>mn</m> entries. One reason we study mathematics is to make
    calculations easier. Our mathematical development will, in fact, 
    do just that.
    </p>
    
    <p>
    There are two matrices that appear so often that they have special names:
    </p>
    
    <definition> <title>The zero matrix</title>
    <statement>
    <p>
    The <term>zero matrix</term>, written as <m>\vec0</m>, has every entry equal to <m>0</m>.
    <me>
    \vec0=
    \begin{bmatrix}
    0 \amp 0\amp \cdots \amp 0\\
    0 \amp 0\amp \cdots \amp 0\\
     \amp \amp \vdots \amp \\
    0 \amp 0\amp \cdots \amp 0
    \end{bmatrix}
    </me>
    </p>
    </statement>
    </definition>
    
    <definition><title>The identity matrix</title>
    <statement>
    <p>
    The <term>identity matrix</term>, written as <m>I</m>, is a square matrix 
    in which every entry equal to <m>0</m> or <m>1</m>.
    <me>
    I=
    \begin{bmatrix}
    1 \amp 0\amp \cdots \amp 0 \amp 0\\
    0 \amp 1\amp \cdots \amp 0 \amp 0\\
     \amp \amp \vdots \amp \\
    0 \amp 0\amp \cdots \amp 1 \amp 0\\
    0 \amp 0\amp \cdots \amp 0 \amp 1
    \end{bmatrix}
    </me>
    When we want to emphasize the size of the matrix is <m>n\times n</m>, we write it as <m>I_n</m>.
    Hence, if <m>I_n=[a_{i,j}]</m>, we have
    <me>
    a_{i,j}=
    \begin{cases}
    1 \amp \textrm{if } i=j\\
    0 \amp \textrm{if } i\not=j
    \end{cases}
    </me>
    </p>
    </statement>
    </definition>
    
    </section>
    
    <section><title>Addition and subtraction of matrices</title>

        <subsection><title>Definitions of addition and subtraction of matrices</title>
        <p>
        For two matrices <m>A=[a_{i,j}]</m> and <m>B = [b_{i,j}]</m>, addition is defined if and
        only if the matrices have the same size. In that case, we say that the matrix 
        <m>C = [c_{i,j}]</m> satisfies <m>C=A+B</m> if and only if
        <me>
        c_{i,j} = a_{i,j}+b_{i,j}
        </me>
        for all <m>1\leq i\leq m</m> and <m>1\leq j\leq n</m>.
        </p>

        <p>
        Similarly, for two matrices <m>A</m> and <m>B</m> of the same size, <m>C=A-B</m> is defined by 
        <me>
        c_{i,j} = a_{i,j}-b_{i,j} 
        </me>
        for all <m>1\leq i\leq m</m> and <m>1\leq j\leq n</m>.
        When two matrices are of the same size, and hence their addition
        is defined, they are called
        <term>conformable for addition.</term>
        </p>
        
        <example><title>Addition and subtraction of matrices</title>
        <p>
        <me>
        A=
        \begin{bmatrix}
        1 \amp 2 \amp 3\\
        4 \amp 5 \amp 6
        \end{bmatrix}
        \text{ and } 
        B=
        \begin{bmatrix}
        5 \amp 3 \amp 1\\
        0 \amp -1 \amp -2
        \end{bmatrix}
        </me>
        then
        <me>
        A+B=
        \begin{bmatrix}
        6 \amp 5 \amp 4\\
        4 \amp 4 \amp 4
        \end{bmatrix}
        </me>
        and
        <me>
        A-B=
        \begin{bmatrix}
        -4 \amp -1 \amp 2\\
        4 \amp 6 \amp 8
        \end{bmatrix}
        </me>
        </p>
        </example>
        
        
        <p>In short, addition and subtraction of two matrices are carried 
        out by adding or subtracting the corresponding positions within the matrices.</p>
        </subsection>
        
        
        <subsection><title> Some properties of addition of matrices </title> 
        
        <theorem xml:id="MatrixAdditionProperties"><title>Addition properties of Matrices</title>
        <statement>
        <p>
        Suppose <m>A</m>, <m>B</m> and <m>C</m> are matrices of the same size, then
        <ul>
           <li><p> If <m>A</m> and <m>B</m> are <m>m\times n</m> matrices, 
               then so is <m>A+B</m>.</p></li>
           <li><p> <m>A+B=B+A\qquad</m> (commutativity of addition) </p></li>
           <li><p><m>(A+B)+C = A+(B+C)\qquad</m> (associativity of addition)</p></li>
        </ul>
        </p>
        </statement>
        <proof>
        <p>
        <ul>
           <li><p>By the definition of matrix addition, the sum of two matrices is a matrix of the same size. </p></li>
           <li><p> We use <m>A=[a_{i,j}]</m> and <m>B=[b_{i,j}]</m>.
               The <m>i</m>-<m>j</m> entry of <m>A+B</m> is <m>a_{i,j}+b_{i,j}</m> while
               the <m>i</m>-<m>j</m> entry of <m>B+A</m> is <m>b_{i,j}+a_{i,j}</m>.
               Hence <m>A+B=B+A</m> means 
               <m>a_{i,j}+b_{i,j}=b_{i,j}+a_{i,j}</m> for each possible <m>i</m> and <m>j</m>.
               We know this latter equation is valid since it uses the known commutative
               property of real numbers.
               (see properties of real numbers in <xref ref="RealNumberProperties" />.)
               </p> 
           </li>
           <li>
           <p>
           The <m>i</m>-<m>j</m> entries of <m>(A+B)+C</m> and <m>A+(B+C)</m> must
           be equal. This says <m>(a_{i,j}+b_{i,j})+c_{i,j}=a_{i,j}+(b_{i,j}+c_{i,j})</m>
           for all possible <m>i</m> and <m>j</m>, and this equation is valid by the distributive
           property of real numbers.
           </p>
           </li>
        </ul>
        </p>
        </proof>
        </theorem> 
              
        </subsection>
    </section>

    <section><title>Scalar multiplication</title>
    <definition><title>Scalars</title>
    <statement>
    <p>A <term>scalar</term> is a real number.</p>
    </statement>
    </definition>
    
    <p>We next define the multiplication of a scalar and a matrix.</p>
    <definition><title>Scalar multiplication</title>
    <statement>
    <p>
    If <m>A=[a_{i,j}]</m> is a matrix and <m>r</m>
    is a scalar, then the matrix <m>C=[c_{i,j}]=rA</m> is defined by
    <me>
    c_{i,j}=ra_{i,j}
    </me>
    In other words, every entry of the matrix <m>C</m> is multiplied by <m>r</m>.
    </p> 
    </statement>
    </definition>
    
    <example><title>Scalar multiplication of a matrix</title>
    <p>
    If
    <me>
    A=
    \begin{bmatrix}
    1 \amp 2 \amp 3\\
    4 \amp 5 \amp 6
    \end{bmatrix}
    \text{ and }
    r=2
    </me>
    then 
    <me>
    rA=2A=
    \begin{bmatrix}
    2 \amp 4 \amp 6\\
    8 \amp 10 \amp 12
    \end{bmatrix}.
    </me>
    </p>
    </example>
    <p>In short, the product <m>rA</m> is computed by multiplying every entry of <m>A</m> by <m>r</m>.</p>
    
    
    <theorem xml:id="ScalarMultiplicationProperties"><title>Properties of scalar multiplication</title>
    <statement>
    <p>Suppose that <m>A</m> and <m>B</m> are matrices of the same size, and <m>r</m> and <m>s</m> are scalars,
    then
    <ul>
        <li><p> If <m>A</m> is an <m>m\times n</m> matrix, then <m>rA</m> is also <m>m\times n</m>. </p></li>
        <li><p><m>r(A+B)=rA+rB</m></p></li>
        <li><p><m>(r+s)A=rA+sA</m></p></li>
        <li><p><m>(rs)A=r(sA)</m> </p></li>
        <li><p><m>1A=A</m> </p></li>
    </ul>
    </p>
    </statement>
    <proof>
    <p>
    <ul>
        <li><p>
            By the definition of scalar multiplication, if <m>A</m> is an <m>m\times n</m> matrix,
            then <m>rA</m> is an <m>m\times n</m> matrix also.
        </p></li>
        <li><p>
            We use <m>A=[a_{i,j}]</m> and <m>B=[b_{i,j}]</m>.
            Then the <m>i</m>-<m>j</m> entry 
            of <m>r(A+B)</m> is <m>r(a_{i,j}+b_{i,j})</m>
            while he <m>i</m>-<m>j</m> entry 
            of <m>rA+rB</m> is <m>ra_{i,j}+rb_{i,j}</m>.
            Hence the for the equality to be valid, we need
            <m>r(a_{i,j}+b_{i,j})=ra_{i,j}+rb_{i,j}</m>,
            which is the distributive law for real numbers.
            (see properties of real numbers in <xref ref="RealNumberProperties" />.)
        </p></li>
        <li><p>
            We use <m>A=[a_{i,j}].</m>
            The <m>i</m>-<m>j</m> entry of <m>(r+s)A</m>
            is <m>(r+s)a_{i,j}</m> while the
            <m>i</m>-<m>j</m> entry of <m>rA+sA</m> is
            <m>ra_{i,j}+sa_{i,j}</m>. Hence the matrix equation is valid
            if <m>(r+s)a_{i,j}=ra_{i,j}+sa_{i,j}</m>. Since this equation
            is the distributive law for real numbers, the validity is clear.
        </p></li>
        <li><p> 
            We use <m>A=[a_{i,j}].</m>
            The <m>i</m>-<m>j</m> entry of
            <m>(rs)A</m> is  <m>(rs)a_{i,j}</m>
            while the <m>i</m>-<m>j</m> entry of
            <m>r(sA)</m> is <m>r(sa_{i,j}).</m>
            Hence the matrix equation is valid if
            <m>(rs)a_{i,j}=r(sa_{i,j}).</m> Since this
            is the associative law for real numbers,
            the result is clear.
        </p></li>
        <li><p> We use <m>A=[a_{i,j}].</m>
            The <m>i</m>-<m>j</m> entry of <m>1A</m> is <m>1a_{i,j}=a_{i,j}</m>,
            and so <m>1A=A.</m> 
        </p></li>
    </ul>
    </p>
    </proof>
    </theorem>
    
    
    <paragraphs><title>Linear combinations</title>
        <p>
        Matrix addition and scalar multiplication are both used to 
        compute linear combinations.
        </p>
    </paragraphs>
    
    <example><title>A linear combination</title>
        <p>
        Suppose 
        <m>A=\begin{bmatrix} 1\amp 2\\3\amp 4 \end{bmatrix}</m>
        and
        <m>B=\begin{bmatrix} -1\amp 0\\2\amp 1 \end{bmatrix}</m>.
        Then the expression <m>2A+3B</m> makes sense and can
        be evaluated as
        <me>
        \begin{array}{rl}
        2A+3B
        \amp =2 \begin{bmatrix} 1\amp 2\\3\amp 4 \end{bmatrix}
        +3\begin{bmatrix} -1\amp 0\\2\amp 1 \end{bmatrix}\\
        \amp =\begin{bmatrix} 2\amp 4\\6\amp 8 \end{bmatrix}
        +\begin{bmatrix} -3\amp 0\\6\amp 3 \end{bmatrix}\\
        \amp =\begin{bmatrix} -1\amp 4\\ 12\amp 11 \end{bmatrix}
        \end{array}
        </me>.
        </p>
    </example>
    
    <definition><title>Linear combination of two matrices</title>
    <statement>
        <p>
        If <m>A</m> and <m>B</m> are matrices conformable for addition, and
        <m>r</m> and <m>s</m> are scalars, then the matrix of the form
        \[
        rA+sB
        \]
        is called a <term>linear combination of <m>A</m> and <m>B</m>.</term>
        </p>
    </statement>
    </definition>

    <p>
    The concept can be applied easily to more than two matrices.
    </p>
    
    <definition xml:id="LinearCombinationDefinition"><title>Linear combination of matrices</title>
    <statement>
        <p>
        If <m>A_1,A_2,\ldots,A_n</m> are matrices conformable for addition,
        then, for any choice of scalars <m>r_1,r_2,\ldots,r_n</m>, the matrix 
        \[
        r_1A_1+r_2A_2+\cdots+r_nA_n
        \]
        is called a <term>linear combination of <m>A_1,A_2,\ldots,A_n</m></term>.
        </p>
    </statement>
    </definition>
    
    <exercises><title>Scalar multiplication exercises</title>
    <exercise>
    <statement>
        <p>
        Let 
        <m>
        A=
        \begin{bmatrix}
        1\amp2 \amp3\\ 3\amp2\amp1
        \end{bmatrix}
        </m>
        Evaluate
        <ul>
        <li><p><m>2A</m></p></li>
        <li><p><m>-2A</m></p></li>
        <li><p><m>\frac12 A</m></p></li>
        <li><p><m>(-1) A</m></p></li>
        <li><p><m>0A</m></p></li>
        </ul>
        </p>
    </statement>
    
    <solution>
        <p>
        <ul>
        <li><p><m>2A=
            \begin{bmatrix}
            2\amp 4\amp 6\\ 6\amp 4\amp 2
            \end{bmatrix}
            </m></p></li>
        <li><p><m>-2A=
            \begin{bmatrix}
            -2\amp -4\amp -6\\ -6\amp -4\amp -2
            \end{bmatrix}
            </m></p></li>
        <li><p><m>\frac12 A=
            \begin{bmatrix}
            \frac12 \amp 1\amp \frac32\\ \frac32 \amp 1\amp \frac12
            \end{bmatrix}
            </m></p></li>
        <li><p><m>(-1) A=
        \begin{bmatrix}
        -1\amp-2 \amp-3\\ -3\amp-2\amp-1
        \end{bmatrix}
            </m></p></li>
        <li><p><m>0A=
        \begin{bmatrix}
        0\amp0 \amp0\\ 0\amp0\amp0
        \end{bmatrix}
            </m></p></li>
        </ul>
        </p>
    </solution>
    </exercise>
    
    <exercise>
    <statement>
        <p>
        Recall the that zero matrix <m>\vec0</m> has every
        entry equal to zero.
        Show that <m>A+(-1)A=\vec 0</m>.
        </p>
    </statement>
    <solution>
        <p>
        The <m>i</m>-<m>j</m> entry of <m>A+(-1)A</m> is
        <m>a_{i,j}+(-1)a_{i,j}=0</m> and so <m>A+(-1)A=\vec 0</m>.
        </p>
    </solution>
    </exercise>
    
    <exercise>
    <statement>
        <p>
        Let <m>A</m> be a any matrix and <m>r</m> any scalar.
        Show that <m>0A=r\vec 0</m> where <m>\vec 0</m> is
        the zero matrix with the same size as <m>A</m>.
        </p>
    </statement>
    <solution>
        <p>
        The <m>i</m>-<m>j</m> entry of <m>0A</m> is <m>0a_{i,j}=0</m>
        and
        the <m>i</m>-<m>j</m> entry of <m>r\vec0</m> is <m>r0=0</m>.
        Hence <m>0A=\vec0=r\vec0</m>.
     
        </p>
    </solution>
    </exercise>
    </exercises>
    </section>
    
    
    <section><title>Matrix multiplication</title>
    
      
        <subsection><title> First concepts</title>
        <paragraphs><title>Definition of matrix multiplication</title>
        <p>
        The definition of matrix multiplication is very different from that
        of addition and subtraction.  Suppose we have two matrices <m>A</m> and
        <m>B</m> with respective sizes <m>m\times n</m> and <m>r\times s.</m> 
        <em>The product of A and B is defined if and only if <m>n=r</m></em>,
        that is, the number of
        columns of <m>A</m> is equal to the number of rows of <m>B.</m> When this
        is the case, the matrices are said to be 
        <em>conformable for multiplication</em>, and it is possible to evaluate
        the product <m>AB</m>.
        </p>
        </paragraphs>
        
        <definition><title>Matrix multiplication</title>
        <statement>
        <p>
        Consider the entries in row <m>R_i</m> of the matrix <m>A</m>:
        <m>a_{i,1}, a_{i,2},\dots, a_{i,n}</m> and also the entries in column
        <m>C_j</m> of the matrix <m>B:</m> <m>b_{1,j}, b_{2,j},\dots, b_{r,j}.</m>
        
        <me>
        A=\begin{bmatrix} \amp\amp\vdots \\
        \color{red}{a_{i,1}} \amp \color{red}{a_{i,2}}\amp\cdots\amp \color{red}{a_{i,n}}
        \rlap{\quad R_i}\\ \amp\amp\vdots\\ 
        \end{bmatrix}
        \qquad\quad
        \begin{matrix}
        B=\begin{bmatrix}
        \cdots\amp \color{green}{b_{1,j}}\amp\cdots\\
        \cdots\amp \color{green}{b_{2,j}}\amp\cdots\\
        \amp\vdots\\ 
        \cdots\amp \color{green}{b_{r,j}}\amp\cdots
        \end{bmatrix} \\
        \qquad C_j
        \end{matrix}
        </me>
        
        Then for <m>C=AB</m>, each <m>c_{i,j}</m> is computed in the following way:
        <me>
        c_{i,j}=
        \color{red}{a_{i,1}}\color{green}{b_{1,j}}+ 
        \color{red}{a_{i,2}}\color{green}{b_{2,j}}+\cdots +
        \color{red}{a_{i,n}}\color{green}{b_{r,j}}
        </me>
        </p>
        </statement>
        </definition>
        
        <p>
        Notice that the assumption <m>n=r</m> implies that there is just the
        right number of entries in the rows of <m>A</m> and columns of <m>B</m>
        to allow <m>c_{i,j}</m> to be defined.  The number <m>c_{i,j}</m> is
        also called the <em>inner product of row <m>R_i</m> of <m>A</m> and column
        <m>C_j</m> of <m>B.</m></em> 
        This product is written as
        <me>c_{i,j}=R_i\cdot C_j</me>
        Notice that this definition implies that the
        size of the product is <m>m\times s.</m>
        </p>
        
        <example><title>Examples of matrix multiplication</title>
        <p>
        <ul>
            <li><p> Suppose 
            <m>A=\begin{bmatrix} 
            1 \amp 2 \amp 3 \\ 
            4 \amp 5 \amp 6\end{bmatrix}_{2\times 3}</m> 
            and 
            <m>B=\begin{bmatrix} 
            3 \amp 1\\ 
            4 \amp 1 \\
            5 \amp 9
            \end{bmatrix}_{3\times 2}</m>. 
            Then <m>C=AB</m> is defined and has size <m>2\times2</m>. 
            Here are the entries in <m>C</m>:
            <me>\begin{array}{rclcl}
            c_{11}\amp=\amp1\cdot3 + 2\cdot4 + 3\cdot5 \amp=\amp 3+8+15=26\\
            c_{12}\amp=\amp1\cdot1 + 2\cdot1 + 3\cdot9 \amp=\amp 1+2+27=30\\
            c_{21}\amp=\amp4\cdot3 + 5\cdot4 + 6\cdot5 \amp=\amp 12+20+30=62\\
            c_{22}\amp=\amp4\cdot1 + 5\cdot1 + 6\cdot9 \amp=\amp 4+5+54=63
            \end{array}</me>
            In other words
            <m>C=\begin{bmatrix}26\amp30\\62\amp63\end{bmatrix}</m></p>
            </li>
        
            <li><p> Using <m>A</m> and <m>B</m> from the previous example, 
            the matrix <m>D=BA</m> is also defined. In this case the product is of size 
            <m>3\times3.</m> In this case we have
            <md>
            <mrow>
            D\amp =\begin{bmatrix}
            3\cdot1 + 1\cdot4 \amp 3\cdot2 + 1\cdot5 \amp 3\cdot3 + 1\cdot6\\
            4\cdot1 + 1\cdot4 \amp 4\cdot2 + 1\cdot5 \amp 4\cdot3 + 1\cdot6\\
            5\cdot1 + 9\cdot4 \amp 5\cdot2 + 9\cdot5 \amp 5\cdot3 + 9\cdot6
            \end{bmatrix}
            </mrow>
            <mrow>
            \amp =\begin{bmatrix}
            7\amp11\amp15\\8\amp13\amp18\\41\amp55\amp69
            \end{bmatrix}
            </mrow>
            </md>
            Note that <m>AB\not=BA</m> since the two matrices have different size.</p> 
            </li>
        
            <li><p> Let <m>I_2=\begin{bmatrix}1\amp0\\0\amp1\end{bmatrix}</m> and 
            <m>A</m> as in the previous examples. Then 
            <md>
            <mrow>
            I_2A
            \amp =
            \begin{bmatrix}
            1\cdot1+0\cdot4 \amp 1\cdot2+0\cdot5 \amp 1\cdot3 + 0\cdot6\\
            0\cdot1+1\cdot4 \amp 0\cdot2+1\cdot5 \amp 0\cdot3 + 1\cdot6
            \end{bmatrix}
            </mrow>
            <mrow>
            \amp =\begin{bmatrix} 1 \amp 2 \amp 3 \\ 4 \amp 5 \amp 6\end{bmatrix}
            </mrow>
            <mrow>
            \amp =A
            </mrow>
            </md>
            </p></li>
        
            <li>
            <p>Let
            <m>I_3=\begin{bmatrix}1\amp0\amp0\\0\amp1\amp0\\0\amp0\amp1\end{bmatrix}.</m> 
            Computing as in the last example, we have
            <m>AI_3=A.</m></p>
            </li>
        </ul>
        </p>
        </example>
        
        <paragraphs><title>Systems of linear equations and matrix multiplication</title>
        <p>
        We may use matrix multiplication to write systems of linear equations 
        compactly. Suppose we have a system of linear equations written as
        <md>
        <mrow> a_{1,1}x_1+a_{1,2}x_2+\cdots+ a_{1,n}x_n=b_1 </mrow>
        <mrow> a_{2,1}x_1+a_{2,2}x_2+\cdots+ a_{2,n}x_n=b_2 </mrow>
        <mrow> \vdots </mrow>
        <mrow> a_{m,1}x_1+a_{m,2}x_2+\cdots+ a_{m,n}x_n=b_m </mrow>
        </md>
        We then have <m>A=[a_{i,j}]</m> as the coefficient matrix. We also define
        <me>
        \vec{b} = \begin{bmatrix} b_1\\b_2\\ \vdots \\b_m \end{bmatrix}
        </me>
        and
        <me>
        \vec{x} = \begin{bmatrix} x_1\\x_2\\ \vdots \\x_n \end{bmatrix}
        </me>
        Matrix multiplication is defined so that the system of linear equations
        is exactly the same as the matrix equation
        <me>
        A\vec x=\vec b
        </me>
        </p>
        </paragraphs>
           <exercise>
           <statement>
           <p>
           Let 
           <m>A=\left[\begin{smallmatrix}2\amp1\\ -1\amp0\end{smallmatrix}\right]</m>
           and
           <m>B=\left[\begin{smallmatrix}1\amp2\\ -1\amp2\end{smallmatrix}\right]</m>.
           Evaluate
           <ul>
           <li><p><m>AB</m></p></li>
           <li><p><m>BA</m></p></li>
           <li><p><m>A^2=AA</m></p></li>
           <li><p><m>B^2=BB</m></p></li>
           <li><p><m>ABA</m></p></li>
           <li><p><m>BAB</m></p></li>
           </ul></p>
           </statement>
           <solution>
           <p>
           <ul>
           <li><p><m>AB=\left[
              \begin{smallmatrix}1\amp 6\\-1\amp -2\end{smallmatrix}\right]</m></p></li>
           <li><p><m>BA=\left[
              \begin{smallmatrix}0\amp 1\\-4\amp -1\end{smallmatrix}\right]</m></p></li>
           <li><p><m>A^2=AA=\left[
              \begin{smallmatrix}3\amp 2\\-2\amp -1\end{smallmatrix}\right]</m></p></li>
           <li><p><m>B^2=BB=\left[
              \begin{smallmatrix}-1\amp 6\\-3\amp 2\end{smallmatrix}\right]</m></p></li>
           <li><p><m>ABA=\left[
              \begin{smallmatrix}-4\amp 1\\0\amp -1\end{smallmatrix}\right]</m></p></li>
           
           <li><p><m>BAB=\left[
              \begin{smallmatrix}-1\amp 2\\-3\amp -10\end{smallmatrix}\right]</m></p></li>
           </ul>
           </p>
           </solution>   
           </exercise>
        <paragraphs><title>Linear combinations and matrix multiplication</title>
        <p>
        Suppose we start with an <m>m\times n</m> matrix <m>A</m> whose columns are
        <m>C_1,C_2,\ldots,C_n</m>.
        Recall from
        <xref ref="LinearCombinationDefinition" /> 
        that a linear combination of these columns has the form
        \[r_1C_1+r_2C_2+\cdots+r_nC_n.\]
        Consider the equation
        <mdn>
        <mrow xml:id="LinearCombinationEquation">
        B=r_1C_1+r_2C_2+\cdots+r_nC_n.
        </mrow>
        </mdn>
        Since <m>B</m> is conformable for addition, it must have <m>m</m> rows, and so we have
        <me>
        B=
        \begin{bmatrix}
        b_1 \\b_2 \\ \vdots\\ b_m
        \end{bmatrix}
        \textrm{ and }
        R=
        \begin{bmatrix}
        r_1 \\r_2 \\ \vdots\\ r_n
        \end{bmatrix}
        </me>
        
        Then then <xref ref="LinearCombinationEquation" />
        is identical to the equation
        \[
        AR=B.
        \]
        </p>
        </paragraphs>
        
        </subsection>
            
        <subsection><title> Properties of Matrix Multiplication </title>
        
        <paragraphs><title>Matrix multiplication is <em>not</em> commutative</title>
        <p>
        The most important difference between the multiplication of matrices
        and the multiplication of real numbers is that real numbers <m>x</m> and <m>y</m>
        always commute (that is <m>xy=yx</m>), but the same in not true for 
        matrices. For matrices
        <me>
        X=
        \begin{bmatrix}
        1\amp2\\3\amp4
        \end{bmatrix}
        \textrm{ and }
        Y=
        \begin{bmatrix}
        2\amp1\\0\amp-1
        \end{bmatrix}
        </me>
        we have
        <me>
        XY=
        \begin{bmatrix}
        2\amp-1\\6\amp-1
        \end{bmatrix}
        \textrm{ and }
        YX=
        \begin{bmatrix}
        5\amp8\\-3\amp-4
        \end{bmatrix}
        </me>.
        On the other hand, if 
        <me>
        Z=
        \begin{bmatrix}
        -2\amp 2\\3\amp 1
        \end{bmatrix}
        </me>
        then
        <me>
        XZ=
        \begin{bmatrix}
        4\amp 4\\6\amp 10
        \end{bmatrix}
        =ZX
        </me>.
        When <m>ZX=XZ</m>, we say that <m>X</m> and <m>Z</m> are
        <term>commuting matrices</term>.
        </p>
        </paragraphs>
        
        <!--
        <exercises>
        <exercisegroup>
        -->
            <exercise>
            <statement>
            <p>
            Suppose two matrices <m>X</m> and <m>Z</m> commute, that is, satisfy the equation
            <me>XZ=ZX.</me>
            Show that <m>X</m> and <m>Z</m> 
            are square matrices of the same size.
            </p>
            </statement>
            <solution>
            <p>
            Suppose that <m>X</m> is of size <m>m\times n</m> and
            <m>Z</m> is of size <m>r\times s</m>. Since the matrix <m>XZ</m> exists,
            the comformability condition says <m>n=r</m>. Similarly since <m>ZX</m>
            exists we have <m>s=m</m>. Since <m>XZ</m> is of size <m>m\times s</m> and 
            <m>ZX</m> is of size <m>n\times r</m>, the equation <m>XZ=ZX</m>
            implies that <m>m=n</m> and <m>s=r</m>. Putting the equalities together,
            we have <m>m=n=r=s</m>.
            </p>
            </solution>
            </exercise>
        
            <exercise>
            <statement>
            <p>
            Let <m>X=
            \left[\begin{smallmatrix}
            1\amp2\\3\amp4
            \end{smallmatrix}\right]</m>. Find all matrices <m>Z</m> so that
            <m>XZ=ZX</m>.
            </p>
            </statement>
            <hint>
            <p>
            Let 
            <m>Z=
            \begin{bmatrix}
            x\amp y\\ z\amp w
            \end{bmatrix}
            </m>
            and evaluate <m>XZ</m> and <m>ZX</m>
            </p>
            </hint>
            </exercise>
        
        <!--
        </exercisegroup>
        </exercises>
        -->
        
        <theorem><title>Left distributive law</title>
        <statement>
            <p>Let <m>A</m>, <m>B</m> and <m>C</m> be matrices of the right size
            for matrix multiplication. Then <m>A(B+C)=AB+AC</m>
            </p>
        </statement>
        <proof>
        <p>We compute the <m>i</m>-<m>j</m> entry on both sides of the equation:
        <ul>
            <li><p>
                Left hand side:
                The <m>k</m>-<m>j</m> entry of <m>B+C</m>
                is <m>b_{k,j}+c_{k,j}</m>, and so the <m>i</m>-<m>j</m> entry of
                <m>A(B+C)</m> is
                <md>
                <mrow>
                \amp a_{i,1}(b_{1,j}+c_{1,j})+
                a_{i,2}(b_{2,j}+c_{2,j})+
                \cdots+ a_{i,n}(b_{n,j}+c_{n,j})
                </mrow>
                <mrow>  \amp = a_{i,1}b_{1,j}+a_{i,1}c_{1,j} </mrow>
                <mrow> \amp +a_{i,2}b_{2,j}+a_{i,2}c_{2,j} </mrow>
                <mrow> \amp \ \vdots </mrow>
                <mrow> \amp +a_{i,n}b_{n,j}+a_{i,n}c_{n,j} </mrow>
                </md>
            </p></li>
            <li><p>
                Right hand side:
                The the <m>i</m>-<m>j</m> entry of <m>AB</m> is
                <m>a_{i,1}b_{1,j}+a_{i,2}b_{2,j}+\cdots+a_{i,n}b_{n,j}</m> 
                and the <m>i</m>-<m>j</m> entry of <m>AC</m> is
                <m>a_{i,1}c_{1,j}+a_{i,2}c_{2,j}+\cdots+a_{i,n}c_{n,j}</m>
                Hence the <m>i</m>-<m>j</m> entry of <m>AB+AC</m> is
                <md>
                <mrow>
                \amp a_{i,1}b_{1,j}+a_{i,2}b_{2,j}+\cdots+a_{i,n}b_{n,j} 
                </mrow>
                <mrow>
                +\amp a_{i,1}c_{1,j}+a_{i,2}c_{2,j}+\cdots+a_{i,n}c_{n,j}.
                </mrow>
                </md>
            </p>
            </li>
        </ul>
            Notice that the first column of sums for the left-hand side
            is the same as the first row of sums for the right-hand side,
            and similarly for the second column and second row. Hence
            the entries on the left-hand side and the right-hand side
            are equal.
        </p>
        </proof>   
        
        <proof>
        <p>
        Using summation notation:
        <md>
        <mrow> \bigl(A(B+C)\bigr)_{i,j} \amp = \sum_{k=1}^n A_{i,k}(B+C)_{k,j} </mrow>
        <mrow> \amp = \sum_{k=1}^n a_{i,k}(b_{k,j}+c_{k,j}) </mrow>
        <mrow> \amp = \sum_{k=1}^n (a_{i,k} b_{k,j}+a_{i,k} c_{k,j}) </mrow>
        <mrow> \amp = \sum_{k=1}^n a_{i,k} b_{k,j}+\sum_{k=1}^na_{i,k} c_{k,j} </mrow>
        <mrow> \amp = (AB)_{i,j} + (AC)_{i,j}</mrow> 
        </md>
        </p>
        </proof>
        
        </theorem>
        
        <theorem><title>Right distributive law</title>
        <statement>
            <p>Let <m>A</m>, <m>B</m> and <m>C</m> be matrices of the right size
            for matrix multiplication. Then <m>(B+C)A = BA+CA</m>.</p>
        </statement>
        <proof>
        <p>Let <m>B</m>  and <m>C</m> be of size <m>m\times n</m>,
        and <m>A</m> be of size <m>n\times r</m>.
        We use <m>A=[a_{i,j}],</m>  <m>B=[b_{i,j}]</m> and  <m>C=[c_{i,j}]</m>,
        and we compute the <m>i</m>-<m>j</m> entry on both sides of the equation.
        
        <ul>
            <li><p>
                Left hand side:
                The <m>i</m>-<m>k</m> entry of <m>B+C</m> is 
                <m>b_{i,k}+c_{i,k}</m>, 
                and so the  <m>i</m>-<m>j</m> entry of
                <m>(B+C)A</m> is
                <md>
                <mrow>
                (b_{i,1}+c_{i,1})a_{1,j} +
                (b_{i,2}+c_{i,2})a_{2,j}+
                \cdots+ (b_{i,n}+c_{i,n})a_{n,j}
                </mrow>
                <mrow>
                = b_{i,1}a_{1,j}+c_{i,1}a_{1,j} +\cdots
                + b_{i,n}a_{1,n}+c_{i,n}a_{n,j} 
                </mrow>
                </md>.
            </p></li>
        
            <li><p>
                Right hand side:
                The <m>i</m>-<m>j</m> entry of <m>BA</m> is
                <m>b_{i,1}a_{1,j}+b_{i,2}a_{2,j}+\cdots+b_{i,n}a_{n,j}</m> 
                and the <m>i</m>-<m>j</m> entry of <m>CA</m> is
                <m>c_{i,1}a_{1,j}+c_{i,2}a_{2,j}+\cdots+c_{i,n}a_{n,j}</m>
                Hence the <m>i</m>-<m>j</m> entry of <m>BA+CA</m> is
                <md>
                <mrow>
                b_{i,1}a_{1,j}+b_{i,2}a_{2,j}+\cdots+b_{i,n}a_{n,j}
                </mrow>
                <mrow>
                +  c_{i,1}a_{1,j}+c_{i,2}a_{2,j}+\cdots+c_{i,n}a_{n,j}
                </mrow>
                </md>
                Reordering the summands, we get 
                the <m>i</m>-<m>j</m> entry of <m>BA+CA</m> to be
                <m>b_{i,1}a_{1,j}+ c_{i,1}a_{1,j}
                 +\cdots+b_{i,n}a_{n,j}+c_{i,n}a_{n,j}</m>
            </p></li>
        </ul>

        The left-hand side and the right-side agree, and so  <m>(B+C)A=BA+CA</m>.
        </p>
        </proof>   
        
        <proof>
        <p>
        Using summation notation:
        <md>
        <mrow> \bigl((B+C)A\bigr)_{i,j} \amp = \sum_{k=1}^n (B+C)_{i,k}A_{k,j} </mrow>
        <mrow> \amp = \sum_{k=1}^n  (b_{i,k}+c_{i,k}) a_{k,j}</mrow>
        <mrow> \amp = \sum_{k=1}^n (b_{i,k}a_{k,j} + c_{i,k} a_{k,j}) </mrow>
        <mrow> \amp = \sum_{k=1}^n b_{i,k}a_{k,j} + \sum_{k=1}^n c_{i,k} a_{k,j} </mrow>
        <mrow> \amp = (BA)_{i,j} + (CA)_{i,j}</mrow> 
        </md>
        </p>
        </proof>
        </theorem>
        
        
        <theorem><title>Associativity of matrix multiplication</title>
        <statement>
            <p>Let <m>A</m>, <m>B</m> and <m>C</m> be matrices of the right size
            for matrix multiplication. Then <m>A(BC)=(AB)C</m>.</p>
        </statement>
        <proof>
        <p>
            Let <m>A</m> be of size <m>m\times n</m>, and
            <m>B</m>  of size <m>n\times r</m> and
            <m>C</m> of size <m>r\times s</m>.
            We use <m>A=[a_{i,j}],</m>  
            <m>B=[b_{i,j}]</m> and  
            <m>C=[c_{i,j}]</m>,
            and we compute 
            the <m>i</m>-<m>j</m> entry 
            on both sides of the equation.
         
            <ul>
            <li>
            <p>
                Left hand side:
                The <m>i</m>-<m>j</m> entry of <m>A(BC)</m> is
                <m>a_{i,1}(BC)_{1,j} +
                a_{i,2}(BC)_{2,j} + \cdots +
                a_{i,n}(BC)_{n,j}.</m>
         
                In addition, for any <m>t, 1\leq t\leq n,</m> we have
         
                <m>(BC)_{t,j}=b_{t,1}c_{1,j} + b_{t,2}c_{2,j} + \cdots + b_{t,r}c_{r,j}</m>
         
                This means that the left-hand side is
         
                <md>
                <mrow>
                a_{i,1}(b_{1,1}c_{1,j} + b_{1,2}c_{2,j} + \cdots + b_{1,r}c_{r,j})
                </mrow><mrow>
                + a_{i,2}(b_{2,1}c_{1,j} + b_{2,2}c_{2,j} + \cdots + b_{2,r}c_{r,j})
                </mrow><mrow>
                + a_{i,3}(b_{3,1}c_{1,j} + b_{3,2}c_{2,j} + \cdots + b_{3,r}c_{r,j})
                </mrow><mrow>
                \vdots
                </mrow><mrow>
                + a_{i,n}(b_{n,1}c_{1,j} + b_{n,2}c_{2,j} + \cdots + b_{n,r}c_{r,j})
                </mrow>
                </md>
                which is
                <me>\begin{matrix}
                a_{i,1}b_{1,1}c_{1,j} + a_{i,1}b_{1,2}c_{2,j} + \cdots + a_{i,1}b_{1,r}c_{r,j}\\
                + a_{i,2}b_{2,1}c_{1,j} + a_{i,2}b_{2,2}c_{2,j} + \cdots + a_{i,2}b_{2,r}c_{r,j}\\
                + a_{i,3}b_{3,1}c_{1,j} + a_{i,3}b_{3,2}c_{2,j} + \cdots + a_{i,3}b_{3,r}c_{r,j}\\
                \vdots\\
                + a_{i,n}b_{n,1}c_{1,j} + a_{i,n}b_{n,2}c_{2,j} + \cdots + a_{i,n}b_{n,r}c_{r,j}
                \end{matrix}</me>
         
                Notice what this result is actually stating. Each summand is of the form
                <m>a_{i,t}b_{t,u}c_{u,j}</m> where <m>1\leq t\leq n</m> and <m>1\leq u\leq r.</m>
                Since there are <m>nr</m> summands, all different, each possible 
                <m>a_{i,t}b_{t,u}c_{u,j}</m>
                appears exactly once within the sum.
            </p>
        </li>
        
        <li>
            <p>
            Right hand side:
            The argument is almost identical with the one used on the left-hand side.
            The <m>i</m>-<m>j</m> entry of <m>(AB)C</m> is
            <m>(AB)_{i,1}c_{1,j} +
            (AB)_{i,2}c_{2,j} + \cdots +
            (AB)_{i,r}c_{r,j}.</m>
            In addition, for any <m>t, 1\leq t\leq r,</m> we have
            <m>(AB)_{i,t}=a_{i,1}b_{1,t} + a_{i,2}b_{2,t} + \cdots + a_{i,n}b_{n,t}</m>
            This means that the right-hand side is
            
            <md>
            <mrow>
            (a_{i,1}b_{1,1} + a_{i,2}b_{2,1} + \cdots + a_{i,n}b_{n,1})c_{1,j}
            </mrow><mrow>
            (a_{i,1}b_{1,2} + a_{i,2}b_{2,2} + \cdots + a_{i,n}b_{n,2})c_{2,j}
            </mrow><mrow>
            (a_{i,1}b_{1,3} + a_{i,2}b_{2,3} + \cdots + a_{i,n}b_{n,3})c_{3,j}
            </mrow><mrow>
            \vdots
            </mrow><mrow>
            (a_{i,1}b_{1,r} + a_{i,2}b_{2,r} + \cdots + a_{i,n}b_{n,r})c_{r,j}
            </mrow>
            </md>
            which is
            <md>
            <mrow> 
            a_{i,1}b_{1,1}c_{1,j} + a_{i,2}b_{2,1}c_{1,j} + \cdots + a_{i,n}b_{n,1}c_{1,j}
            </mrow><mrow>
            a_{i,1}b_{1,2}c_{2,j} + a_{i,2}b_{2,2}c_{2,j} + \cdots + a_{i,n}b_{n,2}c_{2,j}
            </mrow><mrow>
            \vdots
            </mrow><mrow>
            a_{i,1}b_{1,r}c_{r,j} + a_{i,2}b_{2,r}c_{r,j} + \cdots + a_{i,n}b_{n,r}c_{r,j}
            </mrow>
            </md>
            
            Once again, each summand is of the form
            <m>a_{i,t}b_{t,u}c_{u,j}</m> where <m>1\leq t\leq n</m> and <m>1\leq u\leq r.</m>
            Since there are <m>nr</m> summands, all different, each possible 
            <m>a_{i,t}b_{t,u}c_{u,j}</m>
            appears exactly once within the sum. Hence the left-hand side and right-hand side are equal.
            </p>
        </li>
        </ul>
        
        </p>
        </proof>
        
        <proof>
        <p>
            Let <m>A</m> be of size <m>m\times n</m>, and
            <m>B</m>  of size <m>n\times r</m> and
            <m>C</m> of size <m>r\times s</m>.
            Using summation notation:
            <me>
            \bigl((AB)C\bigr)_{i,j} = \sum_{k=1}^r (AB)_{i,k}c_{k,j} 
            </me>
            while
            <me>
            (AB)_{i,k}=\sum_{\ell=1}^n a_{i,\ell}b_{\ell,k} 
            </me>
            Now we have
            <md>
            <mrow> \bigl((AB)C\bigr)_{i,j} \amp = 
            \sum_{k=1}^r \bigl(\sum_{\ell=1}^n a_{i,\ell}b_{\ell,k}\bigr) c_{k,j} </mrow>
            <mrow> \amp = \sum_{k=1}^r \bigl(\sum_{\ell=1}^n a_{i,\ell}b_{\ell,k}c_{k,j}\bigr) </mrow>
            <mrow> \amp = \sum_{k=1}^r \sum_{\ell=1}^n a_{i,\ell}b_{\ell,k}c_{k,j} </mrow>
            </md>
            On ther other hand
            <me>
            \bigl(A(BC)\bigr)_{i,j} = \sum_{\ell=1}^n a_{i,\ell} (BC)_{\ell,j}
            </me>
            while
            <me>
            (BC)_{\ell,j}=\sum_{k=1}^r b_{\ell,k}c_{k,j} 
            </me>
            and so
            <md>
            <mrow>
            \bigl(A(BC)\bigr)_{i,j} 
            \amp =  \sum_{\ell=1}^n a_{i,\ell}\bigl(\sum_{k=1}^r b_{\ell,k}c_{k,j} \bigr) </mrow>
            <mrow>\amp =  \sum_{\ell=1}^n \sum_{k=1}^r a_{i,\ell} b_{\ell,k}c_{k,j} </mrow>
            </md>
            and so
            <me>(AB)C=A(BC)</me>
        </p>
        </proof>
        </theorem>
        </subsection>
        
        <subsection><title> Solving a matrix equation <m>AX=B</m> </title>
            <p> 
            Suppose that <m>A</m> is an <m>m\times n</m> matrix and 
            <m>B</m> is an <m>m\times r</m> matrix. We want find a matrix <m>X</m> so that
            <m>AX=B.</m> To be conformable for multiplication, <m>X</m> must be of size
            <m>n\times r.</m> Specifically,
            if <m>A=[a_{i,j}]</m>, <m>X=[x_{i,j}]</m>  and <m>B=[b_{i,j}]</m>,
            then we have
         
            <me>
            \begin{bmatrix}
            a_{1,1} \amp a_{1,2} \amp \cdots \amp a_{1,n}\\
            a_{2,1} \amp a_{2,2} \amp \cdots \amp a_{2,n}\\
                    \amp       \amp \vdots \amp\\
            a_{m,1} \amp a_{m,2} \amp \cdots \amp a_{m,n}
            \end{bmatrix}
            \begin{bmatrix}
            x_{1,1} \amp x_{1,2} \amp \cdots \amp x_{1,r}\\
            x_{2,1} \amp x_{2,2} \amp \cdots \amp x_{2,r}\\
                    \amp       \amp \vdots \amp\\
            x_{n,1} \amp x_{n,2} \amp \cdots \amp x_{n,r}
            \end{bmatrix}
            =
            \begin{bmatrix}
            b_{1,1} \amp b_{1,2} \amp \cdots \amp b_{1,r}\\ 
            b_{2,1} \amp b_{2,2} \amp \cdots \amp b_{2,r}\\
                    \amp       \amp \vdots \amp\\
            b_{m,1} \amp b_{m,2} \amp \cdots \amp b_{m,r}
            \end{bmatrix}
            </me>
         
            If we look at the <m>i</m>-<m>j</m> entry on both sides of this equation, we get
         
            <me>
            a_{i,1}x_{1,j} +a_{i,2}x_{2,j}+\cdots +a_{i,n}x_{n,j} = b_{i,j}.
            </me>
         
            If we keep the column number <m>j</m> of <m>X</m> fixed for the moment
            and let <m>i</m> range from <m>1</m> to <m>m</m>, 
            we get a system of linear equations:
         
            <me>
            \begin{bmatrix}
            a_{1,1} \amp a_{1,2} \amp \cdots \amp a_{1,n}\\
            a_{2,1} \amp a_{2,2} \amp \cdots \amp a_{2,n}\\
                    \amp       \amp \vdots \amp\\
            a_{m,1} \amp a_{m,2} \amp \cdots \amp a_{m,n}
            \end{bmatrix}
            \begin{bmatrix}
            x_{1,j}\\
            x_{2,j}\\ 
            \vdots \\
            x_{n,j}
            \end{bmatrix}
            =
            \begin{bmatrix}
            b_{1,j} \\
            b_{2,j}\\
            \vdots\\
            b_{m,j}
            \end{bmatrix}
            </me>
        
            We can solve this system by finding the reduced row echelon form
            of the augmented matrix. Here's the beautiful part: there are <m>n</m>
            different systems of linear equations that arise as <m>j</m> takes on the
            values from <m>1</m> to <m>n</m>, and they all have the identical coefficient
            matrix. This means that the same sequence of elementary row operations
            may be used on each of the equations to find the solutions.
        </p>
        
        <example>
        <p>
        <ul>
        <li><p>
                Let 
                <me>
                A=\begin{bmatrix} 1\amp2\amp3\\4\amp5\amp6\end{bmatrix}
                \textrm{ and }  
                B=\begin{bmatrix} 3\amp1\\4\amp1\end{bmatrix}
                </me>. 
                Then <m>X</m> must be a <m>3\times2</m> matrix so that
                <me>
                X=\begin{bmatrix} x_{1,1}\amp x_{1,2}\\
                x_{2,1}\amp x_{2,2}\\
                x_{3,1}\amp x_{3,2}\end{bmatrix}.
                </me>
                Solving for the first column means finding the reduced row echelon form of
                <m>\left[\begin{smallmatrix} 1\amp2\amp3\amp3 \\4\amp5\amp6\amp4\end{smallmatrix}\right].</m> 
                It is
                <m>\left[\begin{smallmatrix} 1\amp0\amp-1\amp-\tfrac73
                \\0\amp1\amp2\amp\tfrac83\end{smallmatrix}\right]</m>
                and so
                <m>x_{1,1}=-\tfrac73+s </m>, <m>x_{2,1}=\tfrac83-2s</m>, and <m>x_{3,1}=s.</m>
                Solving for the second column means finding the reduced row echelon form of
                <m>\left[\begin{smallmatrix} 1\amp2\amp3\amp1 \\4\amp5\amp6\amp1\end{smallmatrix}\right]</m> 
                which is
                <m>\left[\begin{smallmatrix} 1\amp0\amp-1\amp-1 \\0\amp1\amp2\amp1\end{smallmatrix}\right]</m> 
                and so
                <m>x_{1,2}=-1+t </m>, <m>x_{2,2}=1-2t</m>, and <m>x_{3,2}=t.</m> 
                Hence we conclude that
                <me>X=\begin{bmatrix}
                -\tfrac73+s \amp-1+t\\
                \tfrac83-2s \amp 1-2t\\
                s\amp t
                \end{bmatrix}</me>
                is a solution of the matrix equation <m>AX=B</m> for any choice of <m>s</m> and <m>t</m>.
                Notice the similarty of the two matrices to be put into reduced
                row echelon form. They differ, of course, only in the last column. In
                fact, this means that exactly the same elementary row operations were
                used to put both matrices into reduced row echelon form. Since both
                computations used the same  coefficient matrix, we could have carried
                out both computations at once by starting with the matrix
                <m>\left[\begin{smallmatrix}
                1\amp2\amp3\amp3\amp1\\ 4\amp5\amp6\amp4\amp1
                \end{smallmatrix}\right]</m> 
                and obtaining
                <m>\left[\begin{smallmatrix}
                1\amp0\amp-1\amp-\tfrac73 \amp-1\\0\amp1\amp2\amp\tfrac83\amp1
                \end{smallmatrix}\right]</m> 
                for the reduced row echelon form.
        </p></li>
            
        <li><p>
                An even more striking example can be obtained when there are no free variables. Let
                <m>A=\left[\begin{smallmatrix} 1\amp1\amp3\\3\amp2\amp5\\1\amp1\amp1\end{smallmatrix}\right]</m> and 
                <m>B=\left[\begin{smallmatrix} 1\amp1\amp2\\ 2\amp2\amp3\\ 3\amp3\amp4\end{smallmatrix}\right]</m> 
                so that <m>X</m> is a <m>3\times3</m> matrix. Then the augmented matrix
                <me>\left[\begin{array}{ccc|rrr}
                1\amp1\amp3\amp1\amp1\amp2\\3\amp2\amp5\amp2\amp2\amp3\\1\amp1\amp1\amp3\amp3\amp4
                \end{array}\right]</me> 
                has reduced row echelon form
                <me>\left[\begin{array}{ccc|rrr}
                1\amp0\amp0\amp-1\amp-1\amp-2\\0\amp1\amp0\amp5\amp5\amp7\\0\amp0\amp1\amp-1\amp-1\amp-1
                \end{array}\right]</me>
                From the first column of <m>B</m> we get <m>x_{1,1}=-1</m>, <m>x_{2,1}=5</m> and <m>x_{3,1}=-1.</m>
                From the second column of<m>B</m> we get <m>x_{1,2}=-1</m>, <m>x_{2,2}=5</m> and <m>x_{3,2}=-1.</m>
                From the third column of <m>B</m> we get <m>x_{1,3}=-2</m>, <m>x_{2,3}=7</m> and <m>x_{3,3}=1.</m>
                So 
                <me>
                X= \begin{bmatrix}
                -1\amp-1\amp-2 \\5\amp5\amp7\\-1\amp-1\amp-1
                \end{bmatrix}.
                </me> 
                Notice that <m>X</m> is the right half of the reduced row echelon form, 
                and note that it follows directly from the identity matrix in the left half of the same matrix.
        </p></li>
        
        <li><p>
                If <m>A</m> and <m>B</m> are both square <m>n\times n</m> matrices, 
                and the reduced row echelon form of <m>A</m> is <m>I_n</m>, then the reduced row echelon form of
                <m>[A| B]</m> is <m>[I_n|X]</m> where <m>X</m> satisfies <m>AX=B.</m>
                
                As an example, let us solve the matrix equation
                <me>
                \begin{bmatrix}
                1\amp2\amp-1\\3\amp1\amp0\\2\amp1\amp1
                \end{bmatrix}
                X=\begin{bmatrix}
                -2\amp3\amp3\\2\amp2\amp1\\4\amp2\amp2
                \end{bmatrix}
                </me>
                Then <m>X</m> is a <m>3\times 3</m> matrix, and the reduced row echelon form of
                <me>
                [A| B]=
                \left[\begin{array}{ccr|rrr}
                1\amp2\amp-1\amp-2\amp3\amp3\\ 3\amp1\amp0\amp2\amp2\amp1\\2\amp1\amp1\amp4\amp2\amp2
                \end{array}\right]
                </me>
                is
                <me>
                \left[\begin{array}{ccc|rrr}
                1\amp0\amp0\amp-2\amp1\amp3\\ 0\amp1\amp0\amp2\amp2\amp1\\0\amp0\amp1\amp4\amp2\amp2
                \end{array}\right]
                </me>
                and
                <me>
                X=
                \begin{bmatrix}
                -2\amp1\amp3\\ 2\amp2\amp1 \\4\amp2\amp2
                \end{bmatrix}.
                </me>
        </p></li>
        </ul>
        </p>
        </example>
        
        <paragraphs><title>The reduced row echelon form of a square matrix</title>
        <p>
        When a square matrix <m>A_n</m> is in reduced row echelon form, one of two things
        happens:
        <ul>
        <li><p>The bottom row is an all-zero row.</p></li>
        <li><p>The bottom row is <em>not</em> an all-zero row.</p></li>
        </ul>
        In the second case, this means that every row has a leading one, and,
        since the matrix is square, every column also has a leading one. This means that
        there are no free variables, and so the leading ones are all on the diagonal,
        that is, the reduced row echelon form is <m>I_n</m>.
        </p>
        </paragraphs>
        
        <theorem><title>Reduced row echelon form of a square matrix</title>
        <statement>
        <p>
        The reduced row echelon form of a square matrix <m>A_n</m> 
        <ul>
        <li><p>is <m>I_n</m>, and so the rank is <m>n</m>, or</p></li>
        <li><p>has a bottom row that is an all-zero row,
        and so the rank is less than <m>n</m>.</p></li>
        </ul>
        </p>
        </statement>
        </theorem>
           
        </subsection>
        
    </section>
    
    
    
    <section><title>The transpose and trace of a matrix</title>
    
    <definition><title>The transpose of a matrix</title>
    <statement>
    <p>
        The <term>transpose</term> of <m>A</m> is the matrix <m>A^T</m> derived
        by making the first row of <m>A</m> the first column of <m>A^T</m>,
        the second row of <m>A</m> the second column of <m>A^T</m>, etc. In
        other words, when taking a transpose,
        the rows and columns are interchanged. Another way
        of saying this is that the subscripts have been interchanged,
        that is, if <m>A^T=B=[b_{i,j}].</m> then 
        <m>b_{i,j}=a_{j,i}</m>
    </p>
    </statement>
    </definition>
    
    <example><title>Matrix transposes</title>
    <p>
    <ul>
    <li><p>
        Let <m>A=\begin{bmatrix}
        1\amp2\amp3\\4\amp5\amp6\\7\amp8\amp9
        \end{bmatrix}.</m> Then
        <m>A^T=\begin{bmatrix}
        1\amp4\amp7\\2\amp5\amp8\\3\amp6\amp9
        \end{bmatrix}</m>
    </p></li>
    
    <li>
        The <em>identity matrix</em> <m>I_n</m> of order n has all
        diagonal entries equal to one and all other entries equal to zero.
        <m>I_n=\begin{bmatrix}
            1 \amp 0 \amp 0 \amp \cdots \amp 0\\
            0 \amp 1 \amp 0 \amp \cdots \amp 0\\
            0 \amp 0 \amp 1 \amp \cdots \amp 0\\
            \amp   \amp   \amp \ddots \amp  \\ 
            0 \amp 0 \amp 0 \amp \cdots \amp 1
            \end{bmatrix}_n
        </m>
        Clearly <m>I_n^T=I_n.</m>
    </li>
    
    <li><p>
        The transpose is defined for nonsquare matrices, too.
            Let
            <m>A=\begin{bmatrix}
            1 \amp 2 \amp 3 \amp 4 \\
            5 \amp 6 \amp 7 \amp 8 \\
            9 \amp10\amp11\amp12
            \end{bmatrix}.</m>
           Then     
        <m>A^T=\begin{bmatrix}
           1 \amp 5 \amp9\\
           2 \amp 6 \amp 10\\
           3 \amp 7 \amp 11\\
           4 \amp 8 \amp 12
           \end{bmatrix}.</m>
    </p></li>
    </ul>
    </p>
    </example>
    
    <p>
    Here is an animation that illustrates that <m>A^T</m> may
    be derived from <m>A</m> by reflection on the main diagonal.
    </p>
    <figure>
    <caption/>
    <image width="50%" source="images/300px-Matrix-transpose.gif" />
    </figure>
    
    
    <theorem><title>Properties of the transpose of a matrix</title>
    <statement>
    <p>
    <ul>
        <li><p><m>(A^T)^T=A</m></p></li>
        <li><p><m>(A+B)^T=A^T+B^T</m></p></li>
        <li><p><m>(rA)^T=rA^T</m></p></li>
        <li><p><m>(AB)^T=B^TA^T</m></p></li>
    </ul>
    </p>
    </statement>
    
    <proof>
    <p>
    <ul>
        <li><p>
            The <m>i</m>-<m>j</m> entry of <m>A^T</m> is <m>a_{j,i}</m>, obtained by
            interchanging the subscripts of <m>A</m>. Taking the transpose
            again interchanges the subscripts again, and so the <m>i</m>-<m>j</m>
            entry of <m>(A^T)^T</m> is <m>a_{i,j}</m>, the same as for <m>A</m>.
            Hence <m>(A^T)^T=A</m>.
        </p></li>
    
        <li><p> 
            The <m>i</m>-<m>j</m> entry on both sides of the equation
            is <m>a_{j,i}+b_{j,i}</m>.
        </p></li>
    
        <li><p> 
            The <m>i</m>-<m>j</m> entry on both sides of the equation
            is <m>ra_{j,i}`</m>.
        </p></li>
    
        <li><p> 
            The <m>i</m>-<m>j</m> entry of <m>(AB)^T</m> is
            the <m>j</m>-<m>i</m> entry of <m>AB,</m> 
            which in turn is
            <me>a_{j,1}b_{1,i} +a_{j,2}b_{2,i}+\cdots +a_{j,n}b_{n,i}.</me>
    
    
            The <m>i</m>-<m>j</m> entry of <m>B^TA^T</m> is
            <m>(B^T)_{i,1}(A^T)_{1,j} + (B^T)_{i,2}(A^T)_{2,j}
            +\cdots + (B^T)_{i,n}(A^T)_{n,j}
            =\\
            b_{1,i}a_{j,1} + b_{2,i}a_{j,2}
            +\cdots + b_{n,i}a_{j,n}.</m>
            Hence both sides of the equation have the same
            <m>i</m>-<m>j</m> entry, and so the matrices are equal.
        </p></li>
    </ul>
    </p>
    </proof>
    
    <proof>
    <p>
    <md>
    <mrow>(AB)^T_{i,j} \amp = (AB)_{j,i}</mrow>
    <mrow>\amp = \sum_{k=1}^n a_{j,k}b_{k,i}</mrow>
    <mrow>\amp = \sum_{k=1}^n b_{k,i}a_{j,k}</mrow>
    <mrow>\amp = \sum_{k=1}^n B^T_{i,k}A^T_{k,j}</mrow>
    <mrow>\amp = (B^T A^T)_{i,j}</mrow>
    
    </md>
    </p>
    </proof>
    </theorem>
    
    <definition><title>Trace of a square matrix</title>
    <statement>
    <p>
        The <term>trace</term>  of a square matrix <m>A</m> of size <m>n</m>
        is the sum of the diagonal elements,
        that is,
        <me>
        \mathrm{tr}A=a_{1,1}+a_{2,2}+\cdots + a_{n,n}=\sum_{i=1}^n a_{i,i}
        </me>
    </p>
    </statement>
    </definition>
    
    <theorem> <title>Traces of <m>AB</m> and <m>BA</m> are equal</title>
    <statement>
        <p>If <m>AB</m> and <m>BA</m> are each square, then
        <m>\mathrm{tr}(AB)=\mathrm{tr}(BA)</m></p>
    </statement>
    
    <proof>
        <p>
        Suppose that <m>A</m> is <m>m\times n</m> and <m>B</m> is <m>n\times
        m</m> (remember that <m>AB</m> is then defined and square).
        To evalute both <m>\mathrm{tr}(AB)</m> and <m>\mathrm{tr}(BA)</m>, we
        consider the following rectangular array of numbers:
    
        <me>
        \begin{matrix}
        \amp\amp\amp\amp\amp\text{Row sums}\\
        a_{1,1}b_{1,1}\amp a_{1,2}b_{2,1}\amp a_{1,3}b_{3,1}\amp\cdots\amp
        a_{1,n}b_{n,1} \amp\gets (AB)_{1,1}\\
        a_{2,1}b_{1,2}\amp a_{2,2}b_{2,2}\amp a_{2,3}b_{3,2}\amp\cdots\amp
        a_{2,n}b_{n,2} \amp\gets(AB)_{2,2}\\
        \vdots\amp\vdots\amp\vdots\amp\amp\vdots\amp\vdots\\
        a_{m,1}b_{1,m}\amp a_{m,2}b_{2,m}\amp a_{m,3}b_{3,m}\amp\cdots\amp
        a_{m,n}b_{n,m} \amp\gets (AB)_{m,m}\\
        \\
        \uparrow\amp\uparrow\amp\uparrow\amp\amp\uparrow\\
        \llap{\text{Column sums:}\quad}
        (BA)_{1,1}\amp(BA)_{2,2}\amp(BA)_{3,3}\amp\amp(BA)_{n,n}
        \end{matrix}
        </me>
        
        
        We are going to compute the sum of all the elements of this array in
        two ways.
        Having done this, the two answers will be equal.
        </p>
        
        <p>First we find the sum of all the entries of the array by adding
        row-wise.
        Observe that the sum of the elements in the first row is
        <m>(AB)_{1,1}</m>,
        the sum of those in the second row is <m>(AB)_{2,2}</m>, and so on until
        the sum of
        the elements in the last row is <m>(AB)_{m,m}</m>. Hence the sum of all
        of the
        elements in the array is
        <m>(AB)_{1,1}+(AB)_{2,2}+\cdots+(AB)_{m,m}=\mathrm{tr}(AB)</m>.</p>
        
        <p>Now we add columnwise. Notice that the first column sums to
        <md>
        <mrow>a_{1,1}b_{1,1}\amp +a_{2,1}b_{1,2}+\cdots+a_{m,1}b_{1,m}</mrow>
        <mrow>\amp = b_{1,1}a_{1,1}+b_{2,1}a_{1,2}+\cdots+b_{m,1}a_{1,m}</mrow>
        <mrow>\amp =(BA)_{1,1},</mrow>
        </md>
        the second column sums to <m>(BA)_{2,2}</m> and so on until the last
        column sums to
        <m>(BA)_{n,n}</m>.Hence the sum of all of the
        elements in the array is
        <m>(BA)_{1,1}+(BA)_{2,2}+\cdots+(BA)_{n,n}=\mathrm{tr}(BA)</m>.
        
        Equating the two evaluations gives
        <m>\mathrm{tr}(AB)=\mathrm{tr}(BA)</m>.
        </p>
    </proof>
    
    <proof>
        <p>
        Suppose that <m>A</m> is <m>m\times n</m>. Then, since <m>AB</m> and
        <m>BA</m> are defined, the conformability ensures that
        <m>B</m> is <m>n\times m</m>.
        <md>
        <mrow> \mathrm{tr}(AB) \amp = \sum_{i=1}^m(AB)_{i,i} </mrow>
        <mrow>\amp =  \sum_{i=1}^m  \sum_{j=1}^n a_{i,j}b_{j,i}</mrow>
        <mrow>\amp =  \sum_{i=1}^m  \sum_{j=1}^n b_{j,i}a_{i,j}</mrow>
        <mrow>\amp =   \sum_{j=1}^n\sum_{i=1}^m  b_{j,i}a_{i,j}</mrow>
        <mrow>\amp =   \sum_{j=1}^n (BA)_{j,j}</mrow>
        <mrow>\amp =   \mathrm{tr}(BA)</mrow>
        </md>
        </p>
    </proof>
    </theorem>
    
    </section>
    
    
    <section><title>Powers of a matrix (nonnegative exponents)</title>
    
        <subsection><title> Computing the powers of a square matrix <m>A</m> </title>
            <p>
            If <m>A</m> and <m>B</m> are both square matrices of order <m>n</m>, 
            then <m>AB</m> is also a square matrix
        	of order <m>n</m>. In particular, if <m>A=B</m>, then <m>AA</m> is defined. 
            We call this matrix <m>A^2</m>.
        	We also let <m>A^3=AAA</m>. Similarly we have higher powers of <m>A</m>:
        
            <me>
            A^n=\underbrace{A A A \cdots A A}_{n \text{ factors}}\quad
            \text{ for } n=1,2,\ldots
            </me>.
        
            In addition, we define <m>A^1=A</m>.
        
            For example, if
            <m>A=\begin{bmatrix}1\amp2\\2\amp1\end{bmatrix}</m> then
        
            <me>
            A^1=\begin{bmatrix}1\amp2\\2\amp1\end{bmatrix}\\
            A^2=\begin{bmatrix}5\amp4\\4\amp5\end{bmatrix}\\
            A^3=\begin{bmatrix}13\amp14\\14\amp13\end{bmatrix}\\
            A^4=\begin{bmatrix}40\amp41\\41\amp40\end{bmatrix}\\
            A^5=\begin{bmatrix}121\amp122\\122\amp121\end{bmatrix}\\
            </me>
        
            </p>
        </subsection>
    
        <subsection xml:id="LawOfExponents"> <title> The law of exponents </title>
        
            <p>
            If <m>m</m> and <m>n</m> are positive integers, then by simply counting the factors we
        	get the following two equations:
            
            <ul>
            <li><p>
                <m>A^m A^n=\underbrace{A\cdots A}_{m \text{ factors}}\ 
                \underbrace{A\cdots A}_{n \text{ factors}}=
                \underbrace{A\cdots A}_{m+n\text{ factors}}=A^{m+n},
                </m>
            </p></li>
            
            <li><p> 
                <m>(A^m)^n 
                    =\underbrace{(\underbrace{A\cdots A}_{m \text{ factors}})
                    (\underbrace{A\cdots A}_{m \text{ factors}})\cdots
                    (\underbrace{A\cdots A}_{m \text{ factors}})}_{n \text{ times}}=A^{mn}
                </m>
            </p></li>
            </ul>
        
            These two equations together are called <term>the law of exponents</term>.
            </p>
        </subsection>
    
        <subsection><title> What is <m>A^0</m>? </title>
    
            <p>We want to define <m>A^0</m> so the the law of exponents remains valid. This says
            <me>
            A^n A^0= A^{n+0}=A^n
            </me>
            We observe that if <m>A^0=I</m>, the identity matrix, then this equation is valid.
            With this in mind we <m>\textbf{define}</m> <m>A^0=I</m>.
            </p>
        </subsection>
    
        <subsection><title> Polynomials and powers of a matrix </title>
            <p>
            Recall that polynomials are functions of the form
            <m> p(x)=a_nx^n + a_{n-1}x^{n-1}+\cdots+a_1x+a_0</m>. If we have a square matrix, we may
            refer to <m>p(A)</m>. By this we mean we substitute the matrix <m>A</m> for each <m>x</m> appearing
            in the polynomial. Whenever <m>x^k</m> appears, we substitute <m>A^k</m> for it and do the
            computations. For example, using the same <m>A</m> as before, if <m>p(x)=x^2-2x+1</m>, then
            <me>
            p(A)=A^2-2A+I=
            \begin{bmatrix}5\amp4\\4\amp5\end{bmatrix} 
            -2\begin{bmatrix}1\amp2\\2\amp1\end{bmatrix}
            +\begin{bmatrix}1\amp0\\0\amp1\end{bmatrix}
            =
            \begin{bmatrix}4\amp0\\0\amp4\end{bmatrix}
            </me>
            
            Now consider the matrix
            <me>
            B=\begin{bmatrix}2\amp-1\\1\amp0\end{bmatrix} 
            </me>
            and the same polynomial <m>(x)=x^2-2x+1</m>. In this case
            
            <me>
            p(B)=B^2-2B+I=
            \begin{bmatrix}3\amp-2\\2\amp-1\end{bmatrix} 
            -2\begin{bmatrix}2\amp-1\\1\amp0\end{bmatrix}
            +\begin{bmatrix}1\amp0\\0\amp1\end{bmatrix}
            =
            \begin{bmatrix}0\amp0\\0\amp0\end{bmatrix}
            </me>
            
            and so we get the (matrix) equation <m>P(B)=0</m>.
            When a matrix <m>B</m> satisfies the equation <m>P(B)=0</m>, we call 
            <m>B</m> a <em>root</em> of <m>p(x)</m>.
            </p>
        </subsection>
    
    </section>
    
         
    <section><title> Inverses and powers: Rules of Matrix Arithmetic </title>
    
        <subsection><title> What about division of matrices? </title>
    
        <p>
        We have considered addition, subtraction and multiplication
        of matrices. What about division? When we consider real numbers,
        we can write <m>\tfrac ba</m> as <m>b\cdot\tfrac 1a.</m> In
        addition, we may think of <m>\tfrac 1a</m> as the multiplicative
        inverse of <m>a</m>, that is, it is the number which, when
        multiplied by <m>a</m> yields <m>1.</m> In other words, if we
        set <m>a^{-1}=\tfrac1a</m>, then <m>a\cdot a^{-1}=a^{-1}\cdot
        a=1.</m> Finally, <m>1</m> is the multiplicative identity, that
        is, <m>r1=1r=r</m> for any real number <m>r</m>.  While these
        concepts can not be extended to matrices completely, there are
        some circumstances when they do  make sense.
        </p>
    
        <p>
        First, we can note that <m>1\times1</m> matrices satisfy
        <m>[a] + [b] = [a+b]</m> and <m>[a][b]=[ab]</m>.  This means
        that both addition and multiplication of these matrices are
        just like the addition and multiplication of the real numbers.
        In this sense, matrices may be thought of as a generalization
        of the real numbers.
        </p>
    
        <p>
        Next we remember that if <m>A</m> is <m>m\times n</m>, then
        <m>I_mA=A=AI_n.</m> This means that the identity matrix (or,
        more properly, matrices) acts in the same way as <m>1</m> does
        for the real numbers.  This also means that if we want there
        to be a (single) matrix <m>I</m> satisfying <m>IA=A=AI</m>,
        then we must have <m>m=n</m>. This means we have to restrict
        ourselves to square matrices.
        </p>

        <p>
        If <m>A</m> is an <m>n\times n</m> matrix, then
        <m>I_nA=A=AI_n,</m> and so <m>I_n</m> acts in the same manner as
        does <m>1</m> for the real numbers.  Indeed, that is the reason
        it is called the identity matrix.
        </p>

        <p>
        Finally, we want to find (if possible) a matrix <m>A^{-1}</m>
        so that <m>A^{-1}A=AA^{-1}=I.</m> When such a matrix exists,
        it is called the <em>inverse</em> of <m>A</m>, and the matrix
        <m>A</m> itself is called <em>invertible.</em>
        </p>

        <definition xml:id="InverseMatrixDefinition"> <title>The inverse of a matrix</title> 
        <statement> 
        <p> 
            Let <m>A</m> be a square
            matrix. If there exists a matrix <m>B</m> so that <me> AB=BA=I
            </me> then <m>B</m> is called the <term>inverse of <m>A</m></term>
            and it is written as <m>A^{-1}</m>.  
        </p> 
        </statement> 
        </definition>

        <definition>
        <title>Matrix invertability</title>
        <statement> 
        <p> 
            A matrix <m>A</m> is <term>invertible</term> if it has an inverse,
            that is, if the matrix <m>A^{-1}</m> exists.  
        </p> 
        </statement>
        </definition>

        </subsection>

        <subsection><title>Properties of the Inverse of a Matrix</title>
        <p> 
            We consistently refer to <em>the</em> inverse of <m>A</m>
            rather than <em>an</em> inverse of <m>A,</m> which would seem
            to imply that a matrix can have only one inverse.  This is
            indeed true.
        </p>

        <theorem> <title> Uniqueness of Inverse </title> <statement>
        <p>
            A square matrix <m>A</m> can have no more than one inverse.
        </p>
        </statement>

        <proof> 
        <p>
            Suppose we have matrices <m>B</m> and <m>C</m>
            which both act as inverses, that is, <m>AB=BA=I</m> and
            <m>AC=CA=I</m>. We evaluate <m>BAC</m> in two different ways
            and equate the results: <me> BAC=(BA)C=IC=C\\ BAC=B(AC)=BI=B,
            </me> and so <m>B=C</m>.  
        </p> 
        </proof> 
        </theorem>


        <paragraphs> <title>Inverse Test </title>
        <p> 
            If <m>A</m> and
            <m>B</m> are square matrices of the same size, then <m>B</m> is
            a <term>left inverse</term> of <m>A</m> if <m>BA=I.</m> Similarly,
            it is a <term>right inverse</term> of <m>A</m> if <m>AB=I</m>. 
        </p>
        <p>
            By definition <m>B</m> is the inverse of <m>A</m> if
            <m>AB=BA=I,</m> that is, <m>B</m> is both a left inverse and a
            right inverse. We will show presently that if <m>B</m> is a right
            inverse of a square matrix <m>A</m>, then it is also a left inverse
            of <m>A</m> and hence the inverse of <m>A</m>.  
        </p>
        <p> 
            We next make an observation about the reduced row echelon
            form of square matrices: 
        </p> 
        </paragraphs>

        <lemma> <title >The Reduced Row Echelon Form for Square Matrices
        </title>
        <statement> 
        <p> 
        If <m>A</m> is an <m>n\times n</m> matrix then 
            <ol> 
            <li><p> 
                The reduced row echelon form of <m>A</m> is <m>I_n,</m> or 
                </p></li> 
                <li><p> 
                The last row of the reduced row echelon form of <m>A</m> is all zero. 
            </p></li> 
            </ol> 
        </p>
        </statement>

        <proof> 
        <p> 
            If every row in the reduced row echelon form of
            <m>A</m> has a leading one, then, since <m>A</m> has the same
            number of rows as columns, so does every column.  This means that
            the leading ones must be on the diagonal, and the every other
            entry of the matrix is zero. In other words, the reduced row
            echelon form is <m>I_n.</m> If, on the other hand, some row does
            not have a leading one, then it is an all-zero row.  Since these
            rows are at the bottom of the matrix when it is in reduced row
            echelon form, the last row, in particular, must be all zero.
        </p> 
        </proof> 
        </lemma>

        <definition xml:id="SingularMatrixDefinition"> <title>Matrix
        singularity</title> 
        <statement> 
        <p> 
            A square matrix is <term>nonsingular</term> if its reduced 
            row echelon form is <m>I</m>. Otherwise it is <term>singular</term>.
        </p>
        </statement> 
        </definition>

        <p> 
        Next we give a criterion for nonsingularity. It is trivial
        that if <m>\vec x=\vec0,</m> then <m>M\vec x=\vec0.</m> If this is the
        only vector <m>\vec x</m> for which this is true, then <m>M</m>
        is nonsingular.  
        </p>

        <lemma> <title>Condition for Singularity </title> 
        <statement> 
            <p>
            <m>M</m> is nonsingular if and only if <m>Mx=\vec0</m> 
            implies <m>\vec x=\vec0</m>. 
            </p> 
        </statement> 
        <proof> 
            <p> First, suppose
            that <m>M</m> is nonsingular. The the equation <m>M\vec x=\vec0</m>
            has an augmented matrix which, in reduced row echelon form,
            gives the equation <m>I\vec x=\vec0</m>. Hence <m>\vec x=\vec0</m>. 
            </p>

            <p>
            Now suppose that <m>M</m> is singular. The reduced row echelon
            form is not <m>I_n,</m>  and so some column does not contain a
            leading 1, that is, there must exist a free variable. It can be
            assigned a nonzero value, and thus provide a nonzero solution
            to <m>M\vec x=\vec0.</m> 
            </p> 
        </proof> 
        </lemma>

        <lemma> <title> AB=I and Nonsingularity </title> 
        <statement>
        <p> 
        If <m>AB=I</m> then <m>B</m> is nonsingular.
        </p> 
        </statement>

        <proof> 
        <p> 
            Suppose that <m>B\vec x=\vec0.</m> Multiply both sides
            of the equation by <m>A</m>: <me> A(B\vec x)=A(\vec0)=\vec0\\
            A(B\vec x)=(AB)\vec x=I\vec x=\vec x </me> and so <m>\vec x=\vec0.</m> Hence
            <m>B\vec x=\vec0</m> implies <m>\vec x=\vec0</m> and so <m>B</m> is
            nonsingular.  
        </p> 
        </proof> 
        </lemma>

        <proposition> <title> AB=I implies BA=I </title>
        <statement>
        <p> 
            Suppose the <m>A</m> and <m>B</m> are square matrices with
            <m>AB=I.</m> Then <m>BA=I.</m> </p> </statement> <proof> <p> From
            the previous lemma we know that <m>B</m> is nonsingular. Hence
            we know how to find <!-- link goes here --> <m>C</m> which
            is a solution to the equation <m>BX=I</m>, that is, so that
            <m>BC=I.</m> We now evaluate <m>BABC</m> in two different
            ways and equate the results: 
            <me> BABC=B(AB)C=BIC=BC=I\\ BABC=(BA)(BC)=BA(I)=BA </me> 
        </p> 
        </proof> 
        </proposition>

        <p> We get an important result from this Proposition.  </p>

        <theorem> <title> A Right Inverse is an Inverse </title>
        <statement>
        <p> 
            Suppose <m>A</m> and <m>B</m> are square matrices
            with <m>AB=I.</m> Then <m>B=A^{-1}.</m> 
        </p> 
        </statement>

        <proof> 
        <p> 
            By the Proposition above, <m>AB=I</m> implies
            <m>BA=I.</m> <!-- link goes here --> Since the inverse of <m>A</m>
            is unique, <m>B=A^{-1}.</m> 
        </p> 
        </proof> 
        </theorem>

        <paragraphs><title>New Inverse Test </title>
        <p> 
        If <m>A</m>
        and <m>B</m> are square matrices then <m>B</m> is the inverse
        of <m>A</m> if and only if <m>AB=I.</m> 
        </p> 
        </paragraphs>

        <p>Here is an application of the previous theorem:</p>

        <theorem> <title> Exponents and Transpose </title>
        <statement> 
        <p> 
            If <m>A</m> is a square matrix with inverse
            <m>A^{-1}</m> then <m>(A^T)^{-1}=(A^{-1})^T</m> 
        </p> 
        </statement>
        <proof> 
        <p> 
            Let <m>B=(A^{-1})^T.</m> Then <me> A^TB=A^T(A^{-1})^T
            = (A^{-1}A)^T=I^T=I </me> and so <m>B=(A^T)^{-1}</m>.  
        </p>
        </proof> 
        </theorem>


        <p> Here is another application of the previous theorem: </p>

        <theorem> <title> Inverse of Product of Matrices </title>
        <statement>
        <p> 
            If <m>A</m> and <m>B</m> are invertible
            matrices of the same size, then <m>AB</m> is also invertible
            and <m>(AB)^{-1}=B^{-1}A^{-1}</m> 
        </p> 
        </statement>

        <proof> 
        <p>
            Since <me> (AB)(B^{-1}A^{-1})=
            A(BB^{-1})A^{-1}=AIA^{-1}=AA^{-1}=I, </me> it follows that
            <m>B^{-1}A^{-1}</m> is the inverse of <m>AB</m>.  
        </p> 
        </proof>
        </theorem> 
        
        </subsection>


        <subsection><title>The Computation of the Inverse of a Matrix</title>

        <p> 
        Suppose we have a square matrix <m>A</m> and the reduced
        row echelon form of <m>A</m> is <m>I</m> (that is, <m>A</m> is
        nonsingular). <m>X</m> is the inverse of <m>A</m> if it satisfies
        the equation <m>AX=I.</m> We have seen <!-- link goes here -->
        how to solve such equations.  We conclude that if we start with
        the matrix <m>[A|I]</m> then the reduced row echelon form will
        be <m>[I|A^{-1}]</m>. This not only allows us to compute the
        inverse of <m>A</m> but it also shows that nonsingular matrices
        are invertible and vice-versa.  
        </p>

        <p> 
        Example: If we start with 
        <me>
        A=\begin{bmatrix}1\amp2\amp1\\2\amp3\amp5\\1\amp2\amp0\end{bmatrix},
        </me> 
        then 
        <me> 
        [A\mid I]=
        \left[\begin{array}{ccc|ccc}
             1\amp2\amp1\amp1\amp0\amp0\\
             2\amp3\amp5\amp0\amp1\amp0\\
             1\amp2\amp0\amp0\amp0\amp1
        \end{array}\right] 
        </me> 
        has, as its reduced row echelon form, 
        <me> [I\mid A^{-1}]=
        \left[\begin{array}{ccc|ccc}
            1\amp0\amp0\amp-10\amp2\amp7\\
            0\amp1\amp0\amp5\amp-1\amp-3\\
            0\amp0\amp1\amp1\amp0\amp-1
        \end{array}\right] 
        </me> 
        and so we conclude that 
        <me>
        A^{-1}=\begin{bmatrix}
             ]-10\amp2\amp7\\
             5\amp-1\amp-3\\
             1\amp0\amp-1
        \end{bmatrix}.  
        </me> 
        </p>

        <p> 
        <m>\textbf{The inverse of a}</m> <m>2\times2</m>
        <m>\textbf{matrix}</m> We start with the matrix 
        <me> 
            A=
            \begin{bmatrix} a\amp b\\c\amp d \end{bmatrix} 
        </me> 
        Now we carry out row reduction: 
        <me> 
           \left[ \begin{array}{cc|cc}
               a\amp b\amp1\amp0\rlap{\qquad R_1\gets \frac1a R_1}\\
               c\amp d\amp0\amp1
           \end{array} \right]  \\ 
           \left[ \begin{array}{cc|cc}
               1\amp\frac ba\amp\frac1a\amp0\\ 
               c\amp d\amp0\amp1 \rlap{\qquad R_2\gets R_2-cR_1}
           \end{array} \right]\\ 
           \left[ \begin{array}{cc|cc}
                1\amp\frac ba\amp\frac1a\amp0
                \\ 0\amp d-\frac{bc}a\amp-\frac ca\amp1
            \end{array} \right] \rlap{\hbox{(Rewrite last row)}}   \\
            \left[\begin{array}{cc|cc}
                1\amp\frac ba\amp\frac1a\amp0\\ 
                0\amp\frac{ad-bc}a\amp-\frac ca\amp1 \rlap{\qquad R_2\gets \frac a{ad-bc}R_2}
            \end{array} \right]\\ 
            \left[ \begin{array}{cc|cc}
                1\amp\frac ba\amp\frac1a\amp0\\ 
                0\amp1\amp-\frac c{ad-cb}\amp \frac a{ad-cb} \rlap{\qquad R_1\gets R_1-\frac ba R_2}
            \end{array}\right]\\ \left[ \begin{array}{cc|cc}
                1\amp0\amp \frac d{ad-cb}\amp -\frac b{ad-cb}   \\
                0\amp1\amp-\frac c{ad-cb}\amp \frac a{ad-cb}
            \end{array} \right] 
        </me>

        On the face of it, this seems to say 
        <me> 
            A^{-1} 
            =
            \begin{bmatrix}
                \frac d{ad-cb}\amp -\frac b{ad-cb} \\
                -\frac c{ad-cb}\amp \frac a{ad-cb}
            \end{bmatrix} 
            = \frac1{ad-cb}
            \begin{bmatrix} d\amp-b\\ -c\amp a \end{bmatrix}
        </me>

        But notice that we have blithely ignored the possibility that
        <m>a=0</m> or that <m>ad-bc=0</m>. Nonetheless we may compute:

        <me> 
        \begin{bmatrix} a\amp b\\c\amp d \end{bmatrix}
        \begin{bmatrix} d\amp-b\\-c\amp a \end{bmatrix} =
        \begin{bmatrix} ad-bc\amp0\\0\amp ad-bc \end{bmatrix}
        =(ad-bc)I </me>

        Hence if 
        <me> 
            A=
            \begin{bmatrix} a\amp b\\c\amp d \end{bmatrix} 
            \hbox{ and } 
            B=
            \frac1{ad-bc}\begin{bmatrix} d\amp-b\\-c\amp a \end{bmatrix}
        </me> 
        then 
        <me> AB=I </me> 
        and so 
        <me>    B=A^{-1}
        </me> 
        we conclude that if 
        <me> 
        A= \begin{bmatrix} a\amp b\\c\amp d \end{bmatrix} 
        </me> 
        where <m>ad-bc\neq 0</m> then 
        <me> 
        A^{-1}=
        \frac1{ad-bc}\begin{bmatrix} d\amp-b\\-c\amp a \end{bmatrix}
        </me> 
        </p> 
        </subsection>


        <subsection> <title>Applying the Inverse of a Matrix to Systems of Linear Equations</title>
        <theorem><title> Solving Equations Using the Matrix Inverse
        </title> <statement>
        <p> 
            If a system of linear equations is given by the
            equations <m>Ax=b</m>, and <m>A</m> has an inverse, then
            <m>x=A^{-1}b</m>.  
        </p>
        </statement> 
        <proof>
        <p> 
            We take the equation <m>Ax=b</m> and multiply
            both sides by <m>A^{-1}:</m> 
            <me> 
            A^{-1}(Ax)=A^{-1}b\\
            A^{-1}(Ax)=(A^{-1}A)x=Ix=x 
            </me> 
            and so <m>x=A^{-1}b</m>.
        </p>
        </proof>
        </theorem>


        <example> <title>Solving a system of linear equations using
        the matrix inverse</title> 
        <statement> 
        <p>
        Suppose we want to
        solve the system of equations 
        <me> 
            \begin{array}{rrrrrrr}
            %x_1\amp+\amp2x_2\amp+\ampx_3\amp=\amp1\\
            %2x_1\amp+\amp3x_2\amp+\amp5x_3\amp=\amp1\\
            %x_1\amp+\amp2x_2\amp\amp\amp=\amp1
            x_1+2x_2+x_3\amp=\amp1\\ 
            2x_1+3x_2+5x_3\amp=\amp1\\
            x_1+2x_2\phantom{+0x_3}\amp=\amp1
            \end{array} 
        </me> 
        </p>

        <p>Then let 
        <me>
        A=
        \begin{bmatrix}
            1\amp2\amp1\\
            2\amp3\amp5\\
            1\amp2\amp0
        \end{bmatrix}
        \hbox{ and } 
        b=
        \begin{bmatrix}
            1\\1\\1
        \end{bmatrix}
        </me> 
        so that we are solving <m>Ax=b.</m> We have
        already done the computation to determine that 
        <me>
        A^{-1}=
        \begin{bmatrix}
            -10\amp2\amp7\\
            5\amp-1\amp-3\\
            1\amp0\amp-1
        \end{bmatrix}.  
        </me> 
        Hence 
        <me> 
            x=
            \begin{bmatrix}
                -10\amp2\amp7\\ 
                5\amp-1\amp-3\\ 
                1\amp0\amp-1
            \end{bmatrix} 
            \begin{bmatrix}1\\1\\1\end{bmatrix}
            = 
            \begin{bmatrix}-1\\1\\0\end{bmatrix}, 
        </me> and the (only)
        solution is <m>x_1=-1, x_2=1, x_3=0.</m> 
        </p> 
        </statement>
        </example> 
        </subsection>
    </section>


    <section><title>Some special matrices</title> 
    <introduction>
        <p> 
        There are several matrices that repeatedly show up in many
        different mathematical investigations. These matrices are given
        particular names.  We gather the most important ones and present
        them here.  
        </p>
    </introduction>

        <subsection><title>Square matrices</title> 
        <definition><title>Square matrices</title>
        <statement>
        <p> 
            A <term>square matrix</term> has the same number
            of rows and columns, this is, it is <m>n\times n</m>. The number
            <m>n</m> is called the <term>order of the matrix</term>. 
        </p>
        </statement>
        </definition>

        <p> 
        Here are some square matrices of order <m>3</m>:  
        <me> 
            A=
            \begin{bmatrix} 
            1\amp2\amp3\\
            6\amp2\amp-1\\
            3\amp4\amp4
            \end{bmatrix} 
            \qquad
            I_3= 
            \begin{bmatrix} 
            1\amp0\amp0\\
            0\amp1\amp0\\
            0\amp0\amp1
            \end{bmatrix}
            \qquad
            0_3= 
            \begin{bmatrix} 
            0\amp0\amp0\\
            0\amp0\amp0\\
            0\amp0\amp0
            \end{bmatrix}
        </me>.
        </p>

        <definition> <title>Zero matrix</title> 
        <statement>
        <p> 
            The <term>zero matrix</term> <m>0_n</m> is the square
            matrix of order <m>n</m> with every entry equal to <m>0</m>.
        </p>
        </statement> 
        </definition>

        <definition> <title>Identity matrix</title>
        <statement>
        <p> 
            The <term>identity matrix</term> <m>I_n</m> is a
            square matrix of order <m>n</m> that looks like 
            <me> 
            I_n=
            \begin{bmatrix} 
                1\amp0\amp0\amp\cdots\amp0\amp0\\
                0\amp1\amp0\amp\cdots\amp0\amp0\\
                0\amp0\amp1\amp\cdots\amp0\amp0\\
                \amp\amp\amp\ddots\amp\amp\\ 
                0\amp0\amp0\amp\cdots\amp1\amp0\\
                0\amp0\amp0\amp\cdots\amp0\amp1 
                \end{bmatrix} 
            </me>
            More specifically, if <m>I_n=[a_{i,j}]</m>, then 
            <me>
            a_{i,j}
            = 
            \begin{cases} 
                1 \amp \textrm{ if } i=j\\ 
                0 \amp
            \textrm{otherwise} \end{cases} 
            </me> 
            When it is not necessary
            to emphasize the order of the matrix, the identity matrix
            is simply written as <m>I</m>.  </p>
        </statement> </definition>

        </subsection>

        <subsection><title>Diagonal matrices</title>
        <p> 
            For any square matrix <m>A</m> of order <m>n</m>, 
            the <term>diagonal entries</term> are
                <m>a_{1,1},a_{2,2},a_{3,3},\ldots, a_{n,n}</m>:
                <me> 
                A= 
                \begin{bmatrix} 
                \color{red}{a_{1,1}} \amp a_{1,2}\amp\cdots \amp a_{1,n-1}\amp a_{1,n}\\ 
                a_{2,1} \amp \color{red}{a_{2,2}}\amp\cdots \amp a_{2,n-1}\amp a_{2,n}\\ 
                \amp\amp\ddots\amp\\ 
                a_{n-1,1} \amp a_{n-1,2}\amp\cdots \amp \color{red}{a_{n-1,n-1}} \amp a_{n-1,n}\\ 
                a_{n,1} \amp a_{n,2}\amp\cdots \amp a_{n,n-1}\amp\color{red}{a_{n,n}} 
                \end{bmatrix} 
                </me>

                So these are the entries that start at the upper-left
                corner of the matrix and go down the diagonal to the
                lower-right one. This is also called the <term>main
                diagonal</term> of the matrix.

                Clearly we can describe an identity matrix as one
                whose diagonal entries are <m>1</m> and whose
                remaining entries are <m>0</m>.  
        </p>

        <definition> <title>Diagonal matrices</title>
		    <statement> 
            <p>
		        A <term>diagonal matrix</term> is one for which
		        nonzero entries may only occur on the main diagonal.
		    </p> 
            </statement>
	    </definition>

		<p> 
            This means that the matrix is of the form 
            <me>
		    A=
            \begin{bmatrix} 
                a_{1,1} \amp 0 \amp 0 \amp 0 \amp \cdots \amp 0 \\ 
                0 \amp a_{2,2} \amp 0 \amp 0 \amp \cdots \amp 0 \\ 
                0 \amp 0 \amp a_{3,3} \amp 0 \amp \cdots \amp 0 \\ 
                0 \amp 0 \amp 0 \amp a_{4,4} \amp \cdots \amp 0 \\ 
                \amp\amp\amp\amp\ddots\\ 
                0 \amp 0 \amp 0 \amp 0 \amp \cdots \amp a_{n,n} 
            \end{bmatrix}.
		    </me> 
            Once we know the diagonal entries, we know the
		    whole matrix. Sometimes we abbreviate this as 
            <me>
		        A=\mathrm{diag}(a_{1,1},\ldots,a_{n,n}). 
            </me>

		    An example of a diagonal matrix is the identity matrix <m>I</m>:

		    <me> 
            I= 
            \begin{bmatrix} 
                1\amp0\amp\cdots\amp0\amp0\\
		        0\amp1\amp\cdots\amp0\amp0\\ 
                \amp\amp\ddots\\
		        0\amp0\amp\cdots\amp1\amp0\\ 
                0\amp0\amp\cdots\amp0\amp1
		    \end{bmatrix} 
            =\mathrm{diag}(1,\ldots,1) 
            </me>

		    An alternative way of describing a diagonal matrix 
		    <m>A=[a_{i,j}]</m> is by the condition that <m>a_{i,j}=0</m>
		    whenever <m>i\not=j</m>.  
        </p>

	    <paragraphs><title>Multiplication of diagonal matrices</title>
		    <p>
            Multiplication of diagonal matrices is particularly
		    easy. If 
            <me> 
            D=\mathrm{diag}(d_1,d_2,\ldots,d_n)
		    </me> 
            and 
            <me> 
            E=\mathrm{diag}(e_1,e_2,\ldots,e_n)
		    </me> 
            then it is easy to verify that 
            <me>
		    DE=\mathrm{diag}(d_1e_1,d_2e_2,\ldots,d_ne_n) 
            </me>. 
            </p>
	    </paragraphs>

	    </subsection>

	    <subsection><title>Symmetric matrices</title>

	    <definition> <title>Symmetric matrix</title> <statement>
		    <p> 
                A matrix <m>A=[a_{i,j}]</m> is <term>symmetric</term>
		        if <m>a_{i,j}=a_{j,i}</m> for all <m>i,j=1,2,\ldots,n</m>.
		        Alternatively, we may write this as <m>A=A^T</m>.  
            </p>
	        </statement> 
        </definition>

	    <p> 
            The following matrix is symmetric: 
            <me>
		    \begin{bmatrix} 
            0\amp1\amp2\amp3\amp4\\
		    1\amp5\amp6\amp7\amp8\\ 
            2\amp6\amp9\amp10\amp11\\
		    3\amp7\amp10\amp12\amp13\\ 
            4\amp8\amp11\amp13\amp14\\
		\end{bmatrix}
	    </me>. 
        Notice that the rows <m>R_1, R_2, R_3, R_4, R_5</m>
	    and columns <m>C_1, C_2, C_3, C_4, C_5</m> satisfy 
        <md>
	    <mrow>R_1\amp =C_1</mrow> <mrow>R_2\amp =C_2</mrow>
	    <mrow>R_3\amp =C_3</mrow> <mrow>R_4\amp =C_4</mrow>
	    <mrow>R_5\amp =C_5</mrow> 
        </md>

	    In addition, there is a geometric property. The entry
	    <m>a_{j,i}</m> can be derived from <m>a_{i,j}</m> by
	    reflection across the diagonal.

	    <me> 
        \begin{bmatrix} 
            *\amp\amp\amp\cdots \amp\amp a_{i,j}\amp\\ 
            \amp*\amp\amp\cdots \amp\amp\amp\\ 
            \amp\amp*\amp\cdots \amp\amp\amp\\
	        \amp\amp\amp\ddots\amp\amp\amp\\ 
            \amp\amp\amp\cdots \amp*\amp\\ 
            \amp\amp\amp\cdots \amp\amp*\amp\\ 
            \amp a_{j,i}\amp\amp\cdots \amp\amp\amp*
        \end{bmatrix} 
        \qquad
	    a_{i,j}=a_{j,i} 
        </me> 
        </p>
	</subsection>


	<subsection><title>Triangular matrices</title>

	    <definition> <title>Upper triangular matrices</title>
		<statement> 
        <p> 
        A matrix is <term>upper triangular</term>
		if every nonzero entry is on or above the main
		diagonal. This means that an upper triangular matrix
		<m>A=[a_{i,j}]</m> satisfies 
        <me>
        a_{i,j}=0 \textrm{ if } i\gt j.
        </me>
		 </p>
		</statement>
	    </definition>

	    <p> 
        Notice what we use here.  An entry is below the main
	    diagonal if the row number of the entry is greater than the
	    column number. In other words, <m>a_{i,j}</m> is below the
	    main diagonal if and only if <m>i \gt j</m>.

	    An upper triangular matrix <m>A</m> has the following pattern
	    (<m>*</m> may be zero or nonzero):
	    <me> 
        A= 
        \begin{bmatrix} 
            *\amp*\amp*\amp*\\ 
            0\amp*\amp*\amp*\\
	        0\amp0\amp*\amp*\\ 
            0\amp0\amp0\amp* 
        \end{bmatrix} 
        </me>
	    This matrix is upper triangular: 
        <me>
		A= 
        \begin{bmatrix} 
            1\amp2\amp3\amp4\\ 
            0\amp5\amp6\amp7\\
		    0\amp0\amp8\amp9\\ 
            0\amp0\amp0\amp10 \end{bmatrix}
	    </me>

	    A lower triangular matrix may be thought of at the transpose
	    of an upper triangular matrix.  
        </p>


	    <definition> <title>Lower triangular matrices</title>
	    <statement> 
        <p> 
        A matrix is <term>lower triangular</term> if
	    every nonzero entry is on or below the main diagonal. This
	    means that an lower triangular matrix <m>A=[a_{i,j}]</m>
	    satisfies 
        <me>
        a_{i,j}=0 \textrm{ if } i\lt j.
        </me> 
        </p>
	    </statement>

	    </definition> 
        <p> 
        The following matrix is lower triangular:
	    <me> 
        B= \begin{bmatrix} 
            1\amp0\amp0\amp0\\ 
            2\amp3\amp0\amp0\\
	        4\amp5\amp6\amp0\\ 
            6\amp7\amp8\amp9 
            \end{bmatrix} 
        </me> 
        </p>

	    <definition> <title>Triangular matrix</title> 
        <statement>
	    <p> 
        A matrix is <term>triangular</term> if it is either upper
	    triangular or lower triangular 
        </p> 
        </statement> 
        </definition>

		<theorem> <title>Product of upper triangular matrices is upper triangular</title> 
        <statement> 
        <p>
	        <ul>
		    <li><p> 
                If <m>A</m> and <m>B</m> are upper triangular matrices,
		        the <m>AB</m> is also upper triangular.
            </p></li>
		    <li><p> 
                If <m>A</m> and <m>B</m> are lower triangular matrices,
		        the <m>AB</m> is also lower triangular.
            </p></li>
		    </ul>
	    </p>
		</statement>

		<proof>
        <p> 
            We compute the <m>(i,j)</m> entry of <m>AB</m>
		    by considering row <m>i</m> of <m>A</m> and column
		    <m>j</m> of <m>B</m>: 
            <me> 
            (AB)_{i,j} =a_{i,1}b_{1,j}+ a_{i,2}b_{2,j}+\cdots+ a_{i,n}b_{n,j} 
            </me>
		    Now <m>A</m> and <m>B</m> being upper triangular implies
	        <m>a_{i,1}=a_{i,2}=\cdots=a_{i,i-1}=0</m>
		    and <m>b_{j,j+1}=b_{j,j+2}=\cdots=b_{j,n}=0</m>. This
		    means that
	       <me> 
           (AB)_{i,j}=a_{i,i}b_{i,j}+ a_{i,i+1}b_{i+1,j}
               +\cdots+ a_{i,j-1}b_{j,j-1}+a_{i,j}b_{j,j} 
           </me>
	        Hence we see that <m>(AB)_{i,j}\not=0</m> only if <m>i\leq
	        j</m>, or, <m>(AB)_{i,j}=0</m> whenever <m>i \gt j</m>. This,
	        by definition, makes <m>AB</m> upper triangular.

	        The argument for lower triangular matrices <m>A</m> and
	        <m>B</m> is essentially identical.  
        </p> 
        </proof>

	   <proof> 
       <p>
	        We consider the <m>(i,j)</m> entry of <m>AB</m> by considering
	        row <m>i</m> of <m>A</m> and column <m>j</m> of <m>B</m>: 
            <me>
		    (AB)_{i,j} =a_{i,1}b_{1,j}+ a_{i,2}b_{2,j}+\cdots+
		    a_{i,n}b_{n,j}.  
            </me>
	        If <m>(AB)_{i,j} \not= 0</m>, then there is some <m>k</m>
	        so that
	        <m>a_{i,k}b_{k,j}\not=0</m>. This means that
	        <m>a_{i,k}\not=0</m>, and, since <m>A</m> is upper triangular,
	        we have <m>k \geq i </m>.  Similarly <m>b_{k,j}\not=0</m>
	        implies <m>j \geq k </m>. Hence, if  <m>(AB)_{i,j} \not=
	        0</m>, then <m>j \geq k \geq i</m>, which makes <m>AB</m>
	        upper triangular.
	   </p> 
       </proof>
	   </theorem>

	   <theorem> <title>An upper triangular matrix is invertible if
	   and only if all the diagonal entries are nonzero</title>
	   <statement> 
       <p>
	   An upper triangular matrix <m>A</m> is invertible if and only
	   if every diagonal entry <m>A</m> is nonzero.  
       </p> 
       </statement>

	   <proof> 
       <p> 
       Suppose all diagonal entries of <m>A</m> are
	   nonzero, and <m>A</m> is of size <m>n</m>.  If we carry out
	   the	elementary operations <m>R_i\gets\frac1{a_{i,i}}R_i</m>
	   for <m>i=1,\dots,n</m>, then the diagonal elements are all
	   <m>1</m>. Suppose <m>a_{i,j}</m> is some entry above the
	   diagonal (so <m>i \lt j</m>). Then we use the elementary row
	   operation <m>R_j\gets R_j-a_{ij}R_i</m> to change that entry to
	   <m>0</m>. We may proceed by columns from left to right deleting
	   every <m>a_{i,j}\neq0</m> By this process we reduce <m>A</m>
	   to the matrix <m>I</m>, and so <m>A</m> is invertible.  
       </p>

	   <p> On the other hand, if some diagonal element is zero, then
	   it stays zero as we row reduce an upper triangular matrix
	   (since <m>R_i\gets R_i+\lambda R_j</m> only occurs when 
       <m>i \lt j</m>). That implies that the variable corresponding to
	   that column is free, and that the matrix in not invertible.
	   </p> 
       </proof> 
       </theorem>

	   <theorem> <title> The inverse of an upper triangular matrix is upper triangular </title> 
       <statement> 
       <p> 
        If <m>A</m> is upper triangular and invertible, then <m>A^{-1}</m> is
	    also upper triangular.  
       </p> 
       </statement>

	   <proof> 
       <p> 
       When <m>A</m> is row reduced to <m>I</m>, each row
	   operation corresponds to an elementary matrix that is diagonal
	   or upper triangular.  Since <m>A^{-1}</m> is the product
	   of these elementary matrices, it is also upper triangular.
	   </p> 
       </proof> 
       </theorem>
	</subsection>

	<subsection><title>Permutation matrices</title>

	    <definition> <title>Permutation matrix</title>
		<statement> 
        <p> 
        A <term>permutation matrix</term> is a
		square matrix with two properties: 
        <ol> 
            <li><p> 
            Each entry of the matrix is either <m>0</m> or <m>1</m>. 
            </p></li>
		    <li><p> 
            Every row and every column contains exactly one <m>1</m>.   
            </p></li> 
        </ol> 
        </p> 
        </statement>

	    </definition> 
        <p>
        The only permutation of order <m>1</m>
	    is <m>\begin{bmatrix}1\end{bmatrix}</m>.
        </p>

	    <p>There are two permutation matrices of order <m>2</m>:
	    <me>
        \begin{bmatrix}1\amp0\\ 0\amp1\end{bmatrix}\qquad
	    \begin{bmatrix}0\amp1\\1\amp0\end{bmatrix} 
        </me> 
        </p>

	    <p>There are six permutation matrices of order <m>3</m>:
	    <me> 
            \begin{bmatrix} 
                1\amp0\amp0\\ 
                0\amp1\amp0\\
	            0\amp0\amp1
            \end{bmatrix}
            \qquad 
            \begin{bmatrix}
                1\amp0\amp0\\
	            0\amp0\amp1\\ 
                0\amp1\amp0
            \end{bmatrix}
            \qquad
	        \begin{bmatrix}
                0\amp1\amp0\\ 
                1\amp0\amp0\\
	            0\amp0\amp1
            \end{bmatrix} 
        </me>

	    <me> 
        \begin{bmatrix}
            0\amp0\amp1\\ 
            1\amp0\amp0\\
	        0\amp1\amp0
        \end{bmatrix}
        \qquad 
        \begin{bmatrix}
            0\amp0\amp1\\
	        0\amp1\amp0\\ 
            1\amp0\amp0
        \end{bmatrix}\qquad
	    \begin{bmatrix}
            0\amp1\amp0\\ 
            0\amp0\amp1\\
	        1\amp0\amp0
        \end{bmatrix} 
        </me> 
        </p>

	</subsection>

    </section>


    <section><title>Powers of a matrix (negative exponents)</title>
	<p> 
    Suppose we have a square matrix <m>A</m>.  For positive
    <m>m</m> and <m>n</m> we have proven the law of exponents:
	in <xref ref="LawOfExponents" />.  
        <ul>
	    <li>
            <m>A^n=\underbrace{A\cdots A}_{n \text{ factors}},</m> 
            (Note that <m>A^1=A</m>) 
        </li>

	    <li>
            <m>A^m A^n=\underbrace{A\cdots A}_{m \text{ factors}}
                \underbrace{A\cdots A}_{n \text{ factors}}
	        =\underbrace{A\cdots A}_{m+n \text{ factors}}=A^{m+n},</m>
	    </li>

	    <li>
            <m>(A^m)^n =\underbrace{(\underbrace{A\cdots A}_{m
	        \text{ factors}})
	       (\underbrace{A\cdots A}_{m \text{ factors}})\cdots
	       (\underbrace{A\cdots A}_{m \text{ factors}})}_{n \text{
	       times}}=A^{mn}
	       </m>
        </li>
	    </ul>

	Our goal is to extend this law for invertible matrices so that
	it is valid for any integer exponents.  It turns out that there
	is only one way to do so, and we will see this in successive
	steps.</p>


    <p>
	<ul>
	    <li> <m>n=0</m>: For any matrix <m>A</m>,
	    <me>A^mA^0=A^{m+0}=A^m\\A^0A^m=A^{0+m}=A^m,</me> and so
        it must be that <m>A^0=I</m>.
        </li>

	    <li><m>m=1, n=-1</m>: <me>A^1
	    A^{-1}=A^{1+(-1)}=A^0=I\\A^{-1} A^1=A^{-1+1}=A^0=I </me> so
	    <m>A^{-1}</m> is the inverse of <m>A</m> (and hence the notation).
	    </li>

	    <li><m>n=-m</m>: <me>A^m A^{-m}=A^{m+(-m)}=A^0=I\\A^{-m}
	    A^m=A^{-m+m}=A^0=I</me> so <m>A^{-m}</m> is the inverse of
	    <m>A^m.</m> Notationally this says <m>A^{-m}=(A^m)^{-1}</m>.
	    </li>
	</ul>
    </p>

    <theorem> <title> Laws of Exponents </title> 
    <statement>
	<p>
        For any invertible square matrix <m>A</m> and integers <m>m</m>
	    and <m>n</m>, 
        <ul>
	    <li> <p><m>A^mA^n=A^{m+n}</m></p> </li>
	    <li> <p><m>(A^m)^n=A^{mn}</m></p> </li>
	    <li> <p><m>(A^{-1})^{-1}=A</m></p> </li>
	    <li> <p>(<m>A^n)^{-1}=(A^{-1})^n</m></p> </li>
	    <li> <p><m>(rA)^{-1}=\tfrac 1r A^{-1}</m></p> </li>
	    </ul> 
    </p> 
    </statement>

	<proof>
    <p>
        The first two results have already been verified. The
	    method for the last three is similar: 
        <ul>
        <li><p> 
            <m>(A^{-1})A=I</m> implies that  <m>A</m> is the inverse 
            of <m>A^{-1}</m>, that is, <m>A=(A^{-1})^{-1}.</m>
        </p></li>
        <li><p> 
            Evaluate so the the middle factors "disappear":
	        <me>\begin{array}{rl}
		    A^n(A^{-1})^n 
            \amp= \underbrace{A\cdots A}_{n\text{ factors}}\ 
               \underbrace{A^{-1}\cdots A^{-1}}_{n\text{ factors}} \\ 
            \amp= \underbrace{A\cdots A}_{n-1\text{ factors}}(AA^{-1})
               \underbrace{A^{-1}\cdots A^{-1}}_{n-1\text{ factors}} \\ 
            \amp= \underbrace{A\cdots A}_{n-1\text{ factors}}(I)
               \underbrace{A^{-1}\cdots A^{-1}}_{n-1\text{ factors}}\\ 
            \amp= \underbrace{A\cdots A}_{n-1\text{ factors}}
               \underbrace{A^{-1}\cdots A^{-1}}_{n-1\text{ factors}}\\ 
            \amp=\underbrace{A\cdots A}_{n-2\text{ factors}}(AA^{-1}) 
               \underbrace{A^{-1}\cdots A^{-1}}_{n-2\text{ factors}} \\ 
            \amp\vdots\\ 
            \amp= AA^{-1}\\ 
            \amp= I 
            \end{array}
	        </me> 
            and so <m>(A^{-1})^n</m> is the inverse of <m>A^n</m>.
        </p></li>
        <li><p>
            <m>(rA)(\tfrac 1r A^{-1}) = r\tfrac 1r A A^{-1}=1I=I</m> 
            andso <m>\tfrac 1r A^{-1}</m> is the inverse of <m>rA</m>.  
        </p></li>
	    </ul> 
    </p>
    </proof> 
    </theorem>

    </section>

    <section><title>Elementary matrices</title>
    <introduction>
	<p> 
        We put matrices into reduced row echelon form by a series of
	    elementary row operations.  
    <!-- add a link (<m>\textbf{link}</m> elementary row operations). --> 
        Our first goal is to show that
	    each elementary row operation may be carried out using matrix
	    multiplication.  The matrix <m>E=[e_{i,j}]</m> used in each case
	    is almost an identity matrix.  The product <m>EA</m> will carry
	    out the corresponding elementary row operation on <m>A</m>.  
    </p>
    </introduction> 
    
    <subsection><title>The three types of elementary matrices</title> 
    <p> 
    There are three different elementary row operations:
    <!-- Change to a table -->
	    <ol> 
        <li><p>
            Interchanging two rows (<m>R_i\leftrightarrow R_j)</m>
        </p></li>
        <li><p>
            Multiplying a row by a scalar (<m>R_i\gets \lambda R_i</m> where <m>\lambda\not=0)</m> 
        </p></li>
        <li><p>
            Adding a multiple of one row to another (<m>R_i\gets R_i+\lambda R_j)</m>
        </p></li> 
        </ol>
    We now define an elementary matrix <m>E=[e_{i,j}]</m> for each one of
    these operations: </p>

    <definition xml:id="ElementaryMatrices"> <title>Elementary matrices</title>
	<statement> 
    <p> 
        <ol>
	    <li><p>
            (<m>R_i\leftrightarrow R_j)</m> 
            <me>
            E_1=
            \begin{bmatrix} 
            1 \\ 
            \amp\ddots \\ 
            \amp\amp1\amp\amp\\
	        \amp\amp\amp0\amp\cdots\amp\cdots\amp\cdots\amp1\amp \\
            \amp\amp\amp\vdots\amp1\amp\amp\amp\vdots \\
	        \amp\amp\amp\vdots\amp\amp\ddots \amp\amp\vdots\\
	        \amp\amp\amp\vdots\amp\amp\amp1\amp\vdots \\
	        \amp\amp\amp1\amp\cdots\amp\cdots\amp\cdots\amp0 \\ 
            \amp\amp\amp\amp\amp\amp\amp\amp1 \\
	        \amp\amp\amp\amp\amp\amp\amp\amp\amp\ddots 
            \end{bmatrix} 
            </me>
	        In this case <m>e_{i,i}=e_{j,j}=0</m> and <m>e_{i,j}=e_{i,j}=1</m>.
	        All other entries are the same as those in <m>I</m>.
	    </p></li>

	    <li><p> 
            (<m>R_i\gets \lambda R_i</m> where <m>\lambda\not=0)</m>
	        <me>
            E_2=
            \begin{bmatrix} 
                 1 \\ 
                 \amp\ddots \\ 
                 \amp\amp1 \\ 
                 \amp\amp\amp\lambda \\ 
                 \amp\amp\amp\amp1 \\
                 \amp\amp\amp\amp\amp\ddots \\ 
                 \amp\amp\amp\amp\amp\amp1
            \end{bmatrix} 
            </me> 
            In this case <m>e_{i,i}=\lambda</m>.  All other entries are the same as those in
            <m>I</m>.
	    </p></li>

	    <li><p> 
            (<m>R_i\gets R_i+\lambda R_j)</m> 
            <me>
            E_3=
            \begin{bmatrix} 
                1 \\ 
                \amp\ddots \\ 
                \amp\amp1 \\ 
                \amp\amp\amp1\amp\cdots\amp\lambda \\ 
                \amp\amp\amp\amp1\amp\vdots \\ 
                \amp\amp\amp\amp\amp\ddots \\ 
                \amp\amp\amp\amp\amp\amp1 
            \end{bmatrix} 
            </me> 
            In this case <m>e_{i,j}=\lambda</m>.  All other entries are the same as those in <m>I</m>.
	    </p></li> 
        </ol> 
        </p> 
    
        <p>A matrix <m>E</m> of any of the three types is called an <term>elementary matrix.</term>
        </p> 
        </statement>
        </definition>
    </subsection>

    <subsection><title>Elementary matrices and reduced row echelon form</title>

	<theorem xml:id="ElementaryMatrixRowMultiplication"> <title>Carrying out row operations using matrix multiplication </title>
	<statement>
    <p>
	    Suppose we start with a matrix <m>A</m> and carry out one
	    elementary row operation to get the matrix <m>B</m>. Then there
	    is an elementary matrix <m>E_1</m>, <m>E_2</m>, or <m>E_3</m> so that:
    	<ol>
	    <li><p>
        Interchange rows (<m>R_i\leftrightarrow R_j</m>)
	    \[B=E_1A\]</p></li> <li><p>Multiply a row by a constant
	    (<m>R_i\leftarrow \lambda R_i</m>) \[B=E_2A\]
        </p></li>
	    <li><p>
        Add a multiple of one row to another (<m>R_i\leftarrow
	    R_i + \lambda R_j</m>) \[B=E_3A\]
        </p></li>
	    </ol> 
    </p> 
    </statement> 
    <proof> 
    <p> 
        We use the elementary matrices
	    given in <xref ref="ElementaryMatrices" />.
	    The proof is is then a careful observation of the effect of the
	    nonzero entries in each case.  
    </p> 
    </proof> 
    </theorem>

	<example> <title>Multiplying by an elementary matrix</title> 
    <statement> 
    <p> 
    Let 
    <me> A=
	    \begin{bmatrix} 
            1\amp2\amp3\amp4\amp5\amp6\\
	        7\amp8\amp9\amp10\amp11\amp12\\
            13\amp14\amp15\amp16\amp17\amp18\\
	        19\amp20\amp21\amp22\amp23\amp24\\
	        25\amp26\amp27\amp28\amp29\amp30 
        \end{bmatrix} 
    </me> 
    The entries in the elementary matrices in red are the only ones that differ
	from an identity matrix <m>I</m>.  
        <ol> 
        <li> <p> 
        Interchange rows 2 and 4 (<m>R_2\leftrightarrow R_4</m>) 
        <me>E= 
        \begin{bmatrix}
	        1\amp0\amp0\amp0\amp0\\ 0\amp{\color{red}0}\amp0\amp
	        {\color{red}1} \amp0\\ 0\amp0\amp1\amp0\amp0\\
	        0\amp{\color{red}1} \amp0\amp{\color{red}0}\amp0\\
	        0\amp0\amp0\amp0\amp1 \end{bmatrix} </me> <me>EA= \begin{bmatrix}
	        1\amp2\amp3\amp4\amp5\amp6\\ 19\amp20\amp21\amp22\amp23\amp24\\
	        13\amp14\amp15\amp16\amp17\amp18\\ 7\amp8\amp9\amp10\amp11\amp12\\
	        25\amp26\amp27\amp28\amp29\amp30 \end{bmatrix} 
            </me> 
            </p></li>

	        <li><p> 
            Multiply row 4 by 3 (<m>R_4\leftarrow 3R_4</m>) 
            <me>
                E=
	            \begin{bmatrix} 
                1\amp0\amp0\amp0\amp0\\ 
                0\amp1\amp0\amp0\amp0\\
	            0\amp0\amp1\amp0\amp0\\ 
                0\amp0\amp0\amp{\color{red}3}\amp0\\
	            0\amp0\amp0\amp0\amp1 
                \end{bmatrix} 
            </me> 
            <me>
                EA= 
                \begin{bmatrix}
	                1\amp2\amp3\amp4\amp5\amp6\\ 
                    7\amp8\amp9\amp10\amp11\amp12\\
	                13\amp14\amp15\amp16\amp17\amp18\\
	                57\amp60\amp63\amp66\amp69\amp72\\
	                25\amp26\amp27\amp28\amp29\amp30
                \end{bmatrix} 
            </me> 
            </p></li>

	        <li><p> 
            Subtract twice row 2 from row 4 
            (<m>R_4\leftarrow R_4-2R_2</m>) 
            <me>
                E= 
                \begin{bmatrix} 
                    1\amp0\amp0\amp0\amp0\\
	                0\amp1\amp0\amp0\amp0\\ 0\amp0\amp1\amp0\amp0\\
	                0\amp{\color{red}{-2}}\amp0\amp1\amp0\\ 0\amp0\amp0\amp0\amp1
	            \end{bmatrix} 
            </me>

	        <me>
            EA= 
            \begin{bmatrix} 
                1\amp2\amp3\amp4\amp5\amp6\\
	            7\amp8\amp9\amp10\amp11\amp12\\ 
                13\amp14\amp15\amp16\amp17\amp18\\
	            5\amp4\amp3\amp2\amp1\amp0\\ 
                25\amp26\amp27\amp28\amp29\amp30
	        \end{bmatrix}
            </me> 
            </p></li> 
            </ol>
	</p> 
    
    </statement> 
    </example>
    <p> 
        Notice that  
        <xref ref="ElementaryMatrixRowMultiplication" /> 
        can be applied to a sequence of elementary row
        operations.  If, for example, we have three elementary row operations
        whose corresponding elementary matrices are <m>E_1</m>, <m>E_2</m>
        and <m>E_3</m>, and we apply them in sequence to <m>A</m>, then the
        resulting matrix is the product <m>E_3(E_2(E_1(A)))</m>, or, more
        simply, <m>E_3E_2E_1A</m>.	Note that we are applying <m>E_1</m>
        first followed by <m>E_2</m> and finally <m>E_3</m>.  
    </p>

    <example> <title>Reduction to reduced row echelon form by matrix
    multiplication</title> 
    <statement> 
    <p>
    Let 
    <m>A= \begin{bmatrix}
        1\amp2\amp2 \\
        2\amp4\amp5 \\
        0\amp1\amp1 
        \end{bmatrix} 
        </m>.  
    We put this matrix into reduced row echelon form:
    </p>

    <table> 
    <title> Row reduction by matrix multiplication </title>
    <tabular>
	<row>
	    <cell><m>\textbf{Matrix}</m> </cell>
	    <cell><m>\textbf{Elementary Row}</m> </cell>
	    <cell><m>\textbf{Corresponding}</m></cell>
	</row> <row>
	    <cell><m></m> </cell> <cell><m>\textbf{Operations}</m></cell>
        <cell><m>\textbf{Matrix}</m></cell>
	</row>

	<row>
	    <cell>
            <m>A=
            \begin{bmatrix}
	            1\amp2\amp2\\
                2\amp4\amp5\\
                0\amp1\amp1
	        \end{bmatrix}
            </m>
        </cell> 
        <cell>
        <m>R_2\gets R_2-2R_1</m></cell>
        <cell>
        <m>E_1=
            \begin{bmatrix}
	            1\amp0\amp0\\
                -2\amp1\amp0\\
                0\amp0\amp1
	        \end{bmatrix}
        </m>
        </cell>
	</row>

	<row>
	    <cell>
        <m>E_1A=
            \begin{bmatrix}
	            1\amp2\amp2\\
                0\amp0\amp1\\
                0\amp1\amp1
	        \end{bmatrix}
        </m>
        </cell> 
        
        <cell> <m>R_2\leftrightarrow R_3</m></cell>
        <cell>
        <m>E_2=
            \begin{bmatrix}
	            1\amp0\amp0\\
                0\amp0\amp1\\
                0\amp1\amp0 
            \end{bmatrix}
        </m>
        </cell>
	</row>

	<row>
	    <cell>
        <m>E_2E_1A=
            \begin{bmatrix}
	            1\amp2\amp2\\
                0\amp1\amp1\\
                0\amp0\amp1
	        \end{bmatrix}
        </m>
        </cell> 
        
        <cell><m>R_1\gets R_1-2R_2</m></cell>
	    <cell>
        <m>
        E_3=
            \begin{bmatrix} 
                1\amp-2\amp0\\
                0\amp1\amp0\\
                0\amp0\amp1
            \end{bmatrix}
        </m>
        </cell>
	</row>

	<row>
	    <cell><m>E_3E_2E_1A=\begin{bmatrix}
	    1\amp0\amp0\\0\amp1\amp1\\0\amp0\amp1
	    \end{bmatrix}</m></cell> <cell><m>R_2\gets R_2-R_3</m></cell>
	    <cell><m>E_4=\begin{bmatrix} 1\amp0\amp0\\0\amp1\amp-1
	    \\0\amp0\amp1\end{bmatrix}</m></cell>
	</row>

	<row>
	    <cell>
        <m>
        E_4E_3E_2E_1A=
            \begin{bmatrix}
	            1\amp0\amp0\\
                0\amp1\amp0\\
                0\amp0\amp1 
            \end{bmatrix}
        </m>
        </cell>
	</row>
    </tabular> 
    </table>

    <p>There is a bonus from this computation: since
    <m>(E_4E_3E_2E_1)A=I,</m> <!-- (<m>\textbf{link}</m> we know that) -->
     we know that <m>A(E_4E_3E_2E_1)=I</m>
    and <m>A^{-1}=(E_4E_3E_2E_1).</m> Indeed, 
    <md> 
    <mrow>
        \begin{bmatrix}
            1\amp0\amp0\\
            0\amp1\amp-1 \\
            0\amp0\amp1
            \end{bmatrix}\amp
        \begin{bmatrix} 
            1\amp-2\amp0\\
            0\amp1\amp0 \\
            0\amp0\amp1
        \end{bmatrix}
        \begin{bmatrix} 
            1\amp0\amp0\\
            0\amp0\amp1\\
            0\amp1\amp0 
        \end{bmatrix}
        \begin{bmatrix} 
            1\amp0\amp0\\
            -2\amp1\amp0\\
            0\amp0\amp1
        \end{bmatrix}
    </mrow> 
    <mrow>
        \amp= 
        \begin{bmatrix}
            1\amp0\amp-2\\
            2\amp-1\amp1\\
            -2\amp1\amp0 
        \end{bmatrix}
    </mrow>
    <mrow>
        \amp = A^{-1}
    </mrow> 
     </md>


    We shall see shortly that every elementary matrix
    has an inverse which itself is elementary, and so
    <m>A=E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}</m>. Thus <m>A</m> is a product
    of elementary matrices.  
    </p> 
    </statement> 
    </example> 
    </subsection>

    <subsection>
    <title>Inverses of Elementary Row Operation Matrices</title>
	<p> 
    Each matrix associated with the three elementary row
	operations has an inverse.  While it is easy to define and
	verify the matrix in each case, it is useful to think of the
	effect of the inverse. If <m>E</m> is the matrix associated
	with an elementary row operation, then <m>EA</m> carries out
	that operation on <m>A.</m> Since <m>E^{-1}EA=A,</m> the effect
	of <m>E^{-1}</m> is to undo the operation and return <m>A</m>
	to its original form.

	    <ol>
        <li><p>
	        If <m>E</m> corresponds to interchanging two rows
	        (<m>R_i\leftrightarrow R_j</m>), to undo the operation we
	        just interchange them again. This means the <m>E^{-1}=E.</m>
	    </p></li> 
        <li><p> 
            If <m>E</m> corresponds to multiplying a row
	        by <m>\lambda</m> (<m>R_i\gets\lambda R_i</m>), then multiplying
	        the same row by <m>\tfrac1\lambda</m> returns it to its original
	        form. Hence <m>E^{-1}</m> is formed by replacing <m>\lambda</m> by
	        <m>\tfrac1\lambda</m> in <m>E</m>.  </p></li> <li><p> If <m>E</m>
	        corresponds to adding <m>\lambda</m> times row <m>j</m> to row
	        <m>i</m> (<m>R_i\gets R_i+\lambda R_j</m>), then subtracting
	        <m>\lambda</m> times row <m>j</m> from row <m>i</m> (<m>R_i\gets
	        R_i-\lambda R_j</m>) returns  <m>A</m> to its original form.
	        Hence <m>E^{-1}</m> is formed by replacing <m>\lambda</m> by
	        <m>-\lambda</m> in <m>E</m>. i
        </p></li> 
        </ol> 
    </p>

	<theorem xml:id="ElementaryMatrixInverse"> <title> Inverses of Elementary Matrices </title>
	<statement>
       <p>
       Every elementary matrix is invertible. In particular
       <ol>
       <li><m>E_1^{-1}=E_1</m>.</li> 
       <li>
           <m>E_2^{-1}</m> has <m>\lambda</m> in <m>E_2</m> 
           replaced by <m>\tfrac 1\lambda</m>. 
       </li>
       <li>
           <m>E_3^{-1}</m> has <m>\lambda</m> in <m>E_3</m> 
           replaced by <m>-\lambda</m>.
       </li>
       </ol>
       </p>
	</statement>

	<proof> 
        <p>We give the inverse in each case: 
            <ol>
	        <li><p>
            Interchange rows (<m>R_i\leftrightarrow R_j</m>) 
            <m>E=
	        \begin{bmatrix}
	            1 \\
	            \amp\ddots \\ 
                \amp\amp1\amp\amp\\
	            \amp\amp\amp0\amp\cdots\amp\cdots\amp\cdots\amp1\amp \\ 
                \amp\amp\amp\vdots\amp1\amp\amp\amp\vdots \\
	            \amp\amp\amp\vdots\amp\amp\ddots \amp\amp\vdots\\
	            \amp\amp\amp\vdots\amp\amp\amp1\amp\vdots \\
	            \amp\amp\amp1\amp\cdots\amp\cdots\amp\cdots\amp0 \\ 
                \amp\amp\amp\amp\amp\amp\amp\amp1 \\
	            \amp\amp\amp\amp\amp\amp\amp\amp\amp\ddots 
            \end{bmatrix}
	        </m> and <m>E^{-1}=E</m>. 
            </p></li>

	        <li><p>
            Multiply a row by a constant (<m>R_i\leftarrow \lambda R_i</m>) 
            <m>
            E=
            \begin{bmatrix} 
                1 \\ 
                \amp\ddots \\
	            \amp\amp1 \\ 
                \amp\amp\amp\lambda \\ 
                \amp\amp\amp\amp1 \\
	            \amp\amp\amp\amp\amp\ddots \\ 
                \amp\amp\amp\amp\amp\amp1
	        \end{bmatrix} 
            </m> 
        
            <m>E^{-1}=
            \begin{bmatrix} 1 \\ 
                \amp\ddots \\
	            \amp\amp1 \\ 
                \amp\amp\amp\tfrac1\lambda \\ 
                \amp\amp\amp\amp1 \\ 
                \amp\amp\amp\amp\amp\ddots \\ 
                \amp\amp\amp\amp\amp\amp1
	        \end{bmatrix}
            </m> 
            </p></li>

            <li><p>
            Add a multiple of one row to another 
            (<m>R_i\leftarrow R_i + \lambda R_j</m>) 
            <m>E=
            \begin{bmatrix} 
                1 \\ 
                \amp\ddots \\ 
                \amp\amp1 \\ 
                \amp\amp\amp1\amp\cdots\amp\lambda \\
	            \amp\amp\amp\amp1\amp\vdots \\ 
                \amp\amp\amp\amp\amp\ddots \\
	            \amp\amp\amp\amp\amp\amp1 
            \end{bmatrix}
	        </m> 
            and so  
            <m>
            E^{-1}=
            \begin{bmatrix}
	            1 \\ 
                \amp\ddots \\
                \amp\amp1 \\
	            \amp\amp\amp1\amp\cdots\amp-\lambda \\ 
                \amp\amp\amp\amp1\amp\vdots \\ 
                \amp\amp\amp\amp\amp\ddots \\ 
                \amp\amp\amp\amp\amp\amp1
	        \end{bmatrix}
	        </m> 
            </p></li>
	        </ol>
    
        </p> 
        </proof> 
        </theorem>

        <p>
	    An easy observation:  the inverse in each case of
        <xref ref="ElementaryMatrixInverse"/>, the inverse
        is also an elementary matrix,
	    and so we have proven the following theorem:
        </p>

        <theorem> <title> The Inverse of an Elementary Matrix is Elementary</title> 
        <statement> 
        <p>
        The inverse of an elementary matrix is elementary.
        </p> 
        </statement> 
        </theorem>

        <theorem> <title>An Invertible Matrix is a Product of Elementary Matrices</title>

        <statement> 
        <p>
        Let <m>A</m> be an invertible matrix. Then <m>A=F_1F_2\cdots F_t</m> 
        where each <m>F_i</m> is an elementary matrix.
        </p> 
        </statement>

        <proof> 
        <p>
        <!-- 
        By <xref ref=.../>, the reduced row echelon form of <m>A</m> is <m>I</m>.
        -->
        Put <m>A</m> into reduced row echelon form. Suppose this takes
        <m>t</m> elementary row operations. Then this implies
        that <m>E_tE_{t-1}\cdots E_2E_1A=I.</m> Let <m>F_i=E_i^{-1}</m>
        for <m>i=1,\dots,t.</m> Then <m>F_i</m> is an  elementary matrix,
        and <m>F_1F_2\cdots F_t=F_1F_2\cdots F_tI=F_1F_2\cdots F_t
        E_tE_{t-1}\cdots E_2E_1A=A.</m> 
        </p> 
        </proof>
    
        </theorem> 
        </subsection> 
        </section>
    
        <section><title>Equivalent forms of Invertibility and Singularity of Matrices</title> 
        <p>
    	We have already defined  singular and nonsingular matrices
    	in <xref ref="SingularMatrixDefinition" /> and
    	the inverse of a matrix in 
        <xref ref="InverseMatrixDefinition" />.  
        We now have a new goal: to show that a square
    	matrix is invertible if and only if it is nonsingular. In fact we
    	want to give several other conditions that are equivalent to being
    	invertible. By this we mean that if any one of the conditions
    	are true for a matrix <m>A</m>, then all them are true for that
    	matrix <m>A</m>, and we then call <m>A</m> <em>nonsingular</em> or
    	<em>invertible</em>.  Equivalently, if any one of the conditions
    	are false for a matrix <m>A</m>, then all of them are false
    	for that matrix <m>A</m>. In this case the matrix <m>A</m>
    	is called <em>singular</em>.  </p> <p> The method for showing
    	statements to be equivalent is a little different.  Suppose we
    	have (as will be our case) six statements numbered 
        <m>(1), (2), \ldots,(6)</m> that we wish to show equivalent.  
        We start by
    	showing that if <m>(1)</m> is true, then it follows that <m>(2)</m>
    	is true. We denote this by <m> (1)\Rightarrow(2) </m>.  We continue
    	with <m> (2)\Rightarrow(3), (3)\Rightarrow(4), (4)\Rightarrow(5),
    	(5)\Rightarrow(6), (6)\Rightarrow(1)</m>.  Once this has been done,
    	we may visualize our result: 
        </p> 
    
        <figure>
        <caption>A cycle of implications</caption>
        <image width="30%">
        <asymptote>
        unitsize(1.5cm);
        pair [] Pts = {dir(180), dir(240), dir(300), dir(0), dir(60), dir(120), dir(180)};
        for (int k: sequence(6)) {
            label("$("+string(6-k)+")$", Pts[k]);
            label(rotate(120+60*k)*"$\Rightarrow$",0.5*Pts[k]+0.5*Pts[k+1]);
            }
        </asymptote>
        </image>
        </figure>

        <!--
        <figure> 
        <caption />
        <image width="25%" source="images/matrixequivalences.png" /> 
        </figure> 
        -->

        <p> 
        and so
    	once one statement becomes true, all of them are true.	This also
    	means that once one statement is false then all of them are false
    	(think about the logic!).
        </p>
    
        <theorem xml:id="InvertibilityEquivalence"><title>Equivalent Forms of Invertibility</title>
        <statement> 
        <p>
        Suppose that <m>A</m> is an <m>n\times n</m>
        square matrix.  Then the following statements are equivalent: 
            <ol>
            <li><p> 
            <m>A</m> is invertible
            </p></li> 
            <li><p> 
            <m>A\vec x=0</m>
            if and only if <m>\vec x=0</m>
            </p></li> 
            <li><p> 
                The reduced row echelon form of <m>A</m> is <m>I_n</m>
            </p></li> 
            <li><p> 
                <m>A</m> is a product of elementary matrices
            </p></li> 
            <li><p> 
                <m>A\vec x=\vec b</m> is consistent for any <m>\vec b</m>
            </p></li>
            <li><p> 
                <m>A\vec x=\vec b</m> has exactly one solution for
                any <m>\vec b</m>
            </p></li> 
            </ol> 
        </p> 
        </statement> 
        <proof> 
        <p>
        The logic of the proof is to show that</p>
    
            <p>(1) true implies (2) true (which we write as (1) <m>\Rightarrow </m>(2)),</p>
            <p>(2) true implies (3) true (which we write as (2) <m> \Rightarrow</m> (3)),</p>
            <p>(3) true implies (4) true (which we write as (3) <m>\Rightarrow</m> (4)),</p>
            <p>(4) true implies (5) true (which we write as (4) <m>\Rightarrow</m> (5)),</p>
            <p>(5) true implies (6) true (which we write as (5) <m>\Rightarrow</m> (6)), and</p>
            <p>(6) true implies (1) true (which we write as (6) <m>\Rightarrow</m> (1))</p>
    
        <p> 
        We now proceed to prove the implications one at a time.  
        </p>
    
    	<p> 
        (1) <m>\Rightarrow</m> (2): Assuming (1) means that
    	<m>A</m> is invertible.  If <m>x=\vec0</m>, then
    	<m>A\vec x=A\vec0=\vec0</m>.  Now suppose that
    	<m>A\vec x=\vec0.</m>  Then, since <m>A^{-1}</m> exists,
    	we may evaluate <m>A^{-1}A\vec x</m> in two ways: <me>
    	A^{-1}(A\vec x) = A^{-1}\vec0=\vec0\\ (A^{-1}A)\vec x
        =I\vec x=\vec x </me> Equating the two evaluations,
    	we get <m>\vec x=\vec0.</m> 
        </p>
    
    	<p> 
        (2) <m>\Rightarrow</m> (3): Consider the augmented matrix
    	for the system of linear equations <m>A\vec x=\vec0.</m>
    	If the reduced row echelon from of <m>A</m> is not <m>I_n,</m>
    	then the form for augmented matrix has a free variable; this may
    	be assigned a nonzero value, resulting in a nonzero solution to
    	<m>A\vec x=\vec0.</m> 
        </p>
    
    	<p> 
        (3) <m>\Rightarrow</m> (4): Suppose <m>A</m> is reduced
    	to <m>I_n</m> by a series of elementary row operations whose
    	corresponding elementary matrices are <m>E_1,E_2,\dots,E_k</m>.
    	Then <m>E_kE_{k-1}\cdots E_2E_1A=I_n</m>, and 
        <me>
    	    A=E_1^{-1}E_2^{-1}\cdots E_{k-1}^{-1}E_k^{-1}=F_1F_2\cdots
    	    F_{k-1}F_k.
    	</me> 
        </p>
    
    	<p>
        (4) <m>\Rightarrow</m> (5): Suppose we want to solve
    	<m>A\vec x=\vec b</m> where <m>\vec b</m> is any vector
    	and <m>A=F_1F_2\cdots F_{k-1}F_k.</m> Then we have 
        <me>
    	   F_1F_2\cdots F_{k-1}F_k \vec x=\vec b,
    	</me> 
        and 
        <me> 
        \vec x=F_k^{-1}F_{k-1}^{-1}\cdots
    	F_2^{-1}F_1^{-1} \vec b 
        </me> 
        which then satisfies
    	<me> 
        \begin{split} A\vec x \amp =(F_1F_2\cdots
    	F_{k-1}F_k)(F_k^{-1}F_{k-1}^{-1}\cdots F_2^{-1}F_1^{-1}\vec
    	b)\\ \amp=(F_1F_2\cdots F_{k-1}F_k)(F_k^{-1}F_{k-1}^{-1}\cdots
    	F_2^{-1}F_1^{-1})\vec b \\ \amp=\vec b \end{split} 
        </me>
    	and so gives a solution to <m>A\vec x=\vec b.</m> Hence
    	<m>A\vec x=\vec b</m> is consistent.  
        </p>
    
    	<p> (5) <m>\Rightarrow</m> (6): We take the following values for
    	<m>\vec b_1,\vec b_2,\dots,\vec b_n</m>: 
        <me> 
        \vec b_1=
            \begin{bmatrix} 
            1 \\ 0 \\ 0 \\ \vdots \\ 0
            \end{bmatrix}, 
        \quad 
        \vec b_2=
            \begin{bmatrix} 
            0 \\ 1 \\ 0 \\ \vdots \\ 0
            \end{bmatrix}, 
        \quad 
        \vec b_3=
            \begin{bmatrix} 0 \\ 0 \\ 1 \\ \vdots \\ 0
            \end{bmatrix}, 
        \ldots, 
        \quad 
        \vec b_n=
            \begin{bmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 1
            \end{bmatrix} 
        </me>.
    	Then <m>A\vec x=\vec b_1</m> has a solution <m>\vec c_1</m>, 
        <m>A\vec x=\vec b_2</m> has a solution <m>\vec c_2</m>, 
        <m>A\vec x=\vec b_3</m> has a solution <m>\vec c_3, \dots,</m> 
        <m>A\vec x=\vec b_n</m> has a solution <m>\vec c_n</m>.  
        </p> 
        
        <p>
        Now define the matrix <m>C</m> as the one with 
        <m>\vec c_1,\vec c_2,\dots,\vec c_n</m> as columns, that is,
    	<me> 
        C=
        \begin{bmatrix}
        \vec c_1 \amp\vec c_2 \amp\vec c_3 \amp\cdots \amp\vec c_n
        \end{bmatrix}.  
        </me>
    
    	Then <m>AC=I</m>, and so <m>C=A^{-1}</m> and <m>CA=I.</m> Now
    	if <m>A\vec x=\vec b</m> is consistent, then <m>\vec
    	x=I\vec x=CA\vec x=C\vec b</m> and so <m>\vec
    	x=C\vec b</m> is the one and only solution to <m>A\vec
    	x=\vec b.</m> 
        </p>
    
    	<p>
        (6) <m>\Rightarrow</m> (1): Use the vectors <m>\vec b_1,
    	\vec b_2,\dots,\vec b_n</m> given just above. In each
    	case there is exactly one vector <m>\vec c_k</m> which
    	is a solution to the equation <m>A\vec x=\vec b_k</m>.
    	Let <m>C</m> be the matrix with <m>\vec c_1,\dots,\vec
    	c_n</m> as columns. This matrix satisfies <m>AC=I</m> which
    	makes <m>C=A^{-1}</m> and so <m>A</m> is invertible.
        </p> 
        </proof>
    	</theorem>
        </section>
    
</chapter>
    
<chapter> <title>Additional Topics</title>
    
    <section> <title>Named properties and special subsets of real numbers</title>
        <subsection> <title>Special subsets of the real numbers</title>
        <p>
        For our purposes, the 
        <term>real  numbers</term> consists of all possible decimals. These
        include whole numbers, both positive and negative, fractions,
        simple decimals like <m>1.25</m>, repeating decimals like 
        <m>1.333\ldots=\frac43</m> and irrational numbers like <m>\sqrt2</m>
        and <m>\pi</m>.
        </p>
        
        <p>
        We have some special notation that is used with the real numbers:
            <ul>
            <li><p><m>\N</m>, the <em>natural numbers</em>, is the set of all 
                nonnegative integers: <m>\{0, 1, 2, 3,\ldots\}</m>.</p></li>
            <li><p><m>\Z</m>, the <em>integers</em>, consists of 
                <m>\{0, \pm1, \pm2, \pm3, \ldots\}</m>.</p></li>
            <li><p><m>\Q</m>, the <em>rationals</em>, is the set of all fractions: 
                <m>\frac mn</m> where <m>m</m> and <m>n</m> are integers with <m>n\not=0</m>.</p></li>
            <li><p><m>\R</m> is the set of <em>all</em> real numbers, that is, ordinary decimals.</p></li>
            </ul>
        </p>
        </subsection>
        
        <subsection xml:id="RealNumberProperties"> <title>Named properties of real numbers</title>
        
        <table>
        <title> Properties of real numbers </title>
        <tabular>
        <row> 
           <cell></cell>
           <cell> <em>Additive properties</em>
           </cell> <cell> <em>Multiplicative properties</em>  </cell> 
        </row>
        <row> 
           <cell>Closure:</cell>
           <cell> If <m>x</m> and <m>y</m> are real then so is <m>x+y</m> </cell> 
           <cell> If <m>x</m> and <m>y</m> are real then so is <m>xy</m>  </cell> 
        </row>
        <row> 
           <cell>Associativity:</cell>
           <cell> <m>x+(y+z)=(x+y)+z</m></cell>   
           <cell> <m>(xy)z=x(yz)</m>  </cell> 
        </row>
        <row> 
           <cell>Identity:</cell>
           <cell> <m>x+0=x</m> </cell>            
           <cell> <m>1x=x</m>  </cell> 
        </row>
        <row> 
           <cell>Inverse:</cell>
           <cell> For any <m>x</m> there is a <m>-x</m> so that <m>x+(-x)=0</m> </cell>           
           <cell> For any <m>x\not=0</m> there is a number <m>x^{-1}</m> so that <m>xx^{-1}=1</m></cell> 
        </row>
        <row> 
           <cell>Commutativity:</cell>
           <cell> <m>x+y=y+x</m> </cell>          
           <cell> <m>xy=yx</m>  </cell> 
        </row>
        <row> <cell><m>\strut</m></cell> <cell></cell> <cell></cell> </row>
        <row> 
           <cell>Distributive:</cell>
           <cell colspan="2" halign="center"> <m>x(y+z)=xy+xz</m> and <m>(x+y)z=xz+yz</m> </cell>          
        </row>
        </tabular>
        </table>
        
        </subsection>
        
    </section>
    
    <section> <title>Summation notation</title>
        <subsection> <title>Basic definitions</title>
        <p>
            Summation notation is a compact form for writing sums. It is written with
            three pieces.
            \[
            \sum_{\fbox{<m>k=1</m>}}^\fbox{<m>5</m>} \fbox{<m>2k</m>}
            \]
            The part below the greek letter sigma (<m>\Sigma</m>) tells us to start with <m>k=1</m>.
            We increment <m>k</m> by <m>1</m> until we get to the value above the sigma. In this
            case we let <m>k</m> take on the values <m>1, 2, 3, 4, 5</m>. We next substitute those
            values of <m>k</m> into the expression to the right of the sigma successively and 
            take the sum all of the resulting terms. Here is a table of the resulting values:
            <md>
            <mrow>k \amp\amp 2k</mrow>
            <mrow>1 \amp\amp 2</mrow>
            <mrow>2 \amp\amp 4</mrow>
            <mrow>3 \amp\amp 6</mrow>
            <mrow>4 \amp\amp 8</mrow>
            <mrow>5 \amp\amp 10</mrow>
            </md>
            which evaluates to
            <me> 2+4+6+8+10=30 </me>
            and so we write
            <me> \sum_{k=1}^5 2k=30</me>.
            The variable <m>k</m> is called <em>the index of summation</em>. The number above
            the sigma is called <em>the limit of summation</em>.
            The example shows us how to write a sum of even numbers. We can write the
            sum of odd numbers, too.
            <me> \sum_{k=1}^8 (2k-1)=1+3+5+7+9+11+13+15=64 </me>.
            It is not necessary to use <m>k</m> as a variable or to start the summation
            at <m>k=1</m>. It is easy to verify that
            <me> \sum_{k=1}^8(2k-1)=\sum_{l=0}^7(2l+1)=64 </me>.
            
            The number above the sigma can also be written as a variable:
            <md>
            <mrow>\sum_{k=1}^n a_k=a_1+a_2+\cdots+a_{n-1}+a_n</mrow>
            <mrow>\sum_{k=0}^n a_kb_k=a_0b_0+a_1b_1+a_2b_2+\cdots+a_{n-1}b_{n-1}+a_nb_n</mrow>
            <mrow>\sum_{k=1}^n (2k-1) = 1+3+5+7+\cdots+(2n-1)</mrow>
            </md>
            
            Next we take the last expression and vary <m>n</m>,
            the limit of summation. Here is a table that includes
            some small vaules of <m>n</m>:
        </p>
        <!-- Perhaps better as a side-by-side or aligned eqaution-->
        <table>
        <title/>
           <tabular>
           <row>
              <cell><m>\sum_{k=1}^1(2k-1)=1</m></cell> <cell><m>n=1</m></cell>
           </row>
           <row>
              <cell><m>\sum_{k=1}^2(2k-1)=4</m></cell> <cell><m>n=2</m></cell>
           </row>
           <row>
              <cell><m>\sum_{k=1}^3(2k-1)=9</m></cell> <cell><m>n=3</m></cell>
           </row>
           <row>
              <cell><m>\sum_{k=1}^4(2k-1)=16</m></cell> <cell><m>n=4</m></cell>
           </row>
           <row>
              <cell><m>\vdots</m></cell>
           </row>
           <row>
              <cell><m>\sum_{k=1}^n(2k-1)=1+3+5+\cdots+2n-1=?</m></cell>
           </row>
           </tabular>
        </table>
        <exercise>
            <p>
            What simple expression involving <m>n</m> can replace the question mark?
            </p>
        </exercise>
        <p>
            The summands can be symbolic, too. If <m>f(x)</m> is any function,
            we may write
            <md>
            <mrow>\sum_{i=1}^r f(i)=f(1)+f(2)+\cdots+f(r-1)+f(r)</mrow>
            </md>
        </p>
        </subsection>
        
        <subsection> <title>Alternating sums</title>
        <p>
            A sum is <em>alternating</em> if the signs of consecutive
            terms are alternate between positive and negative.
            
            An example: <m>1-2+3-4+5</m>. 
            These sums appear frequently enough to have this special name.
            They are easy to write using summation notation because of
            a simple fact:
            <me>
            (-1)^k=
            \begin{cases}
            1 \amp  \text{if \(k\) is even}\\
            -1 \amp  \text{if \(k\) is odd}
            \end{cases}
            </me>
            We can then write
            <me>\sum_{k=1}^6 (-1)^k k^2=-1+4-9+16-25+36=21 </me>.
            We can interchange the order of the signs by adding
            (or subtracting) one to the exponent of <m>-1</m>:
            <me>\sum_{k=1}^6 (-1)^{k+1} k^2=1-4+9-16+25-36=-21</me>.
        </p>
        </subsection>
        
        <subsection> <title>Manipulation with summation notation identities</title>
        <p>
            Summation notation is more that mere shorthand. There are rules
            of manipulation that are quite useful.
        </p>
        
        <p>
            For any constant <m>c</m>,
            <me> 
                \sum_{k=1}^n c=\underbrace{c+c+\cdots+c+c}_{n \textrm{ summands}}=nc 
            </me>
            and
            <me> 
                \sum_{k=1}^n ca_k
                =ca_1+\cdots+ca_n
                =c(a_1+\cdots+a_n)
                =c\sum_{k=1}^n a_k
            </me>
            The process of the last equation is sometimes referred to 
            as <em>pulling a constant across the summation sign</em>.
        </p>
        
        <p>
            In addition, 
            <me>
                \sum_{k=1}^n(a_k+b_k)= \sum_{k=1}^na_k+\sum_{k=1}^nb_k,\\
                \sum_{k=1}^n(a_k-b_k)= \sum_{k=1}^na_k-\sum_{k=1}^nb_k
            </me>
            and
            <me>
                \sum_{k=0}^na_k= \sum_{k=t}^{n+t}a_{k-t}
            </me>
            The last identity is sometimes called 
            <em>shifting the index of summation.</em>
        </p>
        <example>
            <p>
            Suppose we want to evaluate the sum of the
            first <m>n</m> positive integers. In other words, we want to find
            <me>
                s_n=1+2+\cdots+n=\sum_{k=1}^nk.
            </me>
            If we are convinced that 
            <m>\sum_{k=1}^n(2k-1)=1+3+5+\cdots+2n-1=n^2</m>,
            then
            <md>
            <mrow>n^2\amp =\sum_{k=1}^n(2k-1)</mrow>
            <mrow>\amp =\sum_{k=1}^n(2k)-\sum_{k=1}^n1</mrow>
            <mrow>\amp =2\sum_{k=1}^nk-n</mrow>
            <mrow>\amp =2s_n-n</mrow>
            </md>
            which yields
            <me>
                s_n=\frac12n(n+1).
            </me>
            </p>
        </example>
        
        <exercise>
            <statement>
            <p>
            Evaluate the following sums
        
            <ul>
                <li><p><m>\sum_{j=1}^5 2j</m></p></li>
                <li><p><m>\sum_{k=1}^5 k(k-1)</m></p></li>
                <li><p><m>\sum_{i=0}^4 2^i</m></p></li>
                <li><p><m>\sum_{\ell=1}^3\ell^2+\frac1\ell</m></p></li>
                <li><p><m>\sum_{j=1}^5 ij</m></p></li>
            </ul>
        
            </p>
            </statement>
        
            <solution>
            <p>
            <ul>
                <li><p><m>2+4+6+8+10=30</m></p></li>
                <li><p><m>2+6+12+20+30=70</m></p></li>
                <li><p><m>1+2+4+8+16=31</m></p></li>
                <li><p><m>1+1+4+\frac12+9+\frac13=15\frac56</m></p></li>
                <li><p><m>i+2i+3i+4i+5i=15i</m></p></li>
            </ul>
            </p>
            </solution>
        
        </exercise>
        
        <exercise>
            <statement>
            <p>Evaluate <m>\sum_{k=0}^n 2^k</m> for <m>n=1,2,3,4</m>.
            Find a simple expression that works for any <m>n</m>.
            Justify your answer.</p>
            </statement>
            <solution>
            <p>
            <me>
            \begin{array}{cc}
            n \amp \sum_{k=0}^n 2^k\\
            \hline
            1 \amp 3\\
            2 \amp 7\\
            3\amp 15\\
            4\amp 31
            \end{array}
            </me>
            We note that each answer is one less than a power of <m>2</m>. This leads to
            the equation
            \[
            \sum_{k=0}^n 2^k=2^{n+1}-1.
            \]
            To justify the equation we note that
            \[
            2(\sum_{k=0}^n 2^k)=\sum_{k=0}^n 2^{k+1}=\sum_{k=1}^{n+1} 2^k
            =\bigl(\sum_{k=0}^{n+1} 2^k\bigr) -1
            \]
            so we get the sum for a given value on <m>n</m> by doubling the value for <m>n-1</m>
            and adding <m>1</m>.
            </p>
            </solution>
        </exercise>
        </subsection>
        
        <subsection> <title>Double summations</title>
        <p>
            Sometimes it is useful to have a formula with two summations, each with
            its own index of summation. It is interpreted in the following way:
            \[
            \sum_{i=1}^m \sum_{j=1}^n f(i,j)=
            \sum_{i=1}^m \bigl(\sum_{j=1}^n f(i,j)\bigr)
            \]
            and so, for example,
            \[
            \sum_{i=1}^3 \sum_{j=1}^2 i^j=\sum_{i=1}^3 (i^1+i^2)=(1+1)+(2+4)+(3+9)=20
            \]
        </p>
        
        <p>
            Double summations are often used with matrices. Suppose we have
            a matrix
            <me>
            A=
            \begin{bmatrix}
            a_{1,1}\amp a_{1,2}\amp \cdots \amp a_{1,n}\\
            a_{2,1}\amp a_{2,2}\amp \cdots \amp a_{2,n}\\
            \vdots\amp \vdots\amp \vdots\amp \vdots\\
            a_{m,1}\amp a_{m,2}\amp \cdots \amp a_{m,n}
            \end{bmatrix}
            </me>
            
            Then
            \[
            \sum_{j=1}^n a_{i,j} = a_{i,1}+a_{i,2}+\cdots+a_{i,n}
            \]
            which is the sum of the entries in the <m>i</m>-th row of the matrix.
            
            Similarly 
            \[
            \sum_{i=1}^m a_{i,j} = a_{1,j}+a_{2,j}+\cdots+a_{m,j}
            \]
            which is the sum of the entries in the <m>j</m>-th column of the matrix.
            We can now see that
            <me>
                \sum_{i=1}^m\sum_{j=1}^n a_{i,j}
                =\sum_{j=1}^n\sum_{i=1}^m a_{i,j}
            </me>
            since each sides of the equation is equal to
            the sum of all of the entries in the matrix.
            This is called 
            <em>interchanging the order of summation.</em>
        </p>
            
        <p>
            Another way of looking at these double summations 
            <m>\sum_{i=1}^m \sum_{j=1}^n f(i,j)</m> is that it is
            the sum
            of <m>f(i,j)</m> as <m>i</m> takes on values from <m>1</m> to <m>m</m>
            and <m>j</m> takes on values from <m>1</m> to <m>n</m>. Clearly
            there are <m>mn</m> summands altogether.
        </p>
        
        </subsection>
    </section>
    
    <section> <title>Trigonometry review</title>
    <introduction>
    <p>
        There are certain trigonometric facts that should be in
        your mathematical toolkit. We review several here.
        <ul>
        <li><p>The law of cosines (<xref ref="LawOfCosines" />)</p></li>
        <li><p>The graphs of <xref ref="GraphSin"><m>\sin(x)</m></xref>
            and 
            <xref ref="GraphCos"><m>\cos(x)</m></xref>
            </p></li>
        <li><p>The parallelogram law (<xref ref="ParallelogramLaw" />)
        </p></li>
        <li><p>The addition laws for <m>\sin(x)</m> and <m>\cos(x)</m>
            (<xref ref="SinCosSums" />)</p></li>
        <li><p>The law of sines (<xref ref="LawOfSines" />)</p></li>
        </ul>
    </p>
    </introduction>
    
        <subsection> <title>Basic Definitions</title>
        <p>
        Trigonometry was originally used for measuring triangles. Given a right
        triangle with hypoteneuse <m>c</m>, legs <m>a</m> and <m>b</m> 
        and angle <m>\theta</m> (as in the left illustration in <xref ref="TrigFunctions"/>), 
        the six trigonometric functions are defined by
        <me>
        \begin{array}{lll}
        \sin(\theta)=\frac bc \amp
        \cos(\theta)=\frac ac \amp
        \tan(\theta)=\frac {\sin(\theta)}{\cos(\theta)}=\frac ba \\
        \cot(\theta)=\frac {\cos(\theta)}{\sin(\theta)}=\frac ab \amp
        \csc(\theta)=\frac 1{\sin(\theta)}=\frac cb \amp
        \sec(\theta)=\frac 1{\cos(\theta)}=\frac ca
        \end{array}
        </me>
        
        The first three, the sine, the cosine and the tangent are by far the
        most important, even though the cotangent, cosecant and secant appear
        in many applications.
        </p>
        
        <figure xml:id="TrigFunctions">
        <caption>Triginometric functions of angles</caption>
        <image width="90%"> 
        <asymptote>
            unitsize(30);
            pair C=(4,0), B=(0,0), A=(4,4), D=(9,2);
            
            dot(A); dot(B); dot(C);
            
            draw(A-- B--C--cycle);
            
            label("\(a\)", (B+C)/2,S);
            label("\(b\)", (A+C)/2,W);
            label("\(c\)", (A+B)/2,NW);
            label("\(\theta\)",B,4*ENE);
            
            pair t=(10,2);
            draw(t+(-2.5,0)--t+(2.5,0)); label("\(x\)",t+(2.5,0),E);
            draw(t+(0,-2.5)--t+(0,2.5)); label("\(y\)",t+(0,2.5),N);
            
            real theta=2;
            pair z=t+2*(cos(theta),sin(theta));
            draw(circle(t+(0,0),2));
            draw(t+(0,0)--z, linewidth(1pt)); dot(z); 
            label("\((\cos(\theta),\sin(\theta))\)",z,NW);
            
            draw(t+(1/sqrt(2),0){up}..{WSW}(z+t)/2, Arrow); 
            label("\(\theta\)", t+(.6,.8));
            
            label("Trigonometric functions with a right triangle",((C.x/2,-1)));
            label("Trigonometric functions with any angle",((10,-1)));
        </asymptote> 
        </image>
        </figure>
        <p>
        In any such right triangle, the angle <m>\theta</m> is necessarily acute. We extend
        the definition of the trigonometric functions to other angles by using the unit circle
        (centre at <m>\vec 0=(0,0)</m> and radius <m>r=1</m>). For any angle <m>\theta</m>,
        draw the radius of the unit circle that makes an angle <m>\theta</m> with the
        positive <m>x</m>-axis. The coordinates of the point where this radius meets the
        circle are <m>(\cos(\theta),\sin(\theta))</m>. Note that when <m>\theta</m> is acute,
        the two definitions coincide.
        </p>
        
        <p>
        Since the point <m>(\cos(\theta),\sin(\theta))</m> is on the unit circle,
        its distance to the origin <m>(0,0)</m> is <m>1</m>. This means
        \[
        \cos^2(\theta) + \sin^2(\theta) =1
        \textrm{ for any angle }
        \theta
        \]
        </p>
        </subsection>
        
        <subsection> <title>The law of cosines</title>
        <p>
        In the right triangle above, the angle opposite the side labeled <m>c</m> is an
        right angle, and the Pythagorean theorem says <m>c^2=a^2+b^2</m>. The law of 
        cosines extends this theorem to triangles with any angle opposite the
        side labeled <m>c</m>. 
        </p>
        
        <theorem xml:id="LawOfCosines"> <title>The law of cosines</title>
        <statement>
        <p>
            If a triangle (as in the figure below) has sides of lengths <m>a</m>, <m>b</m> and <m>c</m>, 
            and the angle at the vertex opposite <m>c</m> is <m>\theta</m>, then
            \[
            c^2=a^2+b^2-2ab\,\cos\theta
            \]
        </p>
        
        <figure>
        <caption />
        <image width="50%"> <asymptote>
        unitsize(48);
        pair C=(1,1), B=(2,3), A=(5,2);
        dot(A); dot(B); dot(C);
        draw(A--B--C--cycle);
        label("\(a\)", (B+C)/2,W);
        label("\(b\)", (A+C)/2,S);
        label("\(c\)", (A+B)/2,N);
        draw(arc(C,.4*length(C),degrees(A-C),degrees(B-C))); label("\(\theta\)", C+(A+B)/10.0);
        </asymptote>
        </image>
        </figure>
        </statement>
        
        <proof>
           <p> 
           Note that since <m>a</m> and <m>b</m> are interchangable in the picture,
           there is no loss of generality to assume <m>a \le b</m>.
           We prove the law of cosines by dropping a perpendicular to the side <m>b</m>: 
           </p>
           
           <figure>
           <caption> <m>\theta</m> acute </caption>
           <image width="50%"> <asymptote>
           unitsize(48);
           pair C=(0,0), B=(1,2), A=(4,1);
           
           dot(A); dot(B); dot(C);
           draw(A--B--C--cycle);
           
           label("\(a\)", (B+C)/2,W);
           label("\(b\)", (A+C)/2,S);
           label("\(c\)", (A+B)/2,N);
           
           draw(arc(C,length(A+B)/15,degrees(A),degrees(B))); 
           label("\(\theta\)",(A+B)/11.0);
           
           
           pair P=(A.x*B.x+A.y*B.y)/(A.x*A.x+A.y*A.y)*A;
           dot(P,red); draw(B--P, red); 
           label("\(h\)",(B+P)/2,E,red);
           label("\(b_1\)",P/2,S,red);
           label("\(b_2\)",(A+P)/2,S,red);
           </asymptote>
           </image>
           </figure>
           
           <p>
           The perpendicular has length <m>h</m> and splits side <m>b</m> into two pieces of
           lengths <m>b_1</m> and <m>b_2</m>.
           </p>
           
           <p> 
           Now we have the following identities:
                <ul>
                <li><p><m>b=b_1+b_2</m></p></li>
                <li><p><m>a^2=b_1^2 + h^2</m></p></li>
                <li><p><m>c^2 = b_2^2 + h^2</m></p></li>
                <li><p><m>b_1=a\cos(\theta)</m></p></li>
                </ul>
           which gives the equations
           <me>
           \begin{array}{rl}
           c^2
           \amp=b_2^2+h^2\\
           \amp=(b-b_1)^2+(a^2-b_1^2)\\
           \amp=b^2 -2bb_1 + b_1^2 +a^2-b_1^2\\
           \amp=a^2+b^2-2bb_1\\
           \amp=a^2+b^2-2ab\cos(\theta)
           \end{array}
           </me>
           We have cheated a little by assuming that the perpendicular divides <m>b</m> into two
           pieces. In fact if <m>\theta\gt\frac\pi2</m> then the picture looks like
          </p>
        
           <figure>
           <caption> <m>\theta</m> obtuse </caption>
           <image width="80%"> <asymptote>
           unitsize(48);
           pair C=(0,0), B=(-2,2), A=(4,1);
           
           dot(A); dot(B); dot(C);
           draw(A--B--C--cycle);
           
           label("\(a\)", (B+C)/2,W);
           label("\(b\)", (A+C)/2,S);
           label("\(c\)", (A+B)/2,N);
           
           draw(arc(C,length(A+B)/15,degrees(A),degrees(B))); 
           label("\(\theta\)",(A+B)/11.0);
           
           pair P=(A.x*B.x+A.y*B.y)/(A.x*A.x+A.y*A.y)*A;
           dot(P,red); draw(B--P, red); draw(P--C,red);
           label("\(h\)",(B+P)/2,E,red);
           label("\(b_1\)",P/2,S,red);
           dot(A); dot(B); dot(C);
           </asymptote>
           </image>
           </figure>
           
           <p>
           Now we have
           <ul>
           <li><p> <m>a^2=b_1^2+h^2</m></p></li>
           <li><p> <m>c^2=(b+b_1)^2+h^2</m></p></li>
           <li><p> <m>b_1=-a\cos(\theta)</m>
               (remember that <m>\cos(\theta)\lt0</m> 
               when <m>\frac\pi 2\lt\theta\lt\pi</m>)</p></li>
           </ul>
           which gives
           <me>
           \begin{array}{rl}
           c^2
           \amp= (b+b_1)^2+h^2\\
           \amp= (b+b_1)^2+(a^2-b_1^2)\\
           \amp= b^2+2bb_1+a^2\\
           \amp= a^2+b^2-2ab\cos(\theta)
           \end{array}
           </me>
        
           In principle there is another possibility:
            </p>
        
           <figure>
           <caption />
           <image width="50%"> <asymptote>
           unitsize(48);
           pair C=(0,0), B=(2,3), A=(2,1);
           
           dot(A); dot(B); dot(C);
           draw(A--B--C--cycle);
           
           label("\(a\)", (B+C)/2,W);
           label("\(b\)", (A+C)/2,S);
           label("\(c\)", (A+B)/2,E);
           
           draw(arc(C,length(A+B)/15,degrees(A),degrees(B))); 
           label("\(\theta\)",(A+B)/11.0);
           
           pair P=(A.x*B.x+A.y*B.y)/(A.x*A.x+A.y*A.y)*A;
           dot(P,red); draw(A--P, red); draw(P--B,red);
           label("\(h\)",(B+P)/2,E,red);
           label("\(b_1\)",(A+P)/2,S,red);
           dot(A); dot(B); dot(C);
           </asymptote></image>
           </figure>
           
           <p>
           In this case we <m>a \gt b</m>, contrary to our assumption that <m>a\leq b</m>. Nonetheless
           we could use the argument:
           <ul>
           <li> <m>a^2=(b+b_1)^2 + h^2</m></li>
           <li> <m>c^2 = b_1^2 + h^2</m></li>
           <li> <m>b+b_1=a\cos(\theta)</m></li>
           </ul>
           and can deduce <m>c^2=a^2+b^2-2ab\cos(\theta)</m> in a manner similar that used in
           the previous cases.
           </p>
        </proof>
        </theorem>
        </subsection>
        
        <subsection> <title>The graphs of <m>\sin(x)</m> and <m>\cos(x)</m></title>
        <p>
        The graphs of the trigonometric functions are revealing.
        First, here is the graph of the function <m>\sin(x)</m>:
        </p>
        
        
        <figure xml:id="GraphSin">
        <caption />
        <image width="75%">
        <asymptote>
        import graph;
        size (0 ,1.4 inch );
        real pi=2*acos(0);
        real xmin =0, xmax =2*pi;
        real ymax =1, ymin = -1;
        draw(graph(sin,0,2*pi));
        xaxis(xmin,xmax,Ticks);
        yaxis(ymin,ymax,LeftTicks);
        label("\(x\)",(xmax,0),E);
        label("\(y\)",(0,ymax),N);
        dot((2*pi,0)); label("\(2\pi\)",(2*pi,0),N);
        dot((pi,0)); label("\(\pi\)",(pi,0),N);
        dot((pi/2,0)); label("\(\frac\pi2\)",(pi/2,0),N);
        dot((3*pi/2,0)); label("\(\frac32 \pi\)",(3*pi/2,0),N);
        label("\(y=\sin(x)\)",(2.6,1),E);
        </asymptote>
        </image>
        </figure>
        <p>
        Notice that <m>\sin(x)</m> is positive in this range precisely when <m>0\lt x\lt\pi</m>.
        </p>
        
        <p>
        Here is the graph of <m>\cos(x)</m>:
        </p>
        
        <figure xml:id="GraphCos">
        <caption />
        <image width="75%">
        <asymptote>
        import graph;
        size (0 ,1.4 inch );
        real pi=2*acos(0);
        real xmin =0, xmax =2*pi;
        real ymax =1, ymin = -1;
        draw(graph(cos,0,2*pi));
        xaxis(xmin,xmax,Ticks);
        yaxis(ymin,ymax,LeftTicks);
        label("\(x\)",(xmax,0),E);
        label("\(y\)",(0,ymax),N);
        dot((2*pi,0)); label("\(2\pi\)",(2*pi,0),N);
        dot((pi,0)); label("\(\pi\)",(pi,0),N);
        dot((pi/2,0)); label("\(\frac\pi2\)",(pi/2,0),N);
        dot((3*pi/2,0)); label("\(\frac32 \pi\)",(3*pi/2,0),N);
        label("\(y=\cos(x)\)",(0.6,1),E);
        </asymptote>
        </image>
        </figure>
        <p>
        Notice that the function <m>\cos(x)</m> is decreasing for <m>0\le x\le\pi</m>.
        By decreasing we mean that if <m>x_1 \lt x_2</m>, then <m>\cos(x_1) \gt \cos(x_2)</m>
        (or, more intuitively, the function goes downhill as we go from left to right). 
        There is an important implication from this: if <m>y=\cos(x)</m> for some
        <m>x</m> with <m>0\le x\le\pi</m>, then for any for any other <m>x'</m> in the same range,
        we have 
        <m>\cos(x') \gt y</m> (when <m>x'\lt x</m>) or
        <m>\cos(x') \lt y</m> (when <m>x'\gt x</m>). In particular this means that for any
        given <m>y_0</m>, there is at most one <m>x_0</m> with <m>0\le x_0\le\pi</m> so that <m>y_0=\cos(x_0)</m>. 
        In fact, the intermediate value theorem from the Calculus implies that
        for any <m>y_0</m> satisfying <m>-1\leq y_0 \leq 1</m> there is some <m>x_0</m> satisfying
        <m>0\le x_0\le\pi</m> so that <m>y_0=\cos(x_0)</m>.
        </p>
        
        <p>
        This may be visualized as follows: pick any <m>y_0</m> between <m>-1</m> and <m>1</m>.
        From that value on the <m>y</m>-axis, go horizontally until you hit the graph of the
        function <m>y=\cos(x)</m> and then drop down to the <m>x</m>-axis at the point <m>x_0</m>.
        This value <m>x_0</m> is the unique one for which <m>\cos(x_0)=y_0</m>.
        </p>
        
        <figure>
        <caption />
        <image width="75%">
        <asymptote>
        import graph;
        size (0 ,2.5 inch );
        real pi=2*acos(0);
        real xmin =0, xmax =pi;
        real ymax =1, ymin = -1;
        real yvalue=.35, xvalue=acos(yvalue);
        draw(graph(cos,0,pi));
        xaxis(xmin,xmax,Ticks);
        yaxis(ymin,ymax,LeftTicks);
        label("\(x\)",(xmax,0),E);
        label("\(y\)",(0,ymax),N);
        dot((pi,0)); label("\(\pi\)",(pi,0),N);
        dot((pi/2,0)); label("\(\frac\pi2\)",(pi/2,0),N);
        label("\(y=\cos(x)\)",(0.6,1),E);
        label("\(y_0\)", (0,yvalue),W,red);
        label("\(x_0\)", (xvalue,0), S,red);
        label("\((x_0,y_0)\)", (xvalue,yvalue), NE, red);
        path p=(0,yvalue)--(xvalue,yvalue)--(xvalue,0);
        draw(p,red);
        </asymptote>
        </image>
        </figure>
        </subsection>
        
        <subsection> <title>Parallelograms</title>
        <p>
        A parallelogram is a quadrilateral with opposite sides parallel.
        Here is an example of one filled in yellow:
        </p>
        
        <figure>
        <caption />
        <image width="50%">
        <asymptote>
        unitsize(1cm);
        pair a=(6,2), b=(2,4), Z=(0,0);
        path p=Z--a--a+b--b--cycle;
        filldraw(p,yellow);
        </asymptote>
        </image>
        </figure>
        
        <p>
        Here are some important properties of parallelograms:
            <ol>
            <li>The angles on consecutive vertices are supplementary (sum to <m>\pi</m>).</li>
            <li>Opposite angles are equal.</li>
            <li>Opposite sides have the same length.</li>
            </ol>
        Here is why these properties hold:
            <ol>
            <li>
                <p>
                 Let one vertex have an angle of <m>\theta</m>. Extend one of the sides as in the figure. 
                 The lines being parallel implies that the angle outside the parallelogram is 
                 also <m>\theta</m>, which in turn makes the next angle within the parallelogram 
                 equal to <m>\pi-\theta</m>.
                </p>
            <!-- image within a list does not satisfy the dtd -->
                <figure>
                <caption/>
                <image width="90%">
                <asymptote>
                 unitsize(1cm);
                 pair a=(6,2), b=(2,4), Z=(0,0);
                 draw(b/2--Z--a--a+b/2);
                 draw(Z--1.6a);
                 fill(b/2--Z--a--a+b/2--cycle,yellow);
                 label("\(\theta\)",Z,4NE);
                 label("\(\theta\)",a,4NE);
                 label("\(\pi-\theta\)",b,N);
                 draw(b{S}..{SE}a+0.3*NW,Arrow);
                </asymptote>
                </image>
                </figure>
            </li>
            <li>
                <p>
                 If one vertex has an angle <m>\theta</m>, then next has angle
                 <m>\pi-\theta</m>. The next vertex after that has angle <m>\pi-(\pi-\theta)=\theta</m>,
                 and the final vertex has angle <m>\pi-\theta</m>. Hence two opposite vertices
                 have an angle <m>\theta</m> while the other two opposite vertices have angle
                 <m>\pi-\theta</m>.  
               </p> 
            </li>
            <li>
                <p>
                 Let <m>b</m> and <m>b'</m> be opposite sides of the parallelogram.  Drop perpendiculars from two vertices to
                 the opposite line as shown in the following figure. Then the length of both of the new lines is the
                 distance, <m>d</m>, between the parallel lines, and so <m>d=b\sin(\theta)=b'\sin(\theta)</m> from which
                 follows <m>b=b'</m>.
                </p>
            </li>
            </ol>
        </p>
        
        <figure>
        <caption />
        <image width="75%">
        <asymptote>
        unitsize(1cm);
        pair a=(6,2), b=(2,4), Z=(0,0);
        pair p1=dot(a,b)/dot(a,a)*a, p2=dot(a+b,a)/dot(a,a)*a;
        draw(b/2--Z--a--a+b/2);
        draw(Z--1.6a);
        path p=Z--a--a+b--b--cycle;
        filldraw(p,yellow);
        label("\(b\)",b/2,NW);
        label("\(b'\)",a+b/2,SE);
        label("\(\theta\)",Z,4NE);
        label("\(\theta\)",a,4NE);
        dot(p1); dot(p2);
        draw(b--p1); draw(a+b--p2);
        label("\(d\)",(b+p1)/2,NE);
        label("\(d\)",(a+b+p2)/2,NE);
        </asymptote>
        </image>
        </figure>
        
        <p>
        The parallelogram law relates the lengths of sides of a
        parallelogram to the lengths of the diagonals.
        </p>
        <theorem xml:id="ParallelogramLaw"> <title>The parallelogram law</title>
        
        <statement>
        <p>
        If a parallelogram has sides of length <m>a</m> and <m>b</m>, and <m>c</m> and
        <m>d</m> are the lengths of the two diagonals, then
        \[
        c^2+d^2=2a^2+2b^2
        \]
        In other words, the sum of the squares of the sides is the sum of the squares 
        of the diagonals.
        </p>
        
        <figure>
        <caption />
        <image width="70%">
        <asymptote>
        unitsize(1cm);
        pair a=(6,2), b=(2,4), Z=(0,0);
        pair p1=dot(a,b)/dot(a,a)*a, p2=dot(a+b,a)/dot(a,a)*a;
        path p=Z--a--a+b--b--cycle;
        filldraw(p,yellow);
        draw(a--b);
        draw(Z--a+b);
        label("\(a\)",a/2,SE);
        label("\(a\)",a/2+b,NW);
        label("\(b\)",b/2,NW);
        label("\(b\)",a+b/2,SE);
        label("\(c\)",.6*(a+b),N);
        label("\(d\)",.3*a+.7*b,N);
        label("\(\theta\)",Z,4*NE);
        </asymptote>
        </image>
        </figure>
        </statement>
        
        <proof>
        <p>
        The proof can be seen from the following two figures. Applying the law of
        cosines to the first figure gives 
        \[
        c^2=a^2+b^2-2ab\cos(\theta)\tag1
        \]
        while applying it to the second figure gives
        \[
        d^2=a^2+b^2-2ab\cos(\pi-\theta)= a^2+b^2+2ab\cos(\theta)\tag2
        \]
        </p>
        
        
        <figure>
        <caption />
        <image width="70%">
        <asymptote>
        unitsize(1cm);
        pair a=(6,2), b=(2,4), Z=(0,0);
        pair p1=dot(a,b)/dot(a,a)*a, p2=dot(a+b,a)/dot(a,a)*a;
        path p=Z--a--a+b--b--cycle;
        filldraw(p,yellow);
        draw(a--b);
        label("\(a\)",a/2,SE);
        label("\(a\)",a/2+b,NW);
        label("\(b\)",b/2,NW);
        label("\(b\)",a+b/2,SE);
        label("\(c\)",1/2*(a+b),N);
        label("\(\theta\)",Z,4*NE);
        </asymptote>
        </image>
        </figure>

        <p>
        and
        </p>
        
        <figure>
        <caption />
        <image width="70%">
        <asymptote>
        unitsize(1cm);
        pair a=(6,2), b=(2,4), Z=(0,0);
        pair p1=dot(a,b)/dot(a,a)*a, p2=dot(a+b,a)/dot(a,a)*a;
        path p=Z--a--a+b--b--cycle;
        filldraw(p,yellow);
        draw(Z--a+b);
        label("\(a\)",a/2,SE);
        label("\(a\)",a/2+b,NW);
        label("\(b\)",b/2,NW);
        label("\(b\)",a+b/2,SE);
        label("\(d\)",1/2*(a+b),S);
        label("\(\pi-\theta\)",b,SE);
        </asymptote>
        </image>
        </figure>

        <p>
        Adding (1) and (2) gives
        \[
        c^2+d^2=2a^2+2b^2
        \]
        </p>
        </proof>
        </theorem>
        </subsection>
         
        <subsection> <title>The addition formulas for <m>\sin(x)</m> and <m>\cos(x)</m></title>
        <p>
        Suppose we know <m>\sin(\alpha)</m>, <m>\cos(\alpha)</m>, <m>\sin(\beta)</m>, and
        <m>\cos(\beta)</m>. We wish to find formulas to compute <m>\sin(\alpha+\beta)</m>,
        and <m>\cos(\alpha+\beta)</m>. The following figure 
        shows us how to do the computation.
        </p>
        
        <p>
        The points <m>A=(\cos(\alpha),\sin(\alpha))</m>,
        <m>B=\cos(\alpha+\beta),\sin(\alpha+\beta))</m> and <m>(0,1)</m> all lie on a circle
        with centre at the origin and radius <m>1</m>. These three points are joined
        by a line to the origin <m>\vec 0</m>.  Drop perpediculars from B to the
        line joining <m>\vec0</m> and <m>(1,0)</m> and also to the line joining  and
        <m>\vec0</m> and <m>A</m>. The points <m>F</m> and <m>E</m> are where the perpendiculars
        meet the respective lines.  The line joining <m>B</m> and <m>E</m> is extended
        until it meets the line joining <m>\vec 0</m> and <m>(1,0)</m> at <m>D=(d,0)</m>.
        </p>
        
        <figure>
        <caption />
        <image>
        <asymptote>
        unitsize(300);
        real alpha=25, beta=45;
        pair A=(Cos(alpha),Sin(alpha)), B=(Cos(alpha+beta),Sin(alpha+beta));
        pair Z=(0,0);
        pair D=(B.x+A.y*B.y/A.x,0);
        filldraw(Z--B--D--cycle,lightyellow);
        
        dot(A); dot(B); dot(D); dot(Z);
        draw(arc(Z,1,0,alpha+beta));
        draw(A--Z--(1,0)..(arc(Z,1,0,alpha+beta)));
        
        draw(B--(B.x,0));
        dot((B.x,0));
        label("\(F\)",(B.x,0),S);
        
        real DP(pair A, pair B) {
           return A.x*B.x+A.y*B.y;
           }
        
        pair T(pair X, pair Y, pair Z) {
           real t= DP((X-Y),(Z-Y))/DP(X-Y,X-Y);
           pair U= t*X+(1-t)*Y;
           dot(U);
           return U;
           }
        
        label("\(E\)",T(Z,A,B),2*ESE);;
        label("\(A=(\cos(\alpha),\sin(\alpha))\)",A,E);
        label("\(B=(\cos(\alpha+\beta),\sin(\alpha+\beta))\)",B,NE);
        label("\((1,0)\)",(1,0),E);
        label("\(D=(d,0)\)",D,S);
        label("\(\mathbf 0\)",(0,0),S);
        label("\(\alpha\)",Z,9(1,Tan(alpha/2)));
        label("\(\beta\)",Z,7(1,Tan(alpha+beta/2)));
        label("\(\frac\pi2-\alpha\ \)",D,3*NW);
        label("\(\quad \alpha\)",B,7*S);
        </asymptote>
        </image>
        </figure>
        
        <p>
        Now we compute the area of the yellow triangle in two different ways
        and equate the results.
        </p>
        
        <p>
        Consider the lengths of these lines:
            <ul>
            <li><p><m>\|BF\|=\sin(\alpha+\beta)</m> (from triangle <m>\vec0 B F</m>)</p></li>
            <li><p><m>\|\vec 0 D\|=d</m></p></li>
            <li><p><m>\|\vec 0 E\|=d\cos(\alpha)=\cos(\beta)</m> (from triangles
            <m>\vec0DE</m> and <m>\vec0BE</m>,
            which implies that <m>d=\frac{\cos(\beta)}{\cos(\alpha)}</m>)</p></li>
            <li><p><m>\|BD\|=\|BE\|+\|ED\|=\sin(\beta)+d\sin(\alpha)</m> (from
            triangles <m>\vec0BE</m> and
            <m>\vec0DE</m>)</p></li>
            </ul>
        </p>
        
        <p>
        On the one hand, twice the area of the yellow triangle is
        \[
        \|BF\|\,\|\vec0 D\| = d\sin(\alpha+\beta)
        \]
        
        On the other hand, twice the triangle area is also
        \[
        \|\vec0 E\|\, \|BD\|=d\cos(\alpha)\left(\sin(\beta)+d\sin(\alpha)\right)
        \]
        
        Equating the two values we get
        \[
        \sin(\alpha+\beta)=\cos(\alpha)\sin(\beta)+d\cos(\alpha)\sin(\alpha)
        =\cos(\alpha)\sin(\beta)+\cos(\beta)\sin(\alpha)
        \]
        </p>
        
        <p>
        Note that, from the right
        triangle <m>\vec0 E D</m>, the angle at <m>D</m> is <m>\frac\pi2-\alpha</m>, and
        then from the right triangle <m>B F D</m> the angle at <m>B</m> is <m>\alpha</m>. We use this
        to compute some more lengths:
            <ul>
            <li><p> <m>\|DF\| = \|BD\|\sin(\alpha)=(\sin(\beta)+d\sin(\alpha))\sin(\alpha)</m></p></li>
            <li><p> <m>\|\vec0 F\|=\cos(\alpha+\beta)</m></p></li>
            </ul>
        Now we have
        <me>
        \begin{array}{rl}
        \|\vec0 F\|
        \amp= \|\vec0 D\|- \|DF\|\\
        \cos(\alpha+\beta)
        \amp=d-(\sin(\beta)+d\sin(\alpha))\sin(\alpha)\\
        \amp=d-\sin(\beta)\sin(\alpha)-d\, \sin^2(\alpha)\\
        \amp= d(1-\sin^2(\alpha))-\sin(\beta)\sin(\alpha)\\
        \amp=d\,\cos^2(\alpha)-\sin(\beta)\sin(\alpha)\\
        \amp= \cos(\beta)\cos(\alpha)-\sin(\beta)\sin(\alpha)
        \end{array}
        </me>
        </p>
        
        
        <p>
        and so we have
        \[
        \sin(\alpha+\beta)=\sin(\alpha)\cos(\beta)+\cos(\alpha)\sin(\beta)\\
        \cos(\alpha+\beta)=\cos(\alpha)\cos(\beta)-\sin(\alpha)\sin(\beta)
        \]
        </p>

        <theorem xml:id="SinCosSums">
        <title>Addition formulas for <m>\sin(x)</m> and <m>\cos(x)</m></title>
        <statement>
        <p>
            <me>
            \sin(\alpha+\beta)=\sin(\alpha)\cos(\beta)+\cos(\alpha)\sin(\beta)\\
            \cos(\alpha+\beta)=\cos(\alpha)\cos(\beta)-\sin(\alpha)\sin(\beta)
            </me>
        </p>
        </statement>
        </theorem>
        </subsection>
        
        <subsection> <title>The law of sines</title>
        <p>
        Suppose a triangle has sides of lengths <m>a</m>, <m>b</m> and <m>c</m>. Let
        <m>\alpha</m>, <m>\beta</m>, and <m>\gamma</m> be the angles opposite
        <m>a</m>, <m>b</m> and <m>c</m> respectively. In addition, let the vertices
        of the triangle be labeled <m>A</m>, <m>B</m> and <m>C</m> at the angles
        <m>\alpha</m>, <m>\beta</m>, and <m>\gamma</m> respectively. Our picture
        looks like this:
        </p>
        
        <figure>
        <caption />
        <image width="80%">
        <asymptote>
        unitsize(48);
        pair A=(4,2), C=(6,0), B=(0,0), D=(A.x,0);
        
        draw(A--B--C--cycle);
        label("\(A\)",A,N);
        label("\(B\)",B,W);
        label("\(C\)",C,E);
        label("\(a\)",1/2*B+1/2*C,S);
        label("\(b\)",1/2*A+1/2*C,NE);
        label("\(c\)",1/2*A+1/2*B,NW);
        
        real t=.9;
        label("\(\beta\)", t*B+(1-t)/2*(A+C));
        label("\(\gamma\)", t*C+(1-t)/2*(A+B));
        label("\(\alpha\)", t*A+(1-t)/2*(B+C));
        t=.8;
        </asymptote>
        </image>
        </figure>
        
        
        <p>
        It is pretty obvious that
        the sides with longer length will have larger angles
        opposite. The law of sines makes this realtionship
        precise.
        </p>
        
        <theorem xml:id="LawOfSines"> <title>The law of sines</title>
        <statement>
        <p>
            Suppose a triangle has sides of lengths <m>a</m>, <m>b</m> and <m>c</m>, and
            <m>\alpha</m>, <m>\beta</m>, and <m>\gamma</m> are the angles opposite
            <m>a</m>, <m>b</m> and <m>c</m> respectively. Then
            \[
            \frac{\sin(\alpha)}{a}
            =\frac{\sin(\beta)}{b}
            =\frac{\sin(\gamma)}{c}
            \]
        </p>
        </statement>
        <proof>
        <p>
            With no loss of generality, we may assume that <m>\alpha</m> is
            the largest angle. It may be acute or obtuse, but <m>\beta</m> and
            <m>\gamma</m> are definitely acute. That implies that we may drop
            a perpendicular from <m>A</m> that will divide the angle <m>\alpha</m> into
            <m>\alpha_1</m> and  <m>\alpha_2</m> the side opposite into segements with
            lengths <m>a_1</m> and <m>a_2</m>.
        </p>

        <figure>
        <caption />
        <image width="100%">
        <asymptote>
        unitsize(48);
        pair A=(4,2), C=(6,0), B=(0,0), D=(A.x,0);
        
        draw(A--B--C--cycle);
        draw(A--D);
        label("\(A\)",A,N);
        label("\(B\)",B,W);
        label("\(C\)",C,E);
        label("\(a\)",1/2*B+1/2*C,S);
        label("\(b\)",1/2*A+1/2*C,NE);
        label("\(c\)",1/2*A+1/2*B,NW);
        label("\(a_1\)",2/3*B+1/3*C,N);
        label("\(a_2\)",1/5*B+4/5*C,N);
        
        real t=.9;
        label("\(\beta\)", t*B+(1-t)/2*(A+C));
        label("\(\gamma\)", t*C+(1-t)/2*(A+B));
        label("\(\alpha\)", t*A+(1-t)/2*(B+C));
        t=.8;
        label("\(\alpha_1\)", t*A+(1-t)/2*(B+D));
        label("\(\alpha_2\)", t*A+(1-t)/2*(C+D));
        label("\(h\)", (A.x,A.y/2), W);
        </asymptote>
        </image>
        </figure>

        <p>
        Then we have
        <me>
        \begin{array}{rl}
        \sin(\alpha) \amp = \sin(\alpha_1+\alpha_2)\\
        \amp =\sin(\alpha_1)\cos(\alpha_2)+\cos(\alpha_1)\sin(\alpha_2)\\
        \amp =\frac{a_1}c \frac hb + \frac hc \frac {a_2}b\\
        \amp =\frac{ha}{bc}
        \end{array}
        </me>
        and so
        \[
        \frac{\sin(\alpha)}a = \frac h{bc}
        \]
        Using <m>\sin(\beta)=\frac hc</m> and <m>\sin(\gamma)=\frac hb</m>,
        it follows that
        \[
        \frac{\sin(\beta)}b = \frac h{bc} \textrm{ and }
        \frac{\sin(\gamma)}c = \frac h{bc}
        \]
        and so
        \[
        \frac h{bc}=\frac{\sin(\alpha)}a =\frac{\sin(\beta)}b =\frac{\sin(\gamma)}c
        \]
        </p>
        </proof>
        </theorem>
        
        </subsection>
    </section>
    

</chapter>


</book>
</pretext>




